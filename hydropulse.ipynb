{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "72d0c1d1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Imports ---\n",
        "import os, json, math, io, zipfile, time, re, shutil, glob, pathlib, sys, subprocess, shlex, tempfile, warnings\n",
        "from pathlib import Path\n",
        "from datetime import datetime as dt, timedelta, timezone, date, datetime\n",
        "from dateutil import parser as dateparser\n",
        "from dateutil.relativedelta import relativedelta\n",
        "from urllib.parse import urljoin, quote\n",
        "from functools import reduce\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from tqdm import tqdm\n",
        "import pytz\n",
        "import xarray as xr\n",
        "import rasterio\n",
        "from rasterio.mask import mask\n",
        "\n",
        "import shapely\n",
        "from shapely import ops\n",
        "from shapely.geometry import Point, Polygon, box, mapping\n",
        "from shapely.ops import unary_union, transform as shp_transform\n",
        "\n",
        "from pyproj import Transformer\n",
        "import geopandas as gpd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Rectangle\n",
        "import folium\n",
        "import ee  # Earth Engine\n",
        "\n",
        "from sklearn.neighbors import BallTree\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.spatial.distance import cdist\n",
        "from geopy.distance import geodesic\n",
        "\n",
        "import yaml\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "\n",
        "def skip_if_exists(path: str) -> bool:\n",
        "    return os.path.exists(path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b6f568ab",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Config loaded from /Users/Shared/blueleaflabs/hydropulse/config/config.yaml\n",
            "Output dir: /Users/Shared/blueleaflabs/hydropulse/results\n",
            "Final daily filename: /Users/Shared/blueleaflabs/hydropulse/results/final_daily_grid_CA_3000m_epsg3310_20240601_20241031.parquet\n"
          ]
        }
      ],
      "source": [
        "# --- Configuration loader (shared HeatShield/HydroPulse) ---\n",
        "from pathlib import Path\n",
        "import os\n",
        "import re\n",
        "import yaml\n",
        "\n",
        "def load_env_file(path: Path) -> dict:\n",
        "    env = {}\n",
        "    if not path.exists():\n",
        "        return env\n",
        "    for line in path.read_text().splitlines():\n",
        "        line = line.strip()\n",
        "        if not line or line.startswith(\"#\") or \"=\" not in line:\n",
        "            continue\n",
        "        key, val = line.split(\"=\", 1)\n",
        "        env[key.strip()] = val.strip().strip('\"').strip(\"'\")\n",
        "    return env\n",
        "\n",
        "def apply_env_overrides() -> None:\n",
        "    env = load_env_file(Path(\".env\"))\n",
        "    for k, v in env.items():\n",
        "        if v:\n",
        "            os.environ[k] = v\n",
        "\n",
        "def _fmt_yyyymmdd(s: str) -> str:\n",
        "    # expects YYYY-MM-DD\n",
        "    return re.sub(r\"-\", \"\", s.strip())\n",
        "\n",
        "def _render_template(tpl: str, *, start: str, end: str, res_m: int, epsg: int, region: str) -> str:\n",
        "    return tpl.format(\n",
        "        start=_fmt_yyyymmdd(start),\n",
        "        end=_fmt_yyyymmdd(end),\n",
        "        res_m=int(res_m),\n",
        "        epsg=int(epsg),\n",
        "        region=str(region),\n",
        "    )\n",
        "\n",
        "def _resolve_out_dir(project_dir: Path, out_dir_value: str | None) -> Path:\n",
        "    out_dir_value = out_dir_value or \"results\"\n",
        "    p = Path(out_dir_value)\n",
        "    return (p if p.is_absolute() else (project_dir / p)).resolve()\n",
        "\n",
        "def _resolve_pathlike_keys(cfg: dict, out_dir: Path) -> None:\n",
        "    \"\"\"\n",
        "    Resolve config values that look like relative file paths under out_dir.\n",
        "    Rule: if key ends with _FILENAME, _PATH, _ZIP, _TIF_NAME, _CSV_NAME, _PARQUET_NAME, _TXT_NAME\n",
        "    and the value is a relative path, make it absolute under out_dir.\n",
        "    \"\"\"\n",
        "    suffixes = (\n",
        "        \"_FILENAME\", \"_PATH\", \"_ZIP\",\n",
        "        \"_TIF_NAME\", \"_CSV_NAME\", \"_PARQUET_NAME\", \"_TXT_NAME\",\n",
        "        \"_NETCDF_NAME\", \"_ZARR_NAME\"\n",
        "    )\n",
        "    for k, v in list(cfg.items()):\n",
        "        if not isinstance(v, str):\n",
        "            continue\n",
        "        if not k.endswith(suffixes):\n",
        "            continue\n",
        "        p = Path(v)\n",
        "        if p.is_absolute():\n",
        "            cfg[k] = str(p)\n",
        "        else:\n",
        "            cfg[k] = str((out_dir / p).resolve())\n",
        "\n",
        "# Fail fast unless PROJECT_DIR is explicitly provided.\n",
        "apply_env_overrides()\n",
        "\n",
        "PROJECT_DIR = os.environ.get(\"PROJECT_DIR\")\n",
        "if not PROJECT_DIR:\n",
        "    raise FileNotFoundError(\n",
        "        \"PROJECT_DIR is not set. Add PROJECT_DIR to .env or environment variables.\"\n",
        "    )\n",
        "\n",
        "PROJECT_DIR = Path(PROJECT_DIR).expanduser().resolve()\n",
        "CONFIG_PATH = PROJECT_DIR / \"config\" / \"config.yaml\"\n",
        "if not CONFIG_PATH.exists():\n",
        "    raise FileNotFoundError(f\"Missing config file: {CONFIG_PATH}\")\n",
        "\n",
        "CONFIG = yaml.safe_load(CONFIG_PATH.read_text()) or {}\n",
        "\n",
        "# Set out_dir to an absolute path early.\n",
        "OUT_DIR = _resolve_out_dir(PROJECT_DIR, CONFIG.get(\"out_dir\", \"results\"))\n",
        "CONFIG[\"out_dir\"] = str(OUT_DIR)\n",
        "\n",
        "# Optional: Apply env overrides ONLY for keys that exist in config.yaml.\n",
        "# This prevents HeatShield-specific lists in shared code.\n",
        "for key in list(CONFIG.keys()):\n",
        "    env_val = os.environ.get(key)\n",
        "    if env_val:\n",
        "        CONFIG[key] = env_val\n",
        "\n",
        "# Also allow a small, explicit allowlist for common optional overrides across projects.\n",
        "for key in [\"PURPLEAIR_SENSOR_INDEX\"]:  # harmless if absent in HydroPulse config\n",
        "    env_val = os.environ.get(key)\n",
        "    if env_val:\n",
        "        CONFIG[key] = env_val\n",
        "\n",
        "# Render FINAL_DAILY_FILENAME from template if provided.\n",
        "# Keeps downstream code stable: always refer to CONFIG[\"FINAL_DAILY_FILENAME\"].\n",
        "if \"FINAL_DAILY_FILENAME_TEMPLATE\" in CONFIG:\n",
        "    region = CONFIG.get(\"region\", \"CA\")\n",
        "    start_date = CONFIG[\"start_date\"]\n",
        "    end_date = CONFIG[\"end_date\"]\n",
        "    res_m = CONFIG.get(\"grid_resolution_m\", 3000)\n",
        "    epsg = CONFIG.get(\"OPS_EPSG\", CONFIG.get(\"CA_ALBERS_EPSG\", 3310))\n",
        "    CONFIG[\"FINAL_DAILY_FILENAME\"] = _render_template(\n",
        "        CONFIG[\"FINAL_DAILY_FILENAME_TEMPLATE\"],\n",
        "        start=start_date,\n",
        "        end=end_date,\n",
        "        res_m=res_m,\n",
        "        epsg=epsg,\n",
        "        region=region,\n",
        "    )\n",
        "\n",
        "# Resolve pathlike keys under out_dir (only for keys that exist).\n",
        "_resolve_pathlike_keys(CONFIG, OUT_DIR)\n",
        "\n",
        "# Create common directories only if they are referenced in config.\n",
        "# (Avoid hardcoding \"manual\" for HydroPulse.)\n",
        "for k, v in CONFIG.items():\n",
        "    if k.endswith(\"_DIRNAME\") and isinstance(v, str):\n",
        "        os.makedirs(Path(CONFIG[\"out_dir\"]) / v, exist_ok=True)\n",
        "\n",
        "# EPSG constants (configurable)\n",
        "WGS84_EPSG = int(CONFIG.get(\"WGS84_EPSG\", 4326))\n",
        "CA_ALBERS_EPSG = int(CONFIG.get(\"CA_ALBERS_EPSG\", 3310))\n",
        "OPS_EPSG = int(CONFIG.get(\"OPS_EPSG\", CA_ALBERS_EPSG))\n",
        "\n",
        "# Set working directory\n",
        "os.chdir(PROJECT_DIR)\n",
        "\n",
        "print(f\"Config loaded from {CONFIG_PATH}\")\n",
        "print(f\"Output dir: {CONFIG['out_dir']}\")\n",
        "print(f\"Final daily filename: {CONFIG.get('FINAL_DAILY_FILENAME')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "19be8061",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "def resolve_out_path(path_str: str) -> str:\n",
        "    \"\"\"\n",
        "    Resolve a path string relative to CONFIG['out_dir'] unless already absolute.\n",
        "    Returns an absolute string path.\n",
        "    \"\"\"\n",
        "    p = Path(path_str)\n",
        "    if p.is_absolute():\n",
        "        return str(p)\n",
        "    return str((Path(CONFIG[\"out_dir\"]) / p).resolve())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "5ea9e2b8",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/98/ykbpmmjd0pdf8zgd8bc1hhmr0000gn/T/ipykernel_34692/3235801746.py:34: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
            "  b = gpd.GeoDataFrame(geometry=[b.unary_union], crs=f\"EPSG:{CA_ALBERS_EPSG}\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: /Users/Shared/blueleaflabs/hydropulse/results/grid_3000m_CA.parquet\n",
            "Cells: 46,495\n",
            "Effective land area ≈ 410,516 km²\n",
            "Implied cell size ≈ 2.97 km\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>col_i</th>\n",
              "      <th>row_j</th>\n",
              "      <th>geometry</th>\n",
              "      <th>cell_area_m2</th>\n",
              "      <th>grid_id</th>\n",
              "      <th>land_area_m2</th>\n",
              "      <th>land_frac</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>215</td>\n",
              "      <td>0</td>\n",
              "      <td>POLYGON ((-117.09883 32.52004, -117.09786 32.5...</td>\n",
              "      <td>9000000.0</td>\n",
              "      <td>CA3310_3000_215_0</td>\n",
              "      <td>3.527256e+06</td>\n",
              "      <td>0.391917</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>216</td>\n",
              "      <td>0</td>\n",
              "      <td>POLYGON ((-117.06697 32.51921, -117.06599 32.5...</td>\n",
              "      <td>9000000.0</td>\n",
              "      <td>CA3310_3000_216_0</td>\n",
              "      <td>2.924116e+06</td>\n",
              "      <td>0.324902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>217</td>\n",
              "      <td>0</td>\n",
              "      <td>POLYGON ((-117.03511 32.51837, -117.03411 32.5...</td>\n",
              "      <td>9000000.0</td>\n",
              "      <td>CA3310_3000_217_0</td>\n",
              "      <td>1.712565e+06</td>\n",
              "      <td>0.190285</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>218</td>\n",
              "      <td>0</td>\n",
              "      <td>POLYGON ((-117.00325 32.51751, -117.00224 32.5...</td>\n",
              "      <td>9000000.0</td>\n",
              "      <td>CA3310_3000_218_0</td>\n",
              "      <td>5.055427e+05</td>\n",
              "      <td>0.056171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>214</td>\n",
              "      <td>1</td>\n",
              "      <td>POLYGON ((-117.12973 32.54795, -117.12876 32.5...</td>\n",
              "      <td>9000000.0</td>\n",
              "      <td>CA3310_3000_214_1</td>\n",
              "      <td>9.690513e+04</td>\n",
              "      <td>0.010767</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   col_i  row_j                                           geometry  \\\n",
              "0    215      0  POLYGON ((-117.09883 32.52004, -117.09786 32.5...   \n",
              "1    216      0  POLYGON ((-117.06697 32.51921, -117.06599 32.5...   \n",
              "2    217      0  POLYGON ((-117.03511 32.51837, -117.03411 32.5...   \n",
              "3    218      0  POLYGON ((-117.00325 32.51751, -117.00224 32.5...   \n",
              "4    214      1  POLYGON ((-117.12973 32.54795, -117.12876 32.5...   \n",
              "\n",
              "   cell_area_m2            grid_id  land_area_m2  land_frac  \n",
              "0     9000000.0  CA3310_3000_215_0  3.527256e+06   0.391917  \n",
              "1     9000000.0  CA3310_3000_216_0  2.924116e+06   0.324902  \n",
              "2     9000000.0  CA3310_3000_217_0  1.712565e+06   0.190285  \n",
              "3     9000000.0  CA3310_3000_218_0  5.055427e+05   0.056171  \n",
              "4     9000000.0  CA3310_3000_214_1  9.690513e+04   0.010767  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# --- Ensure California boundary and build 3 km grid clipped to land ---\n",
        "\n",
        "\n",
        "# Config\n",
        "res_m = int(CONFIG.get(\"grid_resolution_m\", 3000))\n",
        "out_epsg = int(CONFIG.get(\"crs_epsg\", 4326))\n",
        "out_dir = CONFIG[\"out_dir\"]; os.makedirs(out_dir, exist_ok=True)\n",
        "inset_buffer_m = int(CONFIG.get(\"coast_inset_m\", 0))  # e.g. 5000\n",
        "boundary_path = CONFIG.get(\"ca_boundary_path\", None)\n",
        "\n",
        "# 1) Ensure boundary: download Census cartographic boundary if missing\n",
        "if not boundary_path or not os.path.exists(boundary_path):\n",
        "    states_zip = os.path.join(out_dir, \"cb_2023_us_state_20m.zip\")\n",
        "    if not os.path.exists(states_zip):\n",
        "        url = CONFIG[\"CENSUS_STATES_ZIP_URL\"]\n",
        "        r = requests.get(url, timeout=int(CONFIG.get(\"CENSUS_STATES_TIMEOUT\", 120))); r.raise_for_status()\n",
        "        with open(states_zip, \"wb\") as f: f.write(r.content)\n",
        "    # Read from zip directly and select California\n",
        "    states = gpd.read_file(f\"zip://{states_zip}\")\n",
        "    if states.empty:\n",
        "        raise ValueError(\"Census states file loaded empty.\")\n",
        "    ca = states[states[\"STATEFP\"].astype(str).str.zfill(2).eq(\"06\")][[\"geometry\"]]\n",
        "    if ca.empty:\n",
        "        raise ValueError(\"California polygon not found in Census states file.\")\n",
        "    boundary_path = os.path.join(out_dir, \"california_boundary.gpkg\")\n",
        "    ca.to_file(boundary_path, driver=\"GPKG\")\n",
        "    CONFIG[\"ca_boundary_path\"] = boundary_path  # persist for later cells\n",
        "\n",
        "# 2) Load boundary, dissolve, project, optional inward buffer\n",
        "b = gpd.read_file(boundary_path)\n",
        "if b.crs is None: raise ValueError(\"Boundary file has no CRS.\")\n",
        "b = b[[\"geometry\"]].copy()\n",
        "b = b.to_crs(CA_ALBERS_EPSG)\n",
        "b = gpd.GeoDataFrame(geometry=[b.unary_union], crs=f\"EPSG:{CA_ALBERS_EPSG}\")\n",
        "if inset_buffer_m > 0:\n",
        "    b.geometry = b.buffer(-inset_buffer_m)\n",
        "    b = gpd.GeoDataFrame(geometry=[b.unary_union], crs=f\"EPSG:{CA_ALBERS_EPSG}\")\n",
        "\n",
        "# 3) Build snapped rectilinear grid over boundary bounds in EPSG:3310\n",
        "minx, miny, maxx, maxy = b.total_bounds\n",
        "snap_down = lambda v, s: np.floor(v/s)*s\n",
        "snap_up   = lambda v, s: np.ceil(v/s)*s\n",
        "minx, miny = snap_down(minx, res_m), snap_down(miny, res_m)\n",
        "maxx, maxy = snap_up(maxx, res_m), snap_up(maxy, res_m)\n",
        "\n",
        "xs = np.arange(minx, maxx, res_m)\n",
        "ys = np.arange(miny, maxy, res_m)\n",
        "n_rect = len(xs)*len(ys)\n",
        "if n_rect > 3_500_000:\n",
        "    raise MemoryError(f\"Grid too large ({n_rect:,}). Increase res_m or tile the state.\")\n",
        "\n",
        "cells, col_i, row_j = [], [], []\n",
        "for j, y in enumerate(ys):\n",
        "    for i, x in enumerate(xs):\n",
        "        cells.append(box(x, y, x+res_m, y+res_m)); col_i.append(i); row_j.append(j)\n",
        "\n",
        "gdf_proj = gpd.GeoDataFrame({\"col_i\": np.int32(col_i), \"row_j\": np.int32(row_j)},\n",
        "                            geometry=cells, crs=f\"EPSG:{CA_ALBERS_EPSG}\")\n",
        "gdf_proj[\"cell_area_m2\"] = float(res_m)*float(res_m)\n",
        "gdf_proj[\"grid_id\"] = f\"CA3310_{res_m}_\" + gdf_proj[\"col_i\"].astype(str) + \"_\" + gdf_proj[\"row_j\"].astype(str)\n",
        "\n",
        "# 4) Strict land clip and land fraction\n",
        "gdf_proj = gpd.sjoin(gdf_proj, b, how=\"inner\", predicate=\"intersects\").drop(columns=[\"index_right\"])\n",
        "inter = gpd.overlay(gdf_proj[[\"grid_id\",\"geometry\"]], b, how=\"intersection\", keep_geom_type=True)\n",
        "inter[\"land_area_m2\"] = inter.geometry.area\n",
        "land = inter[[\"grid_id\",\"land_area_m2\"]].groupby(\"grid_id\", as_index=False).sum()\n",
        "gdf_proj = gdf_proj.merge(land, on=\"grid_id\", how=\"left\")\n",
        "gdf_proj[\"land_area_m2\"] = gdf_proj[\"land_area_m2\"].fillna(0.0)\n",
        "gdf_proj[\"land_frac\"] = (gdf_proj[\"land_area_m2\"] / gdf_proj[\"cell_area_m2\"]).clip(0,1)\n",
        "gdf_proj = gdf_proj[gdf_proj[\"land_frac\"] > 0].reset_index(drop=True)\n",
        "\n",
        "# 5) Reproject to requested output CRS and save\n",
        "grid_gdf = gdf_proj.to_crs(out_epsg)\n",
        "\n",
        "parquet_path = os.path.join(out_dir, f\"grid_{res_m}m_CA.parquet\")\n",
        "grid_gdf.to_parquet(parquet_path, index=False)\n",
        "\n",
        "geojson_path = os.path.join(out_dir, f\"grid_{res_m}m_CA_head10.geojson\")\n",
        "grid_gdf.head(10).to_file(geojson_path, driver=\"GeoJSON\")\n",
        "\n",
        "# Diagnostics\n",
        "cell_area_km2 = (res_m/1000.0)**2\n",
        "eff_land_km2 = float((grid_gdf.get(\"land_frac\",1.0) * cell_area_km2).sum())\n",
        "print(f\"Saved: {parquet_path}\")\n",
        "print(f\"Cells: {len(grid_gdf):,}\")\n",
        "print(f\"Effective land area ≈ {round(eff_land_km2):,} km²\")\n",
        "print(f\"Implied cell size ≈ {round((eff_land_km2/len(grid_gdf))**0.5,2)} km\")\n",
        "\n",
        "grid_gdf.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "026fd848",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: /Users/Shared/blueleaflabs/hydropulse/results/config_runtime.json\n",
            "Saved: /Users/Shared/blueleaflabs/hydropulse/results/grid_3000m_CA_epsg3310.parquet | cells: 46495\n",
            "Saved: /Users/Shared/blueleaflabs/hydropulse/results/grid_3000m_CA_head500_epsg4326.geojson\n",
            "Saved: /Users/Shared/blueleaflabs/hydropulse/results/grid_3000m_CA_meta.json\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'timestamp_utc': '2026-01-12T18:50:37Z',\n",
              " 'grid_resolution_m': 3000,\n",
              " 'crs_ops_epsg': 3310,\n",
              " 'crs_export_default_epsg': 4326,\n",
              " 'cells': 46495,\n",
              " 'effective_land_area_km2': 410516.3,\n",
              " 'implied_cell_km': 2.9714,\n",
              " 'bbox_km_width_height': [np.float64(915.0), np.float64(1059.0)],\n",
              " 'has_land_frac': True,\n",
              " 'boundary_path': '/Users/Shared/blueleaflabs/hydropulse/results/california_boundary.gpkg',\n",
              " 'parquet_3310_path': '/Users/Shared/blueleaflabs/hydropulse/results/grid_3000m_CA_epsg3310.parquet',\n",
              " 'geojson_preview_4326_path': '/Users/Shared/blueleaflabs/hydropulse/results/grid_3000m_CA_head500_epsg4326.geojson'}"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# --- Persist config + save grid (3310 ops copy, 4326 preview) + write metadata ---\n",
        "\n",
        "# Inputs assumed from prior cell:\n",
        "# - grid_gdf            : current grid GeoDataFrame (any CRS)\n",
        "# - CONFIG              : dict with out_dir, grid_resolution_m, crs_epsg, ca_boundary_path\n",
        "# - CA_ALBERS_EPSG=3310 : defined earlier\n",
        "\n",
        "out_dir = CONFIG[\"out_dir\"]; os.makedirs(out_dir, exist_ok=True)\n",
        "res_m = int(CONFIG.get(\"grid_resolution_m\", 3000))\n",
        "out_epsg = int(CONFIG.get(\"crs_epsg\", 4326))\n",
        "boundary_path = CONFIG.get(\"ca_boundary_path\")\n",
        "\n",
        "# 1) Persist boundary path back to CONFIG \n",
        "if not boundary_path or not os.path.exists(boundary_path):\n",
        "    raise FileNotFoundError(\"CONFIG['ca_boundary_path'] missing or invalid. Rebuild boundary.\")\n",
        "CONFIG[\"ca_boundary_path\"] = boundary_path\n",
        "\n",
        "config_runtime_path = os.path.join(out_dir, \"config_runtime.json\")\n",
        "with open(config_runtime_path, \"w\") as f:\n",
        "    json.dump(CONFIG, f, indent=2)\n",
        "print(\"Saved:\", config_runtime_path)\n",
        "\n",
        "# 2) Ensure we have an EPSG:3310 version for spatial ops\n",
        "if grid_gdf.crs is None:\n",
        "    raise ValueError(\"grid_gdf has no CRS. Rebuild grid.\")\n",
        "grid_3310 = grid_gdf.to_crs(3310) if grid_gdf.crs.to_epsg() != 3310 else grid_gdf\n",
        "\n",
        "# 3) Save operational GeoParquet in 3310 + lightweight WGS84 preview\n",
        "parquet_3310 = os.path.join(out_dir, f\"grid_{res_m}m_CA_epsg3310.parquet\")\n",
        "grid_3310.to_parquet(parquet_3310, index=False)\n",
        "print(\"Saved:\", parquet_3310, \"| cells:\", len(grid_3310))\n",
        "\n",
        "# Optional small preview in 4326 for quick map checks\n",
        "preview_4326 = grid_3310.to_crs(4326).head(500)  # cap to avoid huge files\n",
        "geojson_preview = os.path.join(out_dir, f\"grid_{res_m}m_CA_head500_epsg4326.geojson\")\n",
        "preview_4326.to_file(geojson_preview, driver=\"GeoJSON\")\n",
        "print(\"Saved:\", geojson_preview)\n",
        "\n",
        "# 4) Compute and save metadata\n",
        "cell_area_km2 = (res_m/1000.0)**2\n",
        "effective_land_km2 = float((grid_3310.get(\"land_frac\", 1.0) * cell_area_km2).sum())\n",
        "implied_cell_km = float((effective_land_km2 / len(grid_3310))**0.5)\n",
        "minx, miny, maxx, maxy = grid_3310.total_bounds\n",
        "bbox_km = ((maxx-minx)/1000.0, (maxy-miny)/1000.0)\n",
        "\n",
        "meta = {\n",
        "    \"timestamp_utc\": dt.utcnow().isoformat(timespec=\"seconds\") + \"Z\",\n",
        "    \"grid_resolution_m\": res_m,\n",
        "    \"crs_ops_epsg\": 3310,\n",
        "    \"crs_export_default_epsg\": out_epsg,\n",
        "    \"cells\": int(len(grid_3310)),\n",
        "    \"effective_land_area_km2\": round(effective_land_km2, 2),\n",
        "    \"implied_cell_km\": round(implied_cell_km, 4),\n",
        "    \"bbox_km_width_height\": [round(bbox_km[0], 2), round(bbox_km[1], 2)],\n",
        "    \"has_land_frac\": bool(\"land_frac\" in grid_3310.columns),\n",
        "    \"boundary_path\": boundary_path,\n",
        "    \"parquet_3310_path\": parquet_3310,\n",
        "    \"geojson_preview_4326_path\": geojson_preview,\n",
        "}\n",
        "\n",
        "meta_path = os.path.join(out_dir, f\"grid_{res_m}m_CA_meta.json\")\n",
        "with open(meta_path, \"w\") as f:\n",
        "    json.dump(meta, f, indent=2)\n",
        "print(\"Saved:\", meta_path)\n",
        "meta\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "29ff5b92",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Monthly files written: 15 → /Users/Shared/blueleaflabs/hydropulse/results/cdo_raw_monthly\n",
            "Saved raw:  /Users/Shared/blueleaflabs/hydropulse/results/ghcnd_daily_raw_all.csv\n",
            "Saved wide: /Users/Shared/blueleaflabs/hydropulse/results/ghcnd_daily_wide.csv\n",
            "Counts → raw: 291781 | wide: 163475 | stations: 1305 | dates: 153\n"
          ]
        }
      ],
      "source": [
        "# CDO data fetch and processing functions\n",
        "# repeat some variables for clarity\n",
        "OUT_DIR = CONFIG[\"out_dir\"]\n",
        "RAW_DIR = os.path.join(OUT_DIR, CONFIG[\"CDO_RAW_DIRNAME\"])\n",
        "CLEAN_DIR = os.path.join(OUT_DIR, CONFIG[\"CDO_CLEAN_DIRNAME\"])\n",
        "os.makedirs(RAW_DIR, exist_ok=True); os.makedirs(CLEAN_DIR, exist_ok=True)\n",
        "\n",
        "def month_windows(start_date, end_date):\n",
        "    s = dt.fromisoformat(start_date).date().replace(day=1)\n",
        "    e = dt.fromisoformat(end_date).date()\n",
        "    cur = s\n",
        "    while cur <= e:\n",
        "        nxt = (cur + relativedelta(months=1)) - relativedelta(days=1)\n",
        "        yield cur.isoformat(), min(nxt, e).isoformat()\n",
        "        cur = (cur + relativedelta(months=1)).replace(day=1)\n",
        "\n",
        "def parse_attributes(attr):\n",
        "    parts = (attr or \"\").split(\",\"); parts += [\"\"] * (4 - len(parts))\n",
        "    mflag, qflag, sflag, obs_hhmm = parts[:4]\n",
        "    return mflag or None, qflag or None, sflag or None, obs_hhmm or None\n",
        "\n",
        "def fetch_cdo_page(session, url, headers, params, max_retries=None, base_delay=None, timeout=None):\n",
        "    if max_retries is None:\n",
        "        max_retries = int(CONFIG.get(\"CDO_MAX_RETRIES\", 6))\n",
        "    if base_delay is None:\n",
        "        base_delay = float(CONFIG.get(\"CDO_BACKOFF_BASE\", 0.8))\n",
        "    if timeout is None:\n",
        "        timeout = int(CONFIG.get(\"CDO_TIMEOUT\", 180))\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            r = session.get(url, headers=headers, params=params, timeout=timeout)\n",
        "            if r.status_code in (429, 500, 502, 503, 504):\n",
        "                raise requests.HTTPError(f\"{r.status_code} retry\")\n",
        "            r.raise_for_status()\n",
        "            return r.json()\n",
        "        except Exception:\n",
        "            if attempt == max_retries - 1:\n",
        "                raise\n",
        "            time.sleep(base_delay * (2 ** attempt))\n",
        "\n",
        "\n",
        "def cdo_stream_monthly(datasetid, locationid, startdate, enddate, datatypes, token,\n",
        "                       units=\"standard\", page_limit=1000, force=False):\n",
        "    url = CONFIG[\"CDO_BASE_URL\"]\n",
        "    headers = {\"token\": token}\n",
        "    session = requests.Session()\n",
        "    written = []\n",
        "\n",
        "    for dtid in datatypes:\n",
        "        for ms, me in month_windows(startdate, enddate):\n",
        "            out_csv = os.path.join(RAW_DIR, f\"ghcnd_{dtid}_{ms[:7]}.csv\")\n",
        "            if skip_if_exists(out_csv) and not force:\n",
        "                # resume: skip existing month-datatype file\n",
        "                written.append(out_csv); continue\n",
        "\n",
        "            frames = []\n",
        "            offset = 1\n",
        "            while True:\n",
        "                params = {\n",
        "                    \"datasetid\": datasetid, \"locationid\": locationid,\n",
        "                    \"startdate\": ms, \"enddate\": me,\n",
        "                    \"datatypeid\": dtid, \"units\": units,\n",
        "                    \"limit\": page_limit, \"offset\": offset\n",
        "                }\n",
        "                js = fetch_cdo_page(session, url, headers, params)\n",
        "                rows = js.get(\"results\", [])\n",
        "                if not rows:\n",
        "                    break\n",
        "                frames.append(pd.json_normalize(rows))\n",
        "                if len(rows) < page_limit:\n",
        "                    break\n",
        "                offset += page_limit\n",
        "                time.sleep(0.15)  # gentle pacing\n",
        "\n",
        "            if frames:\n",
        "                df = pd.concat(frames, ignore_index=True)\n",
        "                # normalize\n",
        "                df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\").dt.date\n",
        "                parsed = df[\"attributes\"].apply(parse_attributes)\n",
        "                df[[\"mflag\",\"qflag\",\"sflag\",\"obs_hhmm\"]] = pd.DataFrame(parsed.tolist(), index=df.index)\n",
        "                # scale tenths\n",
        "                scale = {\"PRCP\": 0.1, \"TMAX\": 0.1, \"TMIN\": 0.1}\n",
        "                df[\"datatype\"] = df[\"datatype\"].astype(str)\n",
        "                df[\"value\"] = pd.to_numeric(df[\"value\"], errors=\"coerce\")\n",
        "                df[\"value_scaled\"] = df.apply(lambda r: r[\"value\"] * scale.get(r[\"datatype\"], 1.0), axis=1)\n",
        "                # write monthly raw\n",
        "                df[[\"date\",\"datatype\",\"station\",\"attributes\",\"mflag\",\"qflag\",\"sflag\",\"obs_hhmm\",\"value\",\"value_scaled\"]].to_csv(out_csv, index=False)\n",
        "                written.append(out_csv)\n",
        "            else:\n",
        "                # create an empty file with header to mark completion\n",
        "                with open(out_csv, \"w\", newline=\"\") as f:\n",
        "                    w = csv.writer(f); w.writerow([\"date\",\"datatype\",\"station\",\"attributes\",\"mflag\",\"qflag\",\"sflag\",\"obs_hhmm\",\"value\",\"value_scaled\"])\n",
        "                written.append(out_csv)\n",
        "    return written\n",
        "\n",
        "def build_clean_wide():\n",
        "    # read all monthly raw files and assemble cleaned wide once\n",
        "    files = sorted([os.path.join(RAW_DIR, f) for f in os.listdir(RAW_DIR) if f.endswith(\".csv\")])\n",
        "    if not files:\n",
        "        return None\n",
        "    df = pd.concat((pd.read_csv(f, dtype={\"datatype\":str,\"station\":str}) for f in files), ignore_index=True)\n",
        "    # convert types back\n",
        "    df[\"date\"] = pd.to_datetime(df[\"date\"]).dt.date\n",
        "    # keep good qflag\n",
        "    df = df[(df[\"qflag\"].isna()) | (df[\"qflag\"]==\"\")]\n",
        "    wide = (\n",
        "        df.pivot_table(index=[\"station\",\"date\"], columns=\"datatype\", values=\"value_scaled\", aggfunc=\"mean\")\n",
        "          .reset_index()\n",
        "          .rename(columns={\"date\":\"obs_date\",\"PRCP\":\"precipitation_mm\",\"TMAX\":\"temperature_max_c\",\"TMIN\":\"temperature_min_c\"})\n",
        "          .sort_values([\"obs_date\",\"station\"])\n",
        "    )\n",
        "    # attach obs time from PRCP\n",
        "    prcp_times = df[df[\"datatype\"]==\"PRCP\"][[\"station\",\"date\",\"obs_hhmm\"]].drop_duplicates().rename(columns={\"date\":\"obs_date\"})\n",
        "    wide = wide.merge(prcp_times, on=[\"station\",\"obs_date\"], how=\"left\")\n",
        "    raw_all = os.path.join(OUT_DIR, \"ghcnd_daily_raw_all.csv\")\n",
        "    wide_all = os.path.join(OUT_DIR, \"ghcnd_daily_wide.csv\")\n",
        "    df.to_csv(raw_all, index=False)\n",
        "    wide.to_csv(wide_all, index=False)\n",
        "    return raw_all, wide_all, len(df), len(wide), wide[\"station\"].nunique(), wide[\"obs_date\"].nunique()\n",
        "\n",
        "# ---- Run statewide with resume capability ----\n",
        "token = os.environ.get(\"CDO_TOKEN\") or CONFIG.get(\"CDO_TOKEN\", \"\")\n",
        "if token and token != \"YOUR_NCEI_CDO_TOKEN\":\n",
        "    written = cdo_stream_monthly(\n",
        "        datasetid=\"GHCND\",\n",
        "        locationid=\"FIPS:06\",                      # California statewide\n",
        "        startdate=CONFIG[\"start_date\"],\n",
        "        enddate=CONFIG[\"end_date\"],\n",
        "        datatypes=[\"TMAX\",\"TMIN\",\"PRCP\"],\n",
        "        token=token,\n",
        "        units=\"standard\",\n",
        "        page_limit=1000,\n",
        "        force=False                                 # set True to re-download\n",
        "    )\n",
        "    print(f\"Monthly files written: {len(written)} → {RAW_DIR}\")\n",
        "\n",
        "    res = build_clean_wide()\n",
        "    if res:\n",
        "        raw_all, wide_all, n_raw, n_wide, n_stn, n_dates = res\n",
        "        print(f\"Saved raw:  {raw_all}\")\n",
        "        print(f\"Saved wide: {wide_all}\")\n",
        "        print(f\"Counts → raw: {n_raw} | wide: {n_wide} | stations: {n_stn} | dates: {n_dates}\")\n",
        "else:\n",
        "    print(\"Skipping CDO (missing CDO token).\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "54dc4706",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved cleaned CDO daily → /Users/Shared/blueleaflabs/hydropulse/results/ghcnd_daily_cleaned.parquet (rows=163475, stations=1305)\n"
          ]
        }
      ],
      "source": [
        "# === GHCND DAILY: raw (long) -> cleaned (wide with lat/lon in bbox) ===\n",
        "# Input  (from your earlier step):  results/ghcnd_daily_raw_all.csv  (long form)\n",
        "# Output (used by superset):        results/ghcnd_daily_cleaned.parquet  (wide per station-day with lat/lon)\n",
        "\n",
        "# Need to do this because we aren't getting proper \"joins\" in our superset setup.\n",
        "\n",
        "\n",
        "BASE = CONFIG[\"out_dir\"]\n",
        "RAW = resolve_out_path(CONFIG[\"GHCND_RAW_CSV_NAME\"])\n",
        "OUT_PARQ = resolve_out_path(CONFIG[\"GHCND_CLEAN_PARQUET_NAME\"])\n",
        "OUT_CSV = resolve_out_path(CONFIG[\"GHCND_CLEAN_CSV_NAME\"])\n",
        "\n",
        "assert os.path.exists(RAW), f\"Missing raw GHCND file: {RAW}\"\n",
        "\n",
        "# 1) Ensure we have a station catalog with lat/lon\n",
        "#    Prefer a local copy if you already saved one; otherwise download NOAA's reference once.\n",
        "CAT_DIR = os.path.join(BASE, CONFIG[\"MANUAL_DIRNAME\"]); os.makedirs(CAT_DIR, exist_ok=True)\n",
        "CAT_TXT = os.path.join(CAT_DIR, CONFIG[\"GHCND_STATIONS_TXT_NAME\"])\n",
        "\n",
        "if not os.path.exists(CAT_TXT):\n",
        "    url = CONFIG[\"GHCND_STATIONS_URL\"]\n",
        "    r = requests.get(url, timeout=int(CONFIG.get(\"CENSUS_STATES_TIMEOUT\", 120))); r.raise_for_status()\n",
        "    with open(CAT_TXT, \"wb\") as f: f.write(r.content)\n",
        "\n",
        "# Parse ghcnd-stations.txt (fixed-width)\n",
        "# Columns per docs: ID(1-11), LAT(13-20), LON(22-30), ELEV(32-37), STATE(39-40), NAME(42-71) ...\n",
        "def parse_stations(path):\n",
        "    recs = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "        for line in f:\n",
        "            if len(line) < 40: \n",
        "                continue\n",
        "            sid = line[0:11].strip()\n",
        "            try:\n",
        "                lat = float(line[12:20].strip())\n",
        "                lon = float(line[21:30].strip())\n",
        "            except ValueError:\n",
        "                continue\n",
        "            state = line[38:40].strip()\n",
        "            name  = line[41:71].strip()\n",
        "            recs.append((sid, lat, lon, state, name))\n",
        "    return pd.DataFrame(recs, columns=[\"station_core\",\"lat\",\"lon\",\"state\",\"name\"])\n",
        "\n",
        "stations = parse_stations(CAT_TXT)\n",
        "\n",
        "# 2) Load your raw long-form CDO file\n",
        "# Expected columns seen in your sample:\n",
        "# ['attributes','datatype','date','mflag','obs_hhmm','qflag','sflag','station','value','value_scaled']\n",
        "raw = pd.read_csv(RAW, low_memory=False)\n",
        "\n",
        "# Normalize station key: raw uses \"GHCND:USW00023232\" → core \"USW00023232\"\n",
        "raw[\"station_core\"] = raw[\"station\"].astype(str).str.replace(\"^GHCND:\", \"\", regex=True)\n",
        "\n",
        "# Pick a numeric value column: prefer value_scaled if present; else scale GHCND native units.\n",
        "# GHCND native: PRCP = tenths of mm, TMAX/TMIN = tenths of °C.\n",
        "have_scaled = \"value_scaled\" in raw.columns\n",
        "def scaled_val(row):\n",
        "    if have_scaled and pd.notna(row[\"value_scaled\"]):\n",
        "        return float(row[\"value_scaled\"])\n",
        "    v = pd.to_numeric(row[\"value\"], errors=\"coerce\")\n",
        "    if pd.isna(v): \n",
        "        return np.nan\n",
        "    if row[\"datatype\"] == \"PRCP\":\n",
        "        return v * 0.1             # → mm\n",
        "    if row[\"datatype\"] in (\"TMAX\",\"TMIN\"):\n",
        "        return v * 0.1             # → °C\n",
        "    return v\n",
        "\n",
        "raw[\"val_clean\"] = raw.apply(scaled_val, axis=1)\n",
        "\n",
        "# Filter to the analysis window if your raw contains more than needed\n",
        "if \"start_date\" in CONFIG and \"end_date\" in CONFIG:\n",
        "    sd = pd.to_datetime(CONFIG[\"start_date\"], utc=True, errors=\"coerce\")\n",
        "    ed = pd.to_datetime(CONFIG[\"end_date\"],   utc=True, errors=\"coerce\")\n",
        "    raw[\"date\"] = pd.to_datetime(raw[\"date\"], utc=True, errors=\"coerce\")\n",
        "    raw = raw[(raw[\"date\"]>=sd) & (raw[\"date\"]<=ed)]\n",
        "else:\n",
        "    raw[\"date\"] = pd.to_datetime(raw[\"date\"], utc=True, errors=\"coerce\")\n",
        "\n",
        "# 3) Keep only the datatypes we need and one value per (station,date,datatype)\n",
        "keep_types = {\"PRCP\":\"precipitation_mm\", \"TMAX\":\"temperature_max_c\", \"TMIN\":\"temperature_min_c\"}\n",
        "raw = raw[raw[\"datatype\"].isin(keep_types.keys())].copy()\n",
        "\n",
        "# If multiple rows per (station,date,datatype), average them\n",
        "agg = (raw.groupby([\"station_core\",\"date\",\"datatype\"], as_index=False)[\"val_clean\"]\n",
        "          .mean())\n",
        "\n",
        "# 4) Pivot to wide columns\n",
        "wide = (agg.pivot(index=[\"station_core\",\"date\"], columns=\"datatype\", values=\"val_clean\")\n",
        "           .reset_index())\n",
        "# Rename columns to our canonical names\n",
        "wide = wide.rename(columns={k:v for k,v in keep_types.items() if k in wide.columns})\n",
        "\n",
        "# 5) Attach lat/lon from station catalog and clip to CA bbox\n",
        "wide = wide.merge(stations[[\"station_core\",\"lat\",\"lon\"]], on=\"station_core\", how=\"left\")\n",
        "\n",
        "# Clip to CONFIG[\"bbox\"] (California in your setup)\n",
        "bbox = CONFIG[\"bbox\"]\n",
        "minx, miny, maxx, maxy = bbox[\"nwlng\"], bbox[\"selat\"], bbox[\"selng\"], bbox[\"nwlat\"]\n",
        "in_box = (wide[\"lon\"].between(minx, maxx)) & (wide[\"lat\"].between(miny, maxy))\n",
        "wide = wide[in_box].copy()\n",
        "\n",
        "# 6) Final tidy columns + sorts\n",
        "cols_order = [\"station_core\",\"date\",\"lat\",\"lon\",\n",
        "              \"precipitation_mm\",\"temperature_max_c\",\"temperature_min_c\"]\n",
        "for c in cols_order:\n",
        "    if c not in wide.columns: wide[c] = np.nan\n",
        "wide = wide[cols_order].sort_values([\"station_core\",\"date\"])\n",
        "\n",
        "# 7) Save for the superset\n",
        "wide.to_parquet(OUT_PARQ, index=False)\n",
        "wide.to_csv(OUT_CSV, index=False)\n",
        "print(f\"Saved cleaned CDO daily → {OUT_PARQ} (rows={len(wide)}, stations={wide['station_core'].nunique()})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "28ac19ff",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GRID_PATH: /Users/Shared/blueleaflabs/hydropulse/results/grid_3000m_CA_epsg3310.parquet\n",
            "GHCND_PATH: /Users/Shared/blueleaflabs/hydropulse/results/ghcnd_daily_cleaned.parquet\n",
            "SHARDS_DIR: /Users/Shared/blueleaflabs/hydropulse/results/derived/final_daily_shards_prcp\n",
            "FINAL_PATH: /Users/Shared/blueleaflabs/hydropulse/results/final_daily_grid_CA_3000m_epsg3310_20240601_20241031.parquet\n",
            "Grid: 46495 cells | CRS EPSG: 3310 | id col: grid_id\n",
            "Station-day rows: 122769 | stations: 1039 | dates: 153\n",
            "Shards written: 153 | skipped (resume): 0 | total days: 153\n",
            "Saved FINAL: /Users/Shared/blueleaflabs/hydropulse/results/final_daily_grid_CA_3000m_epsg3310_20240601_20241031.parquet\n",
            "Final rows: 7113735 | cells: 46495 | dates: 153\n",
            "             grid_id                      date   prcp_mm  n_used  maxdist_km  \\\n",
            "0  CA3310_3000_0_293 2024-06-01 00:00:00+00:00  0.000000       8   29.701700   \n",
            "1  CA3310_3000_0_293 2024-06-02 00:00:00+00:00  0.001268       8   29.701700   \n",
            "2  CA3310_3000_0_293 2024-06-03 00:00:00+00:00  0.048528       8   29.521121   \n",
            "3  CA3310_3000_0_293 2024-06-04 00:00:00+00:00  0.002183       8   29.701700   \n",
            "4  CA3310_3000_0_293 2024-06-05 00:00:00+00:00  0.000079       8   29.701700   \n",
            "\n",
            "   method  \n",
            "0  idw_k8  \n",
            "1  idw_k8  \n",
            "2  idw_k8  \n",
            "3  idw_k8  \n",
            "4  idw_k8  \n"
          ]
        }
      ],
      "source": [
        "# === HydroPulse | Final Daily Grid Builder (v0: GHCND PRCP only) ===\n",
        "# Produces: CONFIG[\"FINAL_DAILY_FILENAME\"] as a canonical daily grid table:\n",
        "#   grid_id × date → prcp_mm + QC\n",
        "#\n",
        "# Inputs:\n",
        "#   - Grid parquet (EPSG:3310): CONFIG[\"GRID_FILENAME\"]\n",
        "#   - GHCND cleaned station-day parquet: resolve_out_path(CONFIG[\"GHCND_CLEAN_PARQUET_NAME\"])\n",
        "#\n",
        "# Notes for later HeatShield refactor:\n",
        "#   - This cell establishes the common \"final builder\" contract: {grid_id, date, variables..., QC...}\n",
        "#   - Keep the interface stable; only swap/extend source adapters per repo.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "from pathlib import Path\n",
        "from sklearn.neighbors import BallTree\n",
        "\n",
        "# -----------------------------\n",
        "# Paths\n",
        "# -----------------------------\n",
        "OUT_DIR = Path(CONFIG[\"out_dir\"])\n",
        "GRID_PATH = Path(resolve_out_path(CONFIG[\"GRID_FILENAME\"]))\n",
        "GHCND_PATH = Path(resolve_out_path(CONFIG[\"GHCND_CLEAN_PARQUET_NAME\"]))\n",
        "FINAL_PATH = Path(resolve_out_path(CONFIG[\"FINAL_DAILY_FILENAME\"]))\n",
        "\n",
        "# Resume via daily shards\n",
        "SHARDS_DIR = OUT_DIR / \"derived\" / \"final_daily_shards_prcp\"\n",
        "SHARDS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"GRID_PATH:\", GRID_PATH)\n",
        "print(\"GHCND_PATH:\", GHCND_PATH)\n",
        "print(\"SHARDS_DIR:\", SHARDS_DIR)\n",
        "print(\"FINAL_PATH:\", FINAL_PATH)\n",
        "\n",
        "# -----------------------------\n",
        "# Column contract (confirmed)\n",
        "# -----------------------------\n",
        "STATION_COL = \"station_core\"\n",
        "DATE_COL    = \"date\"\n",
        "LAT_COL     = \"lat\"\n",
        "LON_COL     = \"lon\"\n",
        "PRCP_COL_IN = \"precipitation_mm\"\n",
        "\n",
        "# Output column naming for the final table\n",
        "PRCP_COL_OUT = \"prcp_mm\"\n",
        "\n",
        "# -----------------------------\n",
        "# Tunables (can later move to config)\n",
        "# -----------------------------\n",
        "K = int(CONFIG.get(\"PRCP_IDW_K\", 8))                        # k nearest stations\n",
        "HARD_CAP_KM = float(CONFIG.get(\"PRCP_HARD_CAP_KM\", 100.0))  # ignore stations beyond this radius\n",
        "POWER = float(CONFIG.get(\"PRCP_IDW_POWER\", 2.0))            # IDW power\n",
        "\n",
        "OPS_EPSG = int(CONFIG.get(\"OPS_EPSG\", 3310))\n",
        "WGS84_EPSG = int(CONFIG.get(\"WGS84_EPSG\", 4326))\n",
        "\n",
        "# -----------------------------\n",
        "# Helpers (keep these stable across repos)\n",
        "# -----------------------------\n",
        "def ensure_epsg(gdf: gpd.GeoDataFrame, epsg: int) -> gpd.GeoDataFrame:\n",
        "    if gdf.crs is None:\n",
        "        gdf = gdf.set_crs(f\"EPSG:{WGS84_EPSG}\")\n",
        "    if (gdf.crs.to_epsg() or 0) != epsg:\n",
        "        gdf = gdf.to_crs(epsg)\n",
        "    return gdf\n",
        "\n",
        "def ensure_grid_id(grid: gpd.GeoDataFrame) -> tuple[gpd.GeoDataFrame, str]:\n",
        "    for c in [\"grid_id\", \"cell_id\", \"id\"]:\n",
        "        if c in grid.columns:\n",
        "            return grid, c\n",
        "    grid = grid.copy()\n",
        "    grid[\"grid_id\"] = np.arange(len(grid), dtype=np.int32)\n",
        "    return grid, \"grid_id\"\n",
        "\n",
        "def build_balltree_from_points(geom: gpd.GeoSeries) -> BallTree:\n",
        "    xy = np.column_stack([geom.x.values, geom.y.values])\n",
        "    return BallTree(xy, metric=\"euclidean\")\n",
        "\n",
        "def idw(dist_m: np.ndarray, vals: np.ndarray, power: float) -> float:\n",
        "    if np.any(dist_m == 0):\n",
        "        return float(vals[np.argmin(dist_m)])\n",
        "    w = 1.0 / np.power(dist_m, power)\n",
        "    return float(np.sum(w * vals) / np.sum(w))\n",
        "\n",
        "def interpolate_prcp_for_day(grid_centroids: gpd.GeoSeries, stations_day: gpd.GeoDataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Returns a DF aligned to grid_centroids order with columns:\n",
        "      prcp_mm, n_used, maxdist_km, method\n",
        "    \"\"\"\n",
        "    n_cells = len(grid_centroids)\n",
        "    out = pd.DataFrame({\n",
        "        PRCP_COL_OUT: np.full(n_cells, np.nan, dtype=float),\n",
        "        \"n_used\": np.zeros(n_cells, dtype=np.int16),\n",
        "        \"maxdist_km\": np.full(n_cells, np.nan, dtype=float),\n",
        "        \"method\": np.full(n_cells, None, dtype=object),\n",
        "    })\n",
        "\n",
        "    if stations_day is None or len(stations_day) == 0:\n",
        "        return out\n",
        "\n",
        "    # Build tree on station points\n",
        "    tree = build_balltree_from_points(stations_day.geometry)\n",
        "    vals = stations_day[PRCP_COL_IN].to_numpy(dtype=float)\n",
        "\n",
        "    qxy = np.column_stack([grid_centroids.x.values, grid_centroids.y.values])\n",
        "    k_eff = min(K, len(stations_day))\n",
        "    dist_m, idx = tree.query(qxy, k=k_eff)\n",
        "\n",
        "    hard_cap_m = HARD_CAP_KM * 1000.0\n",
        "\n",
        "    for i in range(n_cells):\n",
        "        d = dist_m[i]\n",
        "        j = idx[i]\n",
        "\n",
        "        # Apply hard cap\n",
        "        mask = d <= hard_cap_m\n",
        "        if not np.any(mask):\n",
        "            continue\n",
        "\n",
        "        d_use = d[mask]\n",
        "        v_use = vals[j[mask]]\n",
        "\n",
        "        # Drop NaNs (defensive)\n",
        "        good = np.isfinite(v_use)\n",
        "        d_use = d_use[good]\n",
        "        v_use = v_use[good]\n",
        "        if len(v_use) == 0:\n",
        "            continue\n",
        "\n",
        "        if len(v_use) == 1:\n",
        "            out.at[i, PRCP_COL_OUT] = float(v_use[0])\n",
        "            out.at[i, \"method\"] = \"nearest\"\n",
        "        else:\n",
        "            out.at[i, PRCP_COL_OUT] = idw(d_use, v_use, power=POWER)\n",
        "            out.at[i, \"method\"] = f\"idw_k{len(v_use)}\"\n",
        "\n",
        "        out.at[i, \"n_used\"] = int(len(v_use))\n",
        "        out.at[i, \"maxdist_km\"] = float(np.max(d_use) / 1000.0)\n",
        "\n",
        "    return out\n",
        "\n",
        "# -----------------------------\n",
        "# Load grid (EPSG:3310) and compute centroids\n",
        "# -----------------------------\n",
        "grid = gpd.read_parquet(GRID_PATH)\n",
        "grid = ensure_epsg(grid, OPS_EPSG)\n",
        "grid, GID_COL = ensure_grid_id(grid)\n",
        "centroids = grid.geometry.centroid\n",
        "\n",
        "print(f\"Grid: {len(grid)} cells | CRS EPSG: {grid.crs.to_epsg()} | id col: {GID_COL}\")\n",
        "\n",
        "# -----------------------------\n",
        "# Load cleaned GHCND station-day table\n",
        "# -----------------------------\n",
        "cdo = pd.read_parquet(GHCND_PATH)\n",
        "\n",
        "required = [STATION_COL, DATE_COL, LAT_COL, LON_COL, PRCP_COL_IN]\n",
        "missing = [c for c in required if c not in cdo.columns]\n",
        "if missing:\n",
        "    raise KeyError(f\"Missing required columns in {GHCND_PATH.name}: {missing}. Have: {list(cdo.columns)}\")\n",
        "\n",
        "# Normalize date to UTC day\n",
        "cdo[DATE_COL] = pd.to_datetime(cdo[DATE_COL], utc=True, errors=\"coerce\").dt.normalize()\n",
        "\n",
        "# Filter date window\n",
        "start = pd.to_datetime(CONFIG[\"start_date\"], utc=True).normalize()\n",
        "end = pd.to_datetime(CONFIG[\"end_date\"], utc=True).normalize()\n",
        "cdo = cdo[(cdo[DATE_COL] >= start) & (cdo[DATE_COL] <= end)].copy()\n",
        "\n",
        "# Drop invalid coords / missing precip\n",
        "cdo = cdo[np.isfinite(cdo[LAT_COL]) & np.isfinite(cdo[LON_COL])].copy()\n",
        "cdo = cdo[np.isfinite(cdo[PRCP_COL_IN])].copy()\n",
        "\n",
        "print(\"Station-day rows:\", len(cdo), \"| stations:\", cdo[STATION_COL].nunique(), \"| dates:\", cdo[DATE_COL].nunique())\n",
        "\n",
        "# GeoDataFrame in OPS_EPSG\n",
        "pts = gpd.GeoDataFrame(\n",
        "    cdo,\n",
        "    geometry=gpd.points_from_xy(cdo[LON_COL], cdo[LAT_COL]),\n",
        "    crs=f\"EPSG:{WGS84_EPSG}\"\n",
        ")\n",
        "pts = ensure_epsg(pts, OPS_EPSG)\n",
        "\n",
        "# Pre-group by date for speed (avoid repeated boolean filters)\n",
        "pts_by_date = {d: df for d, df in pts.groupby(DATE_COL)}\n",
        "all_dates = pd.date_range(start=start, end=end, freq=\"D\")\n",
        "\n",
        "# -----------------------------\n",
        "# Daily loop with resume\n",
        "# -----------------------------\n",
        "written = 0\n",
        "skipped = 0\n",
        "\n",
        "for d in all_dates:\n",
        "    tag = d.strftime(\"%Y%m%d\")\n",
        "    shard_path = SHARDS_DIR / f\"final_prcp_{tag}.parquet\"\n",
        "    if shard_path.exists():\n",
        "        skipped += 1\n",
        "        continue\n",
        "\n",
        "    day_pts = pts_by_date.get(d)\n",
        "    interp = interpolate_prcp_for_day(centroids, day_pts)\n",
        "\n",
        "    out = pd.DataFrame({\n",
        "        GID_COL: grid[GID_COL].values,\n",
        "        \"date\": np.full(len(grid), d),\n",
        "    })\n",
        "    out = pd.concat([out, interp], axis=1)\n",
        "\n",
        "    out.to_parquet(shard_path, index=False)\n",
        "    written += 1\n",
        "\n",
        "print(f\"Shards written: {written} | skipped (resume): {skipped} | total days: {len(all_dates)}\")\n",
        "\n",
        "# -----------------------------\n",
        "# Compose FINAL parquet\n",
        "# -----------------------------\n",
        "shards = sorted(SHARDS_DIR.glob(\"final_prcp_*.parquet\"))\n",
        "if not shards:\n",
        "    raise FileNotFoundError(f\"No shards found in {SHARDS_DIR}\")\n",
        "\n",
        "df_final = pd.concat((pd.read_parquet(p) for p in shards), ignore_index=True)\n",
        "\n",
        "df_final[\"date\"] = pd.to_datetime(df_final[\"date\"], utc=True).dt.normalize()\n",
        "df_final = df_final.sort_values([GID_COL, \"date\"]).reset_index(drop=True)\n",
        "\n",
        "FINAL_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
        "df_final.to_parquet(FINAL_PATH, index=False)\n",
        "\n",
        "print(\"Saved FINAL:\", FINAL_PATH)\n",
        "print(\"Final rows:\", len(df_final), \"| cells:\", df_final[GID_COL].nunique(), \"| dates:\", df_final[\"date\"].nunique())\n",
        "print(df_final.head())"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "blueleaflabs",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
