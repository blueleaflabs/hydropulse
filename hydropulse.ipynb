{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d0c1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "import os, json, math, io, zipfile, time, re, shutil, glob, pathlib, sys, subprocess, shlex, tempfile, warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime as dt, timedelta, timezone, date, datetime\n",
    "from dateutil import parser as dateparser\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from urllib.parse import urljoin, quote\n",
    "from functools import reduce\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import pytz\n",
    "import xarray as xr\n",
    "import rasterio\n",
    "from rasterio.mask import mask\n",
    "\n",
    "import shapely\n",
    "from shapely import ops\n",
    "from shapely.geometry import Point, Polygon, box, mapping\n",
    "from shapely.ops import unary_union, transform as shp_transform\n",
    "\n",
    "from pyproj import Transformer\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import folium\n",
    "import ee  # Earth Engine\n",
    "\n",
    "from sklearn.neighbors import BallTree\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import cdist\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "import yaml\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "\n",
    "def skip_if_exists(path: str) -> bool:\n",
    "    return os.path.exists(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f568ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration loader (shared HeatShield/HydroPulse) ---\n",
    "from pathlib import Path\n",
    "import os\n",
    "import re\n",
    "import yaml\n",
    "\n",
    "def load_env_file(path: Path) -> dict:\n",
    "    env = {}\n",
    "    if not path.exists():\n",
    "        return env\n",
    "    for line in path.read_text().splitlines():\n",
    "        line = line.strip()\n",
    "        if not line or line.startswith(\"#\") or \"=\" not in line:\n",
    "            continue\n",
    "        key, val = line.split(\"=\", 1)\n",
    "        env[key.strip()] = val.strip().strip('\"').strip(\"'\")\n",
    "    return env\n",
    "\n",
    "def apply_env_overrides() -> None:\n",
    "    env = load_env_file(Path(\".env\"))\n",
    "    for k, v in env.items():\n",
    "        if v:\n",
    "            os.environ[k] = v\n",
    "\n",
    "def _fmt_yyyymmdd(s: str) -> str:\n",
    "    # expects YYYY-MM-DD\n",
    "    return re.sub(r\"-\", \"\", s.strip())\n",
    "\n",
    "def _render_template(tpl: str, *, start: str, end: str, res_m: int, epsg: int, region: str) -> str:\n",
    "    return tpl.format(\n",
    "        start=_fmt_yyyymmdd(start),\n",
    "        end=_fmt_yyyymmdd(end),\n",
    "        res_m=int(res_m),\n",
    "        epsg=int(epsg),\n",
    "        region=str(region),\n",
    "    )\n",
    "\n",
    "def _resolve_out_dir(project_dir: Path, out_dir_value: str | None) -> Path:\n",
    "    out_dir_value = out_dir_value or \"results\"\n",
    "    p = Path(out_dir_value)\n",
    "    return (p if p.is_absolute() else (project_dir / p)).resolve()\n",
    "\n",
    "def _resolve_pathlike_keys(cfg: dict, out_dir: Path) -> None:\n",
    "    \"\"\"\n",
    "    Resolve config values that look like relative file paths under out_dir.\n",
    "    Rule: if key ends with _FILENAME, _PATH, _ZIP, _TIF_NAME, _CSV_NAME, _PARQUET_NAME, _TXT_NAME\n",
    "    and the value is a relative path, make it absolute under out_dir.\n",
    "    \"\"\"\n",
    "    suffixes = (\n",
    "        \"_FILENAME\", \"_PATH\", \"_ZIP\",\n",
    "        \"_TIF_NAME\", \"_CSV_NAME\", \"_PARQUET_NAME\", \"_TXT_NAME\",\n",
    "        \"_NETCDF_NAME\", \"_ZARR_NAME\"\n",
    "    )\n",
    "    for k, v in list(cfg.items()):\n",
    "        if not isinstance(v, str):\n",
    "            continue\n",
    "        if not k.endswith(suffixes):\n",
    "            continue\n",
    "        p = Path(v)\n",
    "        if p.is_absolute():\n",
    "            cfg[k] = str(p)\n",
    "        else:\n",
    "            cfg[k] = str((out_dir / p).resolve())\n",
    "\n",
    "# Fail fast unless PROJECT_DIR is explicitly provided.\n",
    "apply_env_overrides()\n",
    "\n",
    "PROJECT_DIR = os.environ.get(\"PROJECT_DIR\")\n",
    "if not PROJECT_DIR:\n",
    "    raise FileNotFoundError(\n",
    "        \"PROJECT_DIR is not set. Add PROJECT_DIR to .env or environment variables.\"\n",
    "    )\n",
    "\n",
    "PROJECT_DIR = Path(PROJECT_DIR).expanduser().resolve()\n",
    "CONFIG_PATH = PROJECT_DIR / \"config\" / \"config.yaml\"\n",
    "if not CONFIG_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Missing config file: {CONFIG_PATH}\")\n",
    "\n",
    "CONFIG = yaml.safe_load(CONFIG_PATH.read_text()) or {}\n",
    "\n",
    "# Set out_dir to an absolute path early.\n",
    "OUT_DIR = _resolve_out_dir(PROJECT_DIR, CONFIG.get(\"out_dir\", \"results\"))\n",
    "CONFIG[\"out_dir\"] = str(OUT_DIR)\n",
    "\n",
    "# Optional: Apply env overrides ONLY for keys that exist in config.yaml.\n",
    "# This prevents HeatShield-specific lists in shared code.\n",
    "for key in list(CONFIG.keys()):\n",
    "    env_val = os.environ.get(key)\n",
    "    if env_val:\n",
    "        CONFIG[key] = env_val\n",
    "\n",
    "# Also allow a small, explicit allowlist for common optional overrides across projects.\n",
    "for key in [\"PURPLEAIR_SENSOR_INDEX\"]:  # harmless if absent in HydroPulse config\n",
    "    env_val = os.environ.get(key)\n",
    "    if env_val:\n",
    "        CONFIG[key] = env_val\n",
    "\n",
    "# Render FINAL_DAILY_FILENAME from template if provided.\n",
    "# Keeps downstream code stable: always refer to CONFIG[\"FINAL_DAILY_FILENAME\"].\n",
    "if \"FINAL_DAILY_FILENAME_TEMPLATE\" in CONFIG:\n",
    "    region = CONFIG.get(\"region\", \"CA\")\n",
    "    start_date = CONFIG[\"start_date\"]\n",
    "    end_date = CONFIG[\"end_date\"]\n",
    "    res_m = CONFIG.get(\"grid_resolution_m\", 3000)\n",
    "    epsg = CONFIG.get(\"OPS_EPSG\", CONFIG.get(\"CA_ALBERS_EPSG\", 3310))\n",
    "    CONFIG[\"FINAL_DAILY_FILENAME\"] = _render_template(\n",
    "        CONFIG[\"FINAL_DAILY_FILENAME_TEMPLATE\"],\n",
    "        start=start_date,\n",
    "        end=end_date,\n",
    "        res_m=res_m,\n",
    "        epsg=epsg,\n",
    "        region=region,\n",
    "    )\n",
    "\n",
    "# Resolve pathlike keys under out_dir (only for keys that exist).\n",
    "_resolve_pathlike_keys(CONFIG, OUT_DIR)\n",
    "\n",
    "# Create common directories only if they are referenced in config.\n",
    "# (Avoid hardcoding \"manual\" for HydroPulse.)\n",
    "for k, v in CONFIG.items():\n",
    "    if k.endswith(\"_DIRNAME\") and isinstance(v, str):\n",
    "        os.makedirs(Path(CONFIG[\"out_dir\"]) / v, exist_ok=True)\n",
    "\n",
    "# EPSG constants (configurable)\n",
    "WGS84_EPSG = int(CONFIG.get(\"WGS84_EPSG\", 4326))\n",
    "CA_ALBERS_EPSG = int(CONFIG.get(\"CA_ALBERS_EPSG\", 3310))\n",
    "OPS_EPSG = int(CONFIG.get(\"OPS_EPSG\", CA_ALBERS_EPSG))\n",
    "\n",
    "# Set working directory\n",
    "os.chdir(PROJECT_DIR)\n",
    "\n",
    "print(f\"Config loaded from {CONFIG_PATH}\")\n",
    "print(f\"Output dir: {CONFIG['out_dir']}\")\n",
    "print(f\"Final daily filename: {CONFIG.get('FINAL_DAILY_FILENAME')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19be8061",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def resolve_out_path(path_str: str) -> str:\n",
    "    \"\"\"\n",
    "    Resolve a path string relative to CONFIG['out_dir'] unless already absolute.\n",
    "    Returns an absolute string path.\n",
    "    \"\"\"\n",
    "    p = Path(path_str)\n",
    "    if p.is_absolute():\n",
    "        return str(p)\n",
    "    return str((Path(CONFIG[\"out_dir\"]) / p).resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea9e2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Ensure California boundary and build 3 km grid clipped to land ---\n",
    "\n",
    "\n",
    "# Config\n",
    "res_m = int(CONFIG.get(\"grid_resolution_m\", 3000))\n",
    "out_epsg = int(CONFIG.get(\"crs_epsg\", 4326))\n",
    "out_dir = CONFIG[\"out_dir\"]; os.makedirs(out_dir, exist_ok=True)\n",
    "inset_buffer_m = int(CONFIG.get(\"coast_inset_m\", 0))  # e.g. 5000\n",
    "boundary_path = CONFIG.get(\"ca_boundary_path\", None)\n",
    "\n",
    "# 1) Ensure boundary: download Census cartographic boundary if missing\n",
    "if not boundary_path or not os.path.exists(boundary_path):\n",
    "    states_zip = os.path.join(out_dir, \"cb_2023_us_state_20m.zip\")\n",
    "    if not os.path.exists(states_zip):\n",
    "        url = CONFIG[\"CENSUS_STATES_ZIP_URL\"]\n",
    "        r = requests.get(url, timeout=int(CONFIG.get(\"CENSUS_STATES_TIMEOUT\", 120))); r.raise_for_status()\n",
    "        with open(states_zip, \"wb\") as f: f.write(r.content)\n",
    "    # Read from zip directly and select California\n",
    "    states = gpd.read_file(f\"zip://{states_zip}\")\n",
    "    if states.empty:\n",
    "        raise ValueError(\"Census states file loaded empty.\")\n",
    "    ca = states[states[\"STATEFP\"].astype(str).str.zfill(2).eq(\"06\")][[\"geometry\"]]\n",
    "    if ca.empty:\n",
    "        raise ValueError(\"California polygon not found in Census states file.\")\n",
    "    boundary_path = os.path.join(out_dir, \"california_boundary.gpkg\")\n",
    "    ca.to_file(boundary_path, driver=\"GPKG\")\n",
    "    CONFIG[\"ca_boundary_path\"] = boundary_path  # persist for later cells\n",
    "\n",
    "# 2) Load boundary, dissolve, project, optional inward buffer\n",
    "b = gpd.read_file(boundary_path)\n",
    "if b.crs is None: raise ValueError(\"Boundary file has no CRS.\")\n",
    "b = b[[\"geometry\"]].copy()\n",
    "b = b.to_crs(CA_ALBERS_EPSG)\n",
    "b = gpd.GeoDataFrame(geometry=[b.unary_union], crs=f\"EPSG:{CA_ALBERS_EPSG}\")\n",
    "if inset_buffer_m > 0:\n",
    "    b.geometry = b.buffer(-inset_buffer_m)\n",
    "    b = gpd.GeoDataFrame(geometry=[b.unary_union], crs=f\"EPSG:{CA_ALBERS_EPSG}\")\n",
    "\n",
    "# 3) Build snapped rectilinear grid over boundary bounds in EPSG:3310\n",
    "minx, miny, maxx, maxy = b.total_bounds\n",
    "snap_down = lambda v, s: np.floor(v/s)*s\n",
    "snap_up   = lambda v, s: np.ceil(v/s)*s\n",
    "minx, miny = snap_down(minx, res_m), snap_down(miny, res_m)\n",
    "maxx, maxy = snap_up(maxx, res_m), snap_up(maxy, res_m)\n",
    "\n",
    "xs = np.arange(minx, maxx, res_m)\n",
    "ys = np.arange(miny, maxy, res_m)\n",
    "n_rect = len(xs)*len(ys)\n",
    "if n_rect > 3_500_000:\n",
    "    raise MemoryError(f\"Grid too large ({n_rect:,}). Increase res_m or tile the state.\")\n",
    "\n",
    "cells, col_i, row_j = [], [], []\n",
    "for j, y in enumerate(ys):\n",
    "    for i, x in enumerate(xs):\n",
    "        cells.append(box(x, y, x+res_m, y+res_m)); col_i.append(i); row_j.append(j)\n",
    "\n",
    "gdf_proj = gpd.GeoDataFrame({\"col_i\": np.int32(col_i), \"row_j\": np.int32(row_j)},\n",
    "                            geometry=cells, crs=f\"EPSG:{CA_ALBERS_EPSG}\")\n",
    "gdf_proj[\"cell_area_m2\"] = float(res_m)*float(res_m)\n",
    "gdf_proj[\"grid_id\"] = f\"CA3310_{res_m}_\" + gdf_proj[\"col_i\"].astype(str) + \"_\" + gdf_proj[\"row_j\"].astype(str)\n",
    "\n",
    "# 4) Strict land clip and land fraction\n",
    "gdf_proj = gpd.sjoin(gdf_proj, b, how=\"inner\", predicate=\"intersects\").drop(columns=[\"index_right\"])\n",
    "inter = gpd.overlay(gdf_proj[[\"grid_id\",\"geometry\"]], b, how=\"intersection\", keep_geom_type=True)\n",
    "inter[\"land_area_m2\"] = inter.geometry.area\n",
    "land = inter[[\"grid_id\",\"land_area_m2\"]].groupby(\"grid_id\", as_index=False).sum()\n",
    "gdf_proj = gdf_proj.merge(land, on=\"grid_id\", how=\"left\")\n",
    "gdf_proj[\"land_area_m2\"] = gdf_proj[\"land_area_m2\"].fillna(0.0)\n",
    "gdf_proj[\"land_frac\"] = (gdf_proj[\"land_area_m2\"] / gdf_proj[\"cell_area_m2\"]).clip(0,1)\n",
    "gdf_proj = gdf_proj[gdf_proj[\"land_frac\"] > 0].reset_index(drop=True)\n",
    "\n",
    "# 5) Reproject to requested output CRS and save\n",
    "grid_gdf = gdf_proj.to_crs(out_epsg)\n",
    "\n",
    "parquet_path = os.path.join(out_dir, f\"grid_{res_m}m_CA.parquet\")\n",
    "grid_gdf.to_parquet(parquet_path, index=False)\n",
    "\n",
    "geojson_path = os.path.join(out_dir, f\"grid_{res_m}m_CA_head10.geojson\")\n",
    "grid_gdf.head(10).to_file(geojson_path, driver=\"GeoJSON\")\n",
    "\n",
    "# Diagnostics\n",
    "cell_area_km2 = (res_m/1000.0)**2\n",
    "eff_land_km2 = float((grid_gdf.get(\"land_frac\",1.0) * cell_area_km2).sum())\n",
    "print(f\"Saved: {parquet_path}\")\n",
    "print(f\"Cells: {len(grid_gdf):,}\")\n",
    "print(f\"Effective land area ≈ {round(eff_land_km2):,} km²\")\n",
    "print(f\"Implied cell size ≈ {round((eff_land_km2/len(grid_gdf))**0.5,2)} km\")\n",
    "\n",
    "grid_gdf.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026fd848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Persist config + save grid (3310 ops copy, 4326 preview) + write metadata ---\n",
    "\n",
    "# Inputs assumed from prior cell:\n",
    "# - grid_gdf            : current grid GeoDataFrame (any CRS)\n",
    "# - CONFIG              : dict with out_dir, grid_resolution_m, crs_epsg, ca_boundary_path\n",
    "# - CA_ALBERS_EPSG=3310 : defined earlier\n",
    "\n",
    "out_dir = CONFIG[\"out_dir\"]; os.makedirs(out_dir, exist_ok=True)\n",
    "res_m = int(CONFIG.get(\"grid_resolution_m\", 3000))\n",
    "out_epsg = int(CONFIG.get(\"crs_epsg\", 4326))\n",
    "boundary_path = CONFIG.get(\"ca_boundary_path\")\n",
    "\n",
    "# 1) Persist boundary path back to CONFIG \n",
    "if not boundary_path or not os.path.exists(boundary_path):\n",
    "    raise FileNotFoundError(\"CONFIG['ca_boundary_path'] missing or invalid. Rebuild boundary.\")\n",
    "CONFIG[\"ca_boundary_path\"] = boundary_path\n",
    "\n",
    "config_runtime_path = os.path.join(out_dir, \"config_runtime.json\")\n",
    "with open(config_runtime_path, \"w\") as f:\n",
    "    json.dump(CONFIG, f, indent=2)\n",
    "print(\"Saved:\", config_runtime_path)\n",
    "\n",
    "# 2) Ensure we have an EPSG:3310 version for spatial ops\n",
    "if grid_gdf.crs is None:\n",
    "    raise ValueError(\"grid_gdf has no CRS. Rebuild grid.\")\n",
    "grid_3310 = grid_gdf.to_crs(3310) if grid_gdf.crs.to_epsg() != 3310 else grid_gdf\n",
    "\n",
    "# 3) Save operational GeoParquet in 3310 + lightweight WGS84 preview\n",
    "parquet_3310 = os.path.join(out_dir, f\"grid_{res_m}m_CA_epsg3310.parquet\")\n",
    "grid_3310.to_parquet(parquet_3310, index=False)\n",
    "print(\"Saved:\", parquet_3310, \"| cells:\", len(grid_3310))\n",
    "\n",
    "# Optional small preview in 4326 for quick map checks\n",
    "preview_4326 = grid_3310.to_crs(4326).head(500)  # cap to avoid huge files\n",
    "geojson_preview = os.path.join(out_dir, f\"grid_{res_m}m_CA_head500_epsg4326.geojson\")\n",
    "preview_4326.to_file(geojson_preview, driver=\"GeoJSON\")\n",
    "print(\"Saved:\", geojson_preview)\n",
    "\n",
    "# 4) Compute and save metadata\n",
    "cell_area_km2 = (res_m/1000.0)**2\n",
    "effective_land_km2 = float((grid_3310.get(\"land_frac\", 1.0) * cell_area_km2).sum())\n",
    "implied_cell_km = float((effective_land_km2 / len(grid_3310))**0.5)\n",
    "minx, miny, maxx, maxy = grid_3310.total_bounds\n",
    "bbox_km = ((maxx-minx)/1000.0, (maxy-miny)/1000.0)\n",
    "\n",
    "meta = {\n",
    "    \"timestamp_utc\": dt.utcnow().isoformat(timespec=\"seconds\") + \"Z\",\n",
    "    \"grid_resolution_m\": res_m,\n",
    "    \"crs_ops_epsg\": 3310,\n",
    "    \"crs_export_default_epsg\": out_epsg,\n",
    "    \"cells\": int(len(grid_3310)),\n",
    "    \"effective_land_area_km2\": round(effective_land_km2, 2),\n",
    "    \"implied_cell_km\": round(implied_cell_km, 4),\n",
    "    \"bbox_km_width_height\": [round(bbox_km[0], 2), round(bbox_km[1], 2)],\n",
    "    \"has_land_frac\": bool(\"land_frac\" in grid_3310.columns),\n",
    "    \"boundary_path\": boundary_path,\n",
    "    \"parquet_3310_path\": parquet_3310,\n",
    "    \"geojson_preview_4326_path\": geojson_preview,\n",
    "}\n",
    "\n",
    "meta_path = os.path.join(out_dir, f\"grid_{res_m}m_CA_meta.json\")\n",
    "with open(meta_path, \"w\") as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "print(\"Saved:\", meta_path)\n",
    "meta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ff5b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CDO data fetch and processing functions\n",
    "# repeat some variables for clarity\n",
    "OUT_DIR = CONFIG[\"out_dir\"]\n",
    "RAW_DIR = os.path.join(OUT_DIR, CONFIG[\"CDO_RAW_DIRNAME\"])\n",
    "CLEAN_DIR = os.path.join(OUT_DIR, CONFIG[\"CDO_CLEAN_DIRNAME\"])\n",
    "os.makedirs(RAW_DIR, exist_ok=True); os.makedirs(CLEAN_DIR, exist_ok=True)\n",
    "\n",
    "def month_windows(start_date, end_date):\n",
    "    s = dt.fromisoformat(start_date).date().replace(day=1)\n",
    "    e = dt.fromisoformat(end_date).date()\n",
    "    cur = s\n",
    "    while cur <= e:\n",
    "        nxt = (cur + relativedelta(months=1)) - relativedelta(days=1)\n",
    "        yield cur.isoformat(), min(nxt, e).isoformat()\n",
    "        cur = (cur + relativedelta(months=1)).replace(day=1)\n",
    "\n",
    "def parse_attributes(attr):\n",
    "    parts = (attr or \"\").split(\",\"); parts += [\"\"] * (4 - len(parts))\n",
    "    mflag, qflag, sflag, obs_hhmm = parts[:4]\n",
    "    return mflag or None, qflag or None, sflag or None, obs_hhmm or None\n",
    "\n",
    "def fetch_cdo_page(session, url, headers, params, max_retries=None, base_delay=None, timeout=None):\n",
    "    if max_retries is None:\n",
    "        max_retries = int(CONFIG.get(\"CDO_MAX_RETRIES\", 6))\n",
    "    if base_delay is None:\n",
    "        base_delay = float(CONFIG.get(\"CDO_BACKOFF_BASE\", 0.8))\n",
    "    if timeout is None:\n",
    "        timeout = int(CONFIG.get(\"CDO_TIMEOUT\", 180))\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            r = session.get(url, headers=headers, params=params, timeout=timeout)\n",
    "            if r.status_code in (429, 500, 502, 503, 504):\n",
    "                raise requests.HTTPError(f\"{r.status_code} retry\")\n",
    "            r.raise_for_status()\n",
    "            return r.json()\n",
    "        except Exception:\n",
    "            if attempt == max_retries - 1:\n",
    "                raise\n",
    "            time.sleep(base_delay * (2 ** attempt))\n",
    "\n",
    "\n",
    "def cdo_stream_monthly(datasetid, locationid, startdate, enddate, datatypes, token,\n",
    "                       units=\"standard\", page_limit=1000, force=False):\n",
    "    url = CONFIG[\"CDO_BASE_URL\"]\n",
    "    headers = {\"token\": token}\n",
    "    session = requests.Session()\n",
    "    written = []\n",
    "\n",
    "    for dtid in datatypes:\n",
    "        for ms, me in month_windows(startdate, enddate):\n",
    "            out_csv = os.path.join(RAW_DIR, f\"ghcnd_{dtid}_{ms[:7]}.csv\")\n",
    "            if skip_if_exists(out_csv) and not force:\n",
    "                # resume: skip existing month-datatype file\n",
    "                written.append(out_csv); continue\n",
    "\n",
    "            frames = []\n",
    "            offset = 1\n",
    "            while True:\n",
    "                params = {\n",
    "                    \"datasetid\": datasetid, \"locationid\": locationid,\n",
    "                    \"startdate\": ms, \"enddate\": me,\n",
    "                    \"datatypeid\": dtid, \"units\": units,\n",
    "                    \"limit\": page_limit, \"offset\": offset\n",
    "                }\n",
    "                js = fetch_cdo_page(session, url, headers, params)\n",
    "                rows = js.get(\"results\", [])\n",
    "                if not rows:\n",
    "                    break\n",
    "                frames.append(pd.json_normalize(rows))\n",
    "                if len(rows) < page_limit:\n",
    "                    break\n",
    "                offset += page_limit\n",
    "                time.sleep(0.15)  # gentle pacing\n",
    "\n",
    "            if frames:\n",
    "                df = pd.concat(frames, ignore_index=True)\n",
    "                # normalize\n",
    "                df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\").dt.date\n",
    "                parsed = df[\"attributes\"].apply(parse_attributes)\n",
    "                df[[\"mflag\",\"qflag\",\"sflag\",\"obs_hhmm\"]] = pd.DataFrame(parsed.tolist(), index=df.index)\n",
    "                # scale tenths\n",
    "                scale = {\"PRCP\": 0.1, \"TMAX\": 0.1, \"TMIN\": 0.1}\n",
    "                df[\"datatype\"] = df[\"datatype\"].astype(str)\n",
    "                df[\"value\"] = pd.to_numeric(df[\"value\"], errors=\"coerce\")\n",
    "                df[\"value_scaled\"] = df.apply(lambda r: r[\"value\"] * scale.get(r[\"datatype\"], 1.0), axis=1)\n",
    "                # write monthly raw\n",
    "                df[[\"date\",\"datatype\",\"station\",\"attributes\",\"mflag\",\"qflag\",\"sflag\",\"obs_hhmm\",\"value\",\"value_scaled\"]].to_csv(out_csv, index=False)\n",
    "                written.append(out_csv)\n",
    "            else:\n",
    "                # create an empty file with header to mark completion\n",
    "                with open(out_csv, \"w\", newline=\"\") as f:\n",
    "                    w = csv.writer(f); w.writerow([\"date\",\"datatype\",\"station\",\"attributes\",\"mflag\",\"qflag\",\"sflag\",\"obs_hhmm\",\"value\",\"value_scaled\"])\n",
    "                written.append(out_csv)\n",
    "    return written\n",
    "\n",
    "def build_clean_wide():\n",
    "    # read all monthly raw files and assemble cleaned wide once\n",
    "    files = sorted([os.path.join(RAW_DIR, f) for f in os.listdir(RAW_DIR) if f.endswith(\".csv\")])\n",
    "    if not files:\n",
    "        return None\n",
    "    df = pd.concat((pd.read_csv(f, dtype={\"datatype\":str,\"station\":str}) for f in files), ignore_index=True)\n",
    "    # convert types back\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"]).dt.date\n",
    "    # keep good qflag\n",
    "    df = df[(df[\"qflag\"].isna()) | (df[\"qflag\"]==\"\")]\n",
    "    wide = (\n",
    "        df.pivot_table(index=[\"station\",\"date\"], columns=\"datatype\", values=\"value_scaled\", aggfunc=\"mean\")\n",
    "          .reset_index()\n",
    "          .rename(columns={\"date\":\"obs_date\",\"PRCP\":\"precipitation_mm\",\"TMAX\":\"temperature_max_c\",\"TMIN\":\"temperature_min_c\"})\n",
    "          .sort_values([\"obs_date\",\"station\"])\n",
    "    )\n",
    "    # attach obs time from PRCP\n",
    "    prcp_times = df[df[\"datatype\"]==\"PRCP\"][[\"station\",\"date\",\"obs_hhmm\"]].drop_duplicates().rename(columns={\"date\":\"obs_date\"})\n",
    "    wide = wide.merge(prcp_times, on=[\"station\",\"obs_date\"], how=\"left\")\n",
    "    raw_all = os.path.join(OUT_DIR, \"ghcnd_daily_raw_all.csv\")\n",
    "    wide_all = os.path.join(OUT_DIR, \"ghcnd_daily_wide.csv\")\n",
    "    df.to_csv(raw_all, index=False)\n",
    "    wide.to_csv(wide_all, index=False)\n",
    "    return raw_all, wide_all, len(df), len(wide), wide[\"station\"].nunique(), wide[\"obs_date\"].nunique()\n",
    "\n",
    "# ---- Run statewide with resume capability ----\n",
    "token = os.environ.get(\"CDO_TOKEN\") or CONFIG.get(\"CDO_TOKEN\", \"\")\n",
    "if token and token != \"YOUR_NCEI_CDO_TOKEN\":\n",
    "    written = cdo_stream_monthly(\n",
    "        datasetid=\"GHCND\",\n",
    "        locationid=\"FIPS:06\",                      # California statewide\n",
    "        startdate=CONFIG[\"start_date\"],\n",
    "        enddate=CONFIG[\"end_date\"],\n",
    "        datatypes=[\"TMAX\",\"TMIN\",\"PRCP\"],\n",
    "        token=token,\n",
    "        units=\"standard\",\n",
    "        page_limit=1000,\n",
    "        force=False                                 # set True to re-download\n",
    "    )\n",
    "    print(f\"Monthly files written: {len(written)} → {RAW_DIR}\")\n",
    "\n",
    "    res = build_clean_wide()\n",
    "    if res:\n",
    "        raw_all, wide_all, n_raw, n_wide, n_stn, n_dates = res\n",
    "        print(f\"Saved raw:  {raw_all}\")\n",
    "        print(f\"Saved wide: {wide_all}\")\n",
    "        print(f\"Counts → raw: {n_raw} | wide: {n_wide} | stations: {n_stn} | dates: {n_dates}\")\n",
    "else:\n",
    "    print(\"Skipping CDO (missing CDO token).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dc4706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === GHCND DAILY: raw (long) -> cleaned (wide with lat/lon in bbox) ===\n",
    "# Input  (from your earlier step):  results/ghcnd_daily_raw_all.csv  (long form)\n",
    "# Output (used by superset):        results/ghcnd_daily_cleaned.parquet  (wide per station-day with lat/lon)\n",
    "\n",
    "# Need to do this because we aren't getting proper \"joins\" in our superset setup.\n",
    "\n",
    "\n",
    "BASE = CONFIG[\"out_dir\"]\n",
    "RAW = resolve_out_path(CONFIG[\"GHCND_RAW_CSV_NAME\"])\n",
    "OUT_PARQ = resolve_out_path(CONFIG[\"GHCND_CLEAN_PARQUET_NAME\"])\n",
    "OUT_CSV = resolve_out_path(CONFIG[\"GHCND_CLEAN_CSV_NAME\"])\n",
    "\n",
    "assert os.path.exists(RAW), f\"Missing raw GHCND file: {RAW}\"\n",
    "\n",
    "# 1) Ensure we have a station catalog with lat/lon\n",
    "#    Prefer a local copy if you already saved one; otherwise download NOAA's reference once.\n",
    "CAT_DIR = os.path.join(BASE, CONFIG[\"MANUAL_DIRNAME\"]); os.makedirs(CAT_DIR, exist_ok=True)\n",
    "CAT_TXT = os.path.join(CAT_DIR, CONFIG[\"GHCND_STATIONS_TXT_NAME\"])\n",
    "\n",
    "if not os.path.exists(CAT_TXT):\n",
    "    url = CONFIG[\"GHCND_STATIONS_URL\"]\n",
    "    r = requests.get(url, timeout=int(CONFIG.get(\"CENSUS_STATES_TIMEOUT\", 120))); r.raise_for_status()\n",
    "    with open(CAT_TXT, \"wb\") as f: f.write(r.content)\n",
    "\n",
    "# Parse ghcnd-stations.txt (fixed-width)\n",
    "# Columns per docs: ID(1-11), LAT(13-20), LON(22-30), ELEV(32-37), STATE(39-40), NAME(42-71) ...\n",
    "def parse_stations(path):\n",
    "    recs = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            if len(line) < 40: \n",
    "                continue\n",
    "            sid = line[0:11].strip()\n",
    "            try:\n",
    "                lat = float(line[12:20].strip())\n",
    "                lon = float(line[21:30].strip())\n",
    "            except ValueError:\n",
    "                continue\n",
    "            state = line[38:40].strip()\n",
    "            name  = line[41:71].strip()\n",
    "            recs.append((sid, lat, lon, state, name))\n",
    "    return pd.DataFrame(recs, columns=[\"station_core\",\"lat\",\"lon\",\"state\",\"name\"])\n",
    "\n",
    "stations = parse_stations(CAT_TXT)\n",
    "\n",
    "# 2) Load your raw long-form CDO file\n",
    "# Expected columns seen in your sample:\n",
    "# ['attributes','datatype','date','mflag','obs_hhmm','qflag','sflag','station','value','value_scaled']\n",
    "raw = pd.read_csv(RAW, low_memory=False)\n",
    "\n",
    "# Normalize station key: raw uses \"GHCND:USW00023232\" → core \"USW00023232\"\n",
    "raw[\"station_core\"] = raw[\"station\"].astype(str).str.replace(\"^GHCND:\", \"\", regex=True)\n",
    "\n",
    "# Pick a numeric value column: prefer value_scaled if present; else scale GHCND native units.\n",
    "# GHCND native: PRCP = tenths of mm, TMAX/TMIN = tenths of °C.\n",
    "have_scaled = \"value_scaled\" in raw.columns\n",
    "def scaled_val(row):\n",
    "    if have_scaled and pd.notna(row[\"value_scaled\"]):\n",
    "        return float(row[\"value_scaled\"])\n",
    "    v = pd.to_numeric(row[\"value\"], errors=\"coerce\")\n",
    "    if pd.isna(v): \n",
    "        return np.nan\n",
    "    if row[\"datatype\"] == \"PRCP\":\n",
    "        return v * 0.1             # → mm\n",
    "    if row[\"datatype\"] in (\"TMAX\",\"TMIN\"):\n",
    "        return v * 0.1             # → °C\n",
    "    return v\n",
    "\n",
    "raw[\"val_clean\"] = raw.apply(scaled_val, axis=1)\n",
    "\n",
    "# Filter to the analysis window if your raw contains more than needed\n",
    "if \"start_date\" in CONFIG and \"end_date\" in CONFIG:\n",
    "    sd = pd.to_datetime(CONFIG[\"start_date\"], utc=True, errors=\"coerce\")\n",
    "    ed = pd.to_datetime(CONFIG[\"end_date\"],   utc=True, errors=\"coerce\")\n",
    "    raw[\"date\"] = pd.to_datetime(raw[\"date\"], utc=True, errors=\"coerce\")\n",
    "    raw = raw[(raw[\"date\"]>=sd) & (raw[\"date\"]<=ed)]\n",
    "else:\n",
    "    raw[\"date\"] = pd.to_datetime(raw[\"date\"], utc=True, errors=\"coerce\")\n",
    "\n",
    "# 3) Keep only the datatypes we need and one value per (station,date,datatype)\n",
    "keep_types = {\"PRCP\":\"precipitation_mm\", \"TMAX\":\"temperature_max_c\", \"TMIN\":\"temperature_min_c\"}\n",
    "raw = raw[raw[\"datatype\"].isin(keep_types.keys())].copy()\n",
    "\n",
    "# If multiple rows per (station,date,datatype), average them\n",
    "agg = (raw.groupby([\"station_core\",\"date\",\"datatype\"], as_index=False)[\"val_clean\"]\n",
    "          .mean())\n",
    "\n",
    "# 4) Pivot to wide columns\n",
    "wide = (agg.pivot(index=[\"station_core\",\"date\"], columns=\"datatype\", values=\"val_clean\")\n",
    "           .reset_index())\n",
    "# Rename columns to our canonical names\n",
    "wide = wide.rename(columns={k:v for k,v in keep_types.items() if k in wide.columns})\n",
    "\n",
    "# 5) Attach lat/lon from station catalog and clip to CA bbox\n",
    "wide = wide.merge(stations[[\"station_core\",\"lat\",\"lon\"]], on=\"station_core\", how=\"left\")\n",
    "\n",
    "# Clip to CONFIG[\"bbox\"] (California in your setup)\n",
    "bbox = CONFIG[\"bbox\"]\n",
    "minx, miny, maxx, maxy = bbox[\"nwlng\"], bbox[\"selat\"], bbox[\"selng\"], bbox[\"nwlat\"]\n",
    "in_box = (wide[\"lon\"].between(minx, maxx)) & (wide[\"lat\"].between(miny, maxy))\n",
    "wide = wide[in_box].copy()\n",
    "\n",
    "# 6) Final tidy columns + sorts\n",
    "cols_order = [\"station_core\",\"date\",\"lat\",\"lon\",\n",
    "              \"precipitation_mm\",\"temperature_max_c\",\"temperature_min_c\"]\n",
    "for c in cols_order:\n",
    "    if c not in wide.columns: wide[c] = np.nan\n",
    "wide = wide[cols_order].sort_values([\"station_core\",\"date\"])\n",
    "\n",
    "# 7) Save for the superset\n",
    "wide.to_parquet(OUT_PARQ, index=False)\n",
    "wide.to_csv(OUT_CSV, index=False)\n",
    "print(f\"Saved cleaned CDO daily → {OUT_PARQ} (rows={len(wide)}, stations={wide['station_core'].nunique()})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599931c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### We've changed our approach -- DO NOT RUN THIS ANY MORE ###\n",
    "\n",
    "# === PRISM | Cell A: Raw ingest via official Web Service (resumable + chunkable) ===\n",
    "#\n",
    "# Web service syntax (per PRISM doc):\n",
    "#   https://services.nacse.org/prism/data/get/<region>/<res>/<element>/<date><?format=[nc|asc|bil]>\n",
    "# One grid per request, returns a .zip.  [oai_citation:4‡Prism Group](https://prism.oregonstate.edu/documents/PRISM_downloads_web_service.pdf)\n",
    "#\n",
    "# PRISM download limits:\n",
    "# - If a file is downloaded twice in a 24-hour period, no more downloads of that file allowed in that period\n",
    "# - Excessive activity may result in IP blocking  [oai_citation:5‡Prism Group](https://prism.oregonstate.edu/documents/PRISM_downloads_web_service.pdf)\n",
    "#\n",
    "# This cell is designed for \"download once, then reuse\", and for running in small chunks.\n",
    "\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# -----------------------------\n",
    "# Required config (uses your existing baseline keys)\n",
    "# -----------------------------\n",
    "BASELINE_START = pd.to_datetime(CONFIG[\"BASELINE_START_DATE\"])\n",
    "BASELINE_END   = pd.to_datetime(CONFIG[\"BASELINE_END_DATE\"])\n",
    "\n",
    "# PRISM web service parameters\n",
    "PRISM_SERVICE_BASE = str(CONFIG.get(\"PRISM_SERVICE_BASE_URL\", \"https://services.nacse.org/prism/data/get\")) \n",
    "PRISM_REGION = str(CONFIG.get(\"PRISM_REGION\", \"us\"))     # 'us' CONUS  [oai_citation:6‡Prism Group](https://prism.oregonstate.edu/documents/PRISM_downloads_web_service.pdf)\n",
    "PRISM_RES    = str(CONFIG.get(\"PRISM_RESOLUTION\", \"4km\"))# '4km' supported  [oai_citation:7‡Prism Group](https://prism.oregonstate.edu/documents/PRISM_downloads_web_service.pdf)\n",
    "PRISM_ELEMENTS = CONFIG.get(\"PRISM_ELEMENTS\", [\"ppt\", \"tmean\"])  # elements list in doc  [oai_citation:8‡Prism Group](https://prism.oregonstate.edu/documents/PRISM_downloads_web_service.pdf)\n",
    "\n",
    "# Output folder\n",
    "OUT_DIR = Path(CONFIG[\"out_dir\"])\n",
    "RAW_DIR = OUT_DIR / \"prism_raw_baseline_ws\"\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Controls for \"small chunks over several days\"\n",
    "# -----------------------------\n",
    "# You can run year-by-year to reduce server load and make progress predictable.\n",
    "# Set these each session (or add to config later if you like).\n",
    "RUN_YEAR_START = int(CONFIG.get(\"PRISM_RUN_YEAR_START\", BASELINE_START.year))\n",
    "RUN_YEAR_END   = int(CONFIG.get(\"PRISM_RUN_YEAR_END\", RUN_YEAR_START))  # default: single year\n",
    "MAX_DOWNLOADS_THIS_RUN = int(CONFIG.get(\"PRISM_MAX_DOWNLOADS_PER_RUN\", 400))  # hard stop per run\n",
    "\n",
    "# Throttling/backoff\n",
    "BASE_SLEEP_S = float(CONFIG.get(\"PRISM_BASE_SLEEP_S\", 2.0))   # PRISM sample script uses sleep 2  [oai_citation:9‡Prism Group](https://prism.oregonstate.edu/documents/PRISM_downloads_web_service.pdf)\n",
    "JITTER_S     = float(CONFIG.get(\"PRISM_JITTER_S\", 0.75))\n",
    "TIMEOUT_S    = int(CONFIG.get(\"PRISM_TIMEOUT_S\", 120))\n",
    "\n",
    "MAX_RETRIES  = int(CONFIG.get(\"PRISM_MAX_RETRIES\", 6))\n",
    "BACKOFF_BASE = float(CONFIG.get(\"PRISM_BACKOFF_BASE\", 1.7))\n",
    "\n",
    "# Optional: request format (default returns COG package; doc mentions optional formats nc/asc/bil)  [oai_citation:10‡Prism Group](https://prism.oregonstate.edu/documents/PRISM_downloads_web_service.pdf)\n",
    "# Leave as None for default; or set to \"bil\" / \"nc\"\n",
    "PRISM_FORMAT = CONFIG.get(\"PRISM_FORMAT\", None)\n",
    "\n",
    "# Optional: releaseDate check (defaults OFF).\n",
    "# When OFF: \"download once\" behavior = if file exists, skip without checking.\n",
    "USE_RELEASEDATE_CHECK = bool(CONFIG.get(\"PRISM_USE_RELEASEDATE_CHECK\", False))\n",
    "\n",
    "# -----------------------------\n",
    "# URL helpers (per doc)\n",
    "# -----------------------------\n",
    "def prism_grid_url(element: str, yyyymmdd: str) -> str:\n",
    "    # https://services.nacse.org/prism/data/get/<region>/<res>/<element>/<date><?format=...>  [oai_citation:11‡Prism Group](https://prism.oregonstate.edu/documents/PRISM_downloads_web_service.pdf)\n",
    "    url = f\"{PRISM_SERVICE_BASE}/{PRISM_REGION}/{PRISM_RES}/{element}/{yyyymmdd}\"\n",
    "    if PRISM_FORMAT:\n",
    "        url += f\"?format={PRISM_FORMAT}\"\n",
    "    return url\n",
    "\n",
    "def prism_release_url(element: str, yyyymmdd: str) -> str:\n",
    "    # https://services.nacse.org/prism/data/get/releaseDate/<region>/<resolution>/<element>/<date>?json=true  [oai_citation:12‡Prism Group](https://prism.oregonstate.edu/documents/PRISM_downloads_web_service.pdf)\n",
    "    return f\"{PRISM_SERVICE_BASE}/releaseDate/{PRISM_REGION}/{PRISM_RES}/{element}/{yyyymmdd}?json=true\"\n",
    "\n",
    "def safe_session() -> requests.Session:\n",
    "    s = requests.Session()\n",
    "    s.headers.update({\n",
    "        \"User-Agent\": CONFIG.get(\"USER_AGENT_HEADERS\", {}).get(\"User-Agent\", \"BlueLeafLabs/HydroPulse\"),\n",
    "        \"Accept\": \"*/*\",\n",
    "    })\n",
    "    return s\n",
    "\n",
    "def parse_filename_from_cd(cd: str) -> str | None:\n",
    "    # Content-Disposition: attachment; filename=prism_ppt_us_4km_19910101.zip\n",
    "    if not cd:\n",
    "        return None\n",
    "    cd = cd.strip()\n",
    "    parts = cd.split(\";\")\n",
    "    for p in parts:\n",
    "        p = p.strip()\n",
    "        if p.lower().startswith(\"filename=\"):\n",
    "            fn = p.split(\"=\", 1)[1].strip().strip('\"')\n",
    "            return fn\n",
    "    return None\n",
    "\n",
    "# -----------------------------\n",
    "# Local pathing\n",
    "# -----------------------------\n",
    "def out_path_for(element: str, yyyymmdd: str, filename_hint: str | None = None) -> Path:\n",
    "    ed = RAW_DIR / element\n",
    "    ed.mkdir(parents=True, exist_ok=True)\n",
    "    if filename_hint:\n",
    "        return ed / filename_hint\n",
    "    # Fallback (stable and unique even if server changes naming slightly)\n",
    "    suffix = PRISM_FORMAT if PRISM_FORMAT else \"cog\"\n",
    "    return ed / f\"prism_{element}_{PRISM_REGION}_{PRISM_RES}_{yyyymmdd}_{suffix}.zip\"\n",
    "\n",
    "# -----------------------------\n",
    "# Optional releaseDate logic\n",
    "# -----------------------------\n",
    "def fetch_release_date(session: requests.Session, element: str, yyyymmdd: str) -> str | None:\n",
    "    # doc: releaseDate service provides release date; older than Apr 2014 may be unpopulated  [oai_citation:13‡Prism Group](https://prism.oregonstate.edu/documents/PRISM_downloads_web_service.pdf)\n",
    "    try:\n",
    "        r = session.get(prism_release_url(element, yyyymmdd), timeout=TIMEOUT_S)\n",
    "        if r.status_code != 200:\n",
    "            return None\n",
    "        js = r.json()\n",
    "        # The PDF describes fields; response structure may be list/dict depending on single vs range.\n",
    "        # We'll defensively extract any plausible release date string.\n",
    "        if isinstance(js, list) and js:\n",
    "            return js[0].get(\"releaseDate\") or js[0].get(\"ReleaseDate\") or js[0].get(\"release_date\")\n",
    "        if isinstance(js, dict):\n",
    "            return js.get(\"releaseDate\") or js.get(\"ReleaseDate\") or js.get(\"release_date\")\n",
    "        return None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# -----------------------------\n",
    "# Download with retries + backoff\n",
    "# -----------------------------\n",
    "def download_one(session: requests.Session, element: str, yyyymmdd: str) -> tuple[str, Path | None]:\n",
    "    url = prism_grid_url(element, yyyymmdd)\n",
    "\n",
    "    # First request HEAD-like via GET stream (server returns a zip)\n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "            r = session.get(url, stream=True, timeout=TIMEOUT_S)\n",
    "            if r.status_code == 404:\n",
    "                return (\"unavailable\", None)\n",
    "            if r.status_code in (429, 500, 502, 503, 504):\n",
    "                raise RuntimeError(f\"transient {r.status_code}\")\n",
    "            r.raise_for_status()\n",
    "\n",
    "            fn = parse_filename_from_cd(r.headers.get(\"Content-Disposition\", \"\"))\n",
    "            out_path = out_path_for(element, yyyymmdd, fn)\n",
    "\n",
    "            if out_path.exists():\n",
    "                # Do not re-download. Close response promptly.\n",
    "                r.close()\n",
    "                return (\"exists\", out_path)\n",
    "\n",
    "            tmp = out_path.with_suffix(out_path.suffix + \".part\")\n",
    "            with open(tmp, \"wb\") as f:\n",
    "                for chunk in r.iter_content(chunk_size=1024 * 1024):\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "            os.replace(tmp, out_path)\n",
    "            return (\"downloaded\", out_path)\n",
    "\n",
    "        except Exception as e:\n",
    "            # backoff + jitter\n",
    "            sleep_s = (BACKOFF_BASE ** attempt) + random.random() * JITTER_S\n",
    "            print(f\"[WARN] {element} {yyyymmdd} attempt {attempt+1}/{MAX_RETRIES}: {e} -> sleep {sleep_s:.2f}s\")\n",
    "            time.sleep(sleep_s)\n",
    "\n",
    "    return (\"failed\", None)\n",
    "\n",
    "# -----------------------------\n",
    "# Build run date range (year-sliced)\n",
    "# -----------------------------\n",
    "run_start = max(BASELINE_START, pd.Timestamp(year=RUN_YEAR_START, month=1, day=1))\n",
    "run_end   = min(BASELINE_END,   pd.Timestamp(year=RUN_YEAR_END,   month=12, day=31))\n",
    "dates = pd.date_range(run_start, run_end, freq=\"D\")\n",
    "\n",
    "print(\"PRISM Cell A (web service) starting\")\n",
    "print(f\"Baseline window: {BASELINE_START.date()} → {BASELINE_END.date()}\")\n",
    "print(f\"Run slice      : {run_start.date()} → {run_end.date()} ({len(dates)} days)\")\n",
    "print(f\"Elements       : {PRISM_ELEMENTS}\")\n",
    "print(f\"Resolution     : {PRISM_RES} | Region: {PRISM_REGION}\")\n",
    "print(f\"Max downloads  : {MAX_DOWNLOADS_THIS_RUN}\")\n",
    "print(f\"Sleep (on dl)  : {BASE_SLEEP_S}s + jitter up to {JITTER_S}s\\n\")\n",
    "\n",
    "# -----------------------------\n",
    "# Main loop\n",
    "# -----------------------------\n",
    "session = safe_session()\n",
    "\n",
    "stats = {\"downloaded\": 0, \"exists\": 0, \"unavailable\": 0, \"failed\": 0, \"skipped_release\": 0}\n",
    "downloads_this_run = 0\n",
    "\n",
    "for element in PRISM_ELEMENTS:\n",
    "    print(f\"--- Element: {element} ---\")\n",
    "    for i, d in enumerate(dates, start=1):\n",
    "        yyyymmdd = d.strftime(\"%Y%m%d\")\n",
    "\n",
    "        # Enforce per-run cap (lets you run small chunks over multiple days)\n",
    "        if downloads_this_run >= MAX_DOWNLOADS_THIS_RUN:\n",
    "            print(f\"[STOP] Reached PRISM_MAX_DOWNLOADS_PER_RUN={MAX_DOWNLOADS_THIS_RUN}. Safe to rerun later.\")\n",
    "            break\n",
    "\n",
    "        # If file exists, skip immediately (resume behavior)\n",
    "        # We don’t know server filename until request, so check fallback name pattern too.\n",
    "        # We’ll do a cheap existence check by globbing element dir for this date.\n",
    "        el_dir = RAW_DIR / element\n",
    "        if el_dir.exists():\n",
    "            hits = list(el_dir.glob(f\"*{yyyymmdd}*.zip\"))\n",
    "            if hits:\n",
    "                stats[\"exists\"] += 1\n",
    "                continue\n",
    "\n",
    "        # Optional release-date check (OFF by default)\n",
    "        if USE_RELEASEDATE_CHECK:\n",
    "            _ = fetch_release_date(session, element, yyyymmdd)  # you can wire this into a manifest later\n",
    "\n",
    "        status, path = download_one(session, element, yyyymmdd)\n",
    "        stats[status] += 1\n",
    "\n",
    "        if status == \"downloaded\":\n",
    "            downloads_this_run += 1\n",
    "            # polite sleep only when we actually transfer bytes (PRISM sample script sleeps 2)  [oai_citation:14‡Prism Group](https://prism.oregonstate.edu/documents/PRISM_downloads_web_service.pdf)\n",
    "            time.sleep(BASE_SLEEP_S + random.random() * JITTER_S)\n",
    "\n",
    "        # Heartbeat every ~50 days\n",
    "        if i % 50 == 0:\n",
    "            print(f\"{element} day {i}/{len(dates)} | dl={stats['downloaded']} exist={stats['exists']} unavail={stats['unavailable']} failed={stats['failed']}\")\n",
    "\n",
    "    print(f\"Completed element: {element}\\n\")\n",
    "\n",
    "print(\"PRISM Cell A complete (this run slice)\")\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a4a349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PRISM | Cell B: Build HydroPulse baseline from PRISM daily long-term normals (avg_30y) ===\n",
    "#\n",
    "# Inputs (local, no downloads):\n",
    "#   {out_dir}/manual/prism/prism_ppt_us_25m_YYYYMMDD_avg_30y.zip\n",
    "#   {out_dir}/manual/prism/prism_tmean_us_25m_YYYYMMDD_avg_30y.zip\n",
    "#\n",
    "# Output:\n",
    "#   {out_dir}/prism_baseline_normals/prism_normals_doy_grid_25m_to_3km.parquet\n",
    "#\n",
    "# Output schema:\n",
    "#   grid_id, doy, prism_ppt_norm_mm, prism_tmean_norm_c\n",
    "\n",
    "import re\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "import rasterio\n",
    "from rasterio.io import MemoryFile\n",
    "from pyproj import Transformer\n",
    "\n",
    "OUT_DIR = Path(CONFIG[\"out_dir\"])\n",
    "\n",
    "# Where you placed PRISM normals\n",
    "PRISM_DIR = Path(CONFIG.get(\"PRISM_MANUAL_DIR\", OUT_DIR / \"manual\" / \"prism\"))\n",
    "if not PRISM_DIR.exists():\n",
    "    raise FileNotFoundError(f\"PRISM_MANUAL_DIR not found: {PRISM_DIR}\")\n",
    "\n",
    "# HydroPulse grid (EPSG:3310)\n",
    "OPS_EPSG = int(CONFIG.get(\"OPS_EPSG\", 3310))\n",
    "GRID_PATH = Path(resolve_out_path(CONFIG[\"GRID_FILENAME\"]))\n",
    "\n",
    "# Output\n",
    "BASE_DIR = OUT_DIR / \"prism_baseline_normals\"\n",
    "BASE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "SHARDS_DIR = BASE_DIR / \"shards_doy\"\n",
    "SHARDS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FINAL_PATH = BASE_DIR / \"prism_normals_doy_grid_25m_to_3km.parquet\"\n",
    "\n",
    "# --- Load grid centroids ---\n",
    "grid = gpd.read_parquet(GRID_PATH)\n",
    "if grid.crs is None or (grid.crs.to_epsg() or 0) != OPS_EPSG:\n",
    "    grid = grid.set_crs(f\"EPSG:{OPS_EPSG}\", allow_override=True) if grid.crs is None else grid.to_crs(OPS_EPSG)\n",
    "\n",
    "if \"grid_id\" not in grid.columns:\n",
    "    raise KeyError(\"Expected grid parquet to contain 'grid_id'.\")\n",
    "\n",
    "grid_id = grid[\"grid_id\"].astype(str).values\n",
    "centroids = grid.geometry.centroid\n",
    "xs = centroids.x.values\n",
    "ys = centroids.y.values\n",
    "n_cells = len(grid_id)\n",
    "\n",
    "print(f\"Grid: {n_cells} cells | EPSG:{OPS_EPSG}\")\n",
    "print(f\"PRISM normals dir: {PRISM_DIR}\")\n",
    "\n",
    "# --- PRISM filename parser (your observed convention) ---\n",
    "# Example: prism_ppt_us_25m_20200115_avg_30y.zip\n",
    "pat = re.compile(r\"^prism_(ppt|tmean)_us_25m_(\\d{8})_avg_30y\\.zip$\", re.IGNORECASE)\n",
    "\n",
    "ppt = {}\n",
    "tmean = {}\n",
    "\n",
    "for p in sorted(PRISM_DIR.glob(\"*.zip\")):\n",
    "    m = pat.match(p.name)\n",
    "    if not m:\n",
    "        continue\n",
    "    var = m.group(1).lower()\n",
    "    yyyymmdd = m.group(2)\n",
    "\n",
    "    # PRISM daily normals commonly use year=2020 as a convenient leap-year index;\n",
    "    # we convert YYYYMMDD -> DOY using that year token directly.\n",
    "    dt = pd.to_datetime(yyyymmdd, format=\"%Y%m%d\", utc=True)\n",
    "    doy = int(dt.dayofyear)\n",
    "\n",
    "    if var == \"ppt\":\n",
    "        ppt[doy] = p\n",
    "    elif var == \"tmean\":\n",
    "        tmean[doy] = p\n",
    "\n",
    "doys = sorted(set(ppt.keys()) & set(tmean.keys()))\n",
    "if not doys:\n",
    "    raise RuntimeError(\n",
    "        \"No matching PRISM ppt/tmean normals found.\\n\"\n",
    "        f\"Expected filenames like: prism_ppt_us_25m_20200115_avg_30y.zip in {PRISM_DIR}\"\n",
    "    )\n",
    "\n",
    "print(f\"Matched DOYs: {len(doys)} (e.g., {doys[:5]})\")\n",
    "\n",
    "# --- Read GeoTIFF inside PRISM zip via MemoryFile ---\n",
    "def open_tif_from_zip(zip_path: Path):\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as z:\n",
    "        tif_names = [n for n in z.namelist() if n.lower().endswith(\".tif\")]\n",
    "        if not tif_names:\n",
    "            raise FileNotFoundError(f\"No .tif found inside {zip_path.name}\")\n",
    "        tif_name = tif_names[0]\n",
    "        tif_bytes = z.read(tif_name)\n",
    "\n",
    "    mem = MemoryFile(tif_bytes)\n",
    "    ds = mem.open()\n",
    "    return mem, ds\n",
    "\n",
    "def sample_zip_to_grid(zip_path: Path, xs3310: np.ndarray, ys3310: np.ndarray) -> np.ndarray:\n",
    "    mem, ds = open_tif_from_zip(zip_path)\n",
    "    try:\n",
    "        tf = Transformer.from_crs(f\"EPSG:{OPS_EPSG}\", ds.crs, always_xy=True)\n",
    "        sx, sy = tf.transform(xs3310, ys3310)\n",
    "\n",
    "        vals = np.array([v[0] for v in ds.sample(zip(sx, sy))], dtype=np.float64)\n",
    "        nodata = ds.nodata\n",
    "        if nodata is not None:\n",
    "            vals[vals == nodata] = np.nan\n",
    "        vals[~np.isfinite(vals)] = np.nan\n",
    "        return vals\n",
    "    finally:\n",
    "        ds.close()\n",
    "        mem.close()\n",
    "\n",
    "# --- Build shards per DOY (resumable) ---\n",
    "written = 0\n",
    "skipped = 0\n",
    "\n",
    "for doy in doys:\n",
    "    shard_path = SHARDS_DIR / f\"prism_normals_doy_{doy:03d}.parquet\"\n",
    "    if shard_path.exists():\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "    ppt_vals = sample_zip_to_grid(ppt[doy], xs, ys).astype(np.float32)      # mm\n",
    "    tm_vals  = sample_zip_to_grid(tmean[doy], xs, ys).astype(np.float32)    # °C\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        \"grid_id\": grid_id,\n",
    "        \"doy\": np.full(n_cells, doy, dtype=np.int16),\n",
    "        \"prism_ppt_norm_mm\": ppt_vals,\n",
    "        \"prism_tmean_norm_c\": tm_vals,\n",
    "    })\n",
    "    out.to_parquet(shard_path, index=False)\n",
    "    written += 1\n",
    "\n",
    "    if written % 25 == 0:\n",
    "        print(f\"Shards written: {written} | latest DOY={doy:03d}\")\n",
    "\n",
    "print(f\"Shard pass complete | written={written} | skipped={skipped}\")\n",
    "\n",
    "# --- Combine to one baseline parquet (fast enough at 366 shards) ---\n",
    "shards = sorted(SHARDS_DIR.glob(\"prism_normals_doy_*.parquet\"))\n",
    "df_all = pd.concat((pd.read_parquet(p) for p in shards), ignore_index=True)\n",
    "df_all.to_parquet(FINAL_PATH, index=False)\n",
    "\n",
    "print(f\"Saved baseline: {FINAL_PATH}\")\n",
    "print(f\"Rows: {len(df_all)} | doys: {df_all['doy'].nunique()} | cells: {df_all['grid_id'].nunique()}\")\n",
    "print(df_all.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9a2bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SMAP (SPL4SMGP) via Harmony: CA-only subset + quiet + strong resume ===\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import datetime as dt\n",
    "\n",
    "import earthaccess\n",
    "from harmony import BBox, Client, Collection, Request, CapabilitiesRequest\n",
    "\n",
    "# ---------- 0) env reload (reuse your existing helpers if available) ----------\n",
    "try:\n",
    "    apply_env_overrides()\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "if not os.environ.get(\"EARTHDATA_TOKEN\") and not (\n",
    "    os.environ.get(\"EARTHDATA_USERNAME\") and os.environ.get(\"EARTHDATA_PASSWORD\")\n",
    "):\n",
    "    raise RuntimeError(\"Missing Earthdata credentials. Set EARTHDATA_TOKEN (recommended) in .env.\")\n",
    "\n",
    "earthaccess.login(strategy=\"environment\")  # should be quiet if already logged in\n",
    "\n",
    "# ---------- 1) config ----------\n",
    "bbox = CONFIG[\"bbox\"]\n",
    "W = float(bbox[\"nwlng\"])\n",
    "S = float(bbox[\"selat\"])\n",
    "E = float(bbox[\"selng\"])\n",
    "N = float(bbox[\"nwlat\"])\n",
    "\n",
    "START = dt.date.fromisoformat(CONFIG[\"start_date\"])\n",
    "END   = dt.date.fromisoformat(CONFIG[\"end_date\"])\n",
    "\n",
    "SHORT_NAME = CONFIG.get(\"SMAP_L4_SHORT_NAME\", \"SPL4SMGP\")\n",
    "DESIRED_VARS = CONFIG.get(\"SMAP_L4_VARIABLES\", [\"sm_surface\", \"sm_rootzone\"])\n",
    "\n",
    "OUT_DIR = Path(CONFIG[\"out_dir\"])\n",
    "SMAP_DIR = OUT_DIR / \"manual\" / \"smap\" / SHORT_NAME\n",
    "SMAP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SENTINEL_DIR = SMAP_DIR / \"_done\"\n",
    "SENTINEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def month_start(d: dt.date) -> dt.date:\n",
    "    return dt.date(d.year, d.month, 1)\n",
    "\n",
    "def next_month(d: dt.date) -> dt.date:\n",
    "    return dt.date(d.year + (d.month == 12), 1 if d.month == 12 else d.month + 1, 1)\n",
    "\n",
    "def month_range(start: dt.date, end: dt.date):\n",
    "    cur = month_start(start)\n",
    "    while cur <= end:\n",
    "        nxt = next_month(cur)\n",
    "        yield cur, min(end + dt.timedelta(days=1), nxt)  # end is effectively exclusive\n",
    "        cur = nxt\n",
    "\n",
    "def has_any_files(folder: Path) -> bool:\n",
    "    if not folder.exists():\n",
    "        return False\n",
    "    # Harmony may write nested outputs; look recursively for non-empty files\n",
    "    for p in folder.rglob(\"*\"):\n",
    "        if p.is_file() and p.stat().st_size > 0:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "print(\"SMAP/Harmony (quiet) setup\")\n",
    "print(\"  short_name:\", SHORT_NAME)\n",
    "print(\"  bbox (W,S,E,N):\", (W, S, E, N))\n",
    "print(\"  time:\", START, \"→\", END)\n",
    "print(\"  root:\", SMAP_DIR)\n",
    "\n",
    "# ---------- 2) capabilities + variable sanitization ----------\n",
    "harmony_client = Client()\n",
    "\n",
    "cap_req = CapabilitiesRequest(short_name=SHORT_NAME)\n",
    "cap = harmony_client.submit(cap_req)\n",
    "\n",
    "import json\n",
    "if isinstance(cap, str):\n",
    "    cap = json.loads(cap)\n",
    "\n",
    "concept_id = cap.get(\"conceptId\") or cap.get(\"concept_id\") or cap.get(\"conceptID\")\n",
    "if not concept_id:\n",
    "    raise RuntimeError(f\"Could not determine conceptId for {SHORT_NAME} from Harmony capabilities.\")\n",
    "\n",
    "available_var_names = set()\n",
    "for v in (cap.get(\"variables\") or []):\n",
    "    if isinstance(v, dict) and \"name\" in v:\n",
    "        available_var_names.add(v[\"name\"])\n",
    "\n",
    "if available_var_names:\n",
    "    chosen = []\n",
    "    for name in DESIRED_VARS:\n",
    "        if name in available_var_names:\n",
    "            chosen.append(name)\n",
    "        else:\n",
    "            hits = [vn for vn in available_var_names if vn == name or vn.endswith(name) or name in vn]\n",
    "            if hits:\n",
    "                chosen.append(hits[0])\n",
    "    DESIRED_VARS = sorted(set(chosen))\n",
    "\n",
    "# ---------- 3) submit/download month-by-month (quiet + strong resume) ----------\n",
    "jobs_submitted = 0\n",
    "months_skipped = 0\n",
    "files_downloaded = 0\n",
    "\n",
    "for m0, m1 in month_range(START, END):\n",
    "    tag = f\"{m0:%Y%m}\"\n",
    "    sentinel = SENTINEL_DIR / f\"{tag}.done\"\n",
    "    month_dir = SMAP_DIR / tag\n",
    "\n",
    "    # Strong resume: skip if sentinel exists OR month folder already has files\n",
    "    if sentinel.exists() or has_any_files(month_dir):\n",
    "        months_skipped += 1\n",
    "        if not sentinel.exists():\n",
    "            # create sentinel so future runs are clean\n",
    "            sentinel.write_text(\"done=1\\nnote=folder already had files\\n\")\n",
    "        print(f\"[SKIP] {tag}\")\n",
    "        continue\n",
    "\n",
    "    req = Request(\n",
    "        collection=Collection(id=concept_id),\n",
    "        spatial=BBox(W, S, E, N),\n",
    "        temporal={\"start\": dt.datetime(m0.year, m0.month, m0.day),\n",
    "                  \"stop\":  dt.datetime(m1.year, m1.month, m1.day)},\n",
    "        variables=DESIRED_VARS if DESIRED_VARS else None,\n",
    "    )\n",
    "\n",
    "    if not req.is_valid():\n",
    "        raise RuntimeError(f\"Invalid Harmony request for month {tag}: check bbox/time/vars\")\n",
    "\n",
    "    print(f\"[RUN] {tag} submitting…\")\n",
    "    job_id = harmony_client.submit(req)\n",
    "    jobs_submitted += 1\n",
    "\n",
    "    # Quiet wait (no progress spam)\n",
    "    harmony_client.wait_for_processing(job_id, show_progress=False)\n",
    "\n",
    "    month_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    futures = harmony_client.download_all(job_id, directory=str(month_dir), overwrite=False)\n",
    "    out_files = [f.result() for f in futures]\n",
    "    out_files = [p for p in out_files if p]\n",
    "\n",
    "    files_downloaded += len(out_files)\n",
    "    sentinel.write_text(f\"job_id={job_id}\\nfiles={len(out_files)}\\n\")\n",
    "    print(f\"[DONE] {tag} files={len(out_files)}\")\n",
    "\n",
    "print(\"SMAP/Harmony complete\")\n",
    "print(\"  jobs submitted:\", jobs_submitted)\n",
    "print(\"  months skipped:\", months_skipped)\n",
    "print(\"  files downloaded this run:\", files_downloaded)\n",
    "print(\"  root dir:\", SMAP_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ee2569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SMAP Cell A (fixed): SPL4SMGP Harmony subset -> daily canonical parquet (resume-safe, quiet) ===\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "OUT_DIR = Path(CONFIG[\"out_dir\"])\n",
    "SMAP_ROOT = OUT_DIR / Path(CONFIG.get(\"SMAP_L4_DIRNAME\", \"manual/smap/SPL4SMGP\"))\n",
    "if not SMAP_ROOT.exists():\n",
    "    raise FileNotFoundError(f\"Missing SMAP root dir: {SMAP_ROOT}\")\n",
    "\n",
    "DAILY_DIR = OUT_DIR / Path(CONFIG.get(\"SMAP_DAILY_DIRNAME\", \"derived/smap_daily\"))\n",
    "DAILY_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "GROUP = (CONFIG.get(\"SMAP_NETCDF_GROUP\", \"Geophysical_Data\") or \"\").strip()\n",
    "VAR_SURF = CONFIG.get(\"SMAP_VAR_SURFACE\", \"sm_surface\")\n",
    "VAR_ROOT = CONFIG.get(\"SMAP_VAR_ROOTZONE\", \"sm_rootzone\")\n",
    "\n",
    "TEMPLATE = CONFIG.get(\"SMAP_DAILY_TEMPLATE\", \"smap_daily_{date}.parquet\")\n",
    "AGG = (CONFIG.get(\"SMAP_DAILY_AGG\", \"median\") or \"median\").lower()\n",
    "LOG_EVERY = int(CONFIG.get(\"SMAP_LOG_EVERY_N_DAYS\", 20))\n",
    "\n",
    "if AGG not in {\"median\", \"mean\"}:\n",
    "    raise ValueError(\"SMAP_DAILY_AGG must be 'median' or 'mean'\")\n",
    "\n",
    "# Filenames like: SMAP_L4_SM_gph_20240731T223000_Vv8010_001_subsetted.nc4\n",
    "TS_RE = re.compile(r\"_(\\d{8})T(\\d{6})_\")\n",
    "\n",
    "def parse_dt_from_name(p: Path) -> pd.Timestamp:\n",
    "    m = TS_RE.search(p.name)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Cannot parse timestamp from filename: {p.name}\")\n",
    "    ymd, hms = m.group(1), m.group(2)\n",
    "    return pd.Timestamp(f\"{ymd}{hms}\", tz=\"UTC\")\n",
    "\n",
    "def out_path_for_date(d_utc: pd.Timestamp) -> Path:\n",
    "    return DAILY_DIR / TEMPLATE.format(date=d_utc.date().isoformat())\n",
    "\n",
    "def open_smap(fp: Path) -> xr.Dataset:\n",
    "    # Use netcdf4 engine; open the science group\n",
    "    return xr.open_dataset(fp, engine=\"netcdf4\", group=GROUP, decode_times=False, mask_and_scale=True)\n",
    "\n",
    "def agg_stack(stack: np.ndarray) -> np.ndarray:\n",
    "    # stack shape: (T, H, W), with NaN\n",
    "    if AGG == \"median\":\n",
    "        with np.errstate(all=\"ignore\"):\n",
    "            return np.nanmedian(stack, axis=0)\n",
    "    return np.nanmean(stack, axis=0)\n",
    "\n",
    "# Collect likely data files (Harmony outputs often .nc4)\n",
    "OK_EXT = {\".nc4\", \".nc\", \".cdf\", \".h5\", \".hdf5\"}\n",
    "all_files = sorted(\n",
    "    p for p in SMAP_ROOT.rglob(\"*\")\n",
    "    if p.is_file()\n",
    "    and \"_done\" not in p.parts\n",
    "    and not p.name.endswith(\".done\")\n",
    "    and p.stat().st_size > 0\n",
    "    and (p.suffix.lower() in OK_EXT)\n",
    ")\n",
    "\n",
    "if not all_files:\n",
    "    raise FileNotFoundError(f\"No SMAP data files found under {SMAP_ROOT}\")\n",
    "\n",
    "# Build index of files with timestamps\n",
    "rows = []\n",
    "for fp in all_files:\n",
    "    try:\n",
    "        ts = parse_dt_from_name(fp)\n",
    "    except Exception:\n",
    "        continue\n",
    "    rows.append((fp, ts, ts.normalize()))\n",
    "\n",
    "idx = pd.DataFrame(rows, columns=[\"path\", \"ts_utc\", \"date_utc\"])\n",
    "if idx.empty:\n",
    "    raise RuntimeError(\"Found SMAP files but none matched the expected timestamp pattern.\")\n",
    "\n",
    "# Filter to your configured analysis window\n",
    "start = pd.Timestamp(CONFIG[\"start_date\"], tz=\"UTC\")\n",
    "end = pd.Timestamp(CONFIG[\"end_date\"], tz=\"UTC\") + pd.Timedelta(days=1) - pd.Timedelta(seconds=1)\n",
    "idx = idx[(idx[\"ts_utc\"] >= start) & (idx[\"ts_utc\"] <= end)].copy()\n",
    "idx.sort_values([\"date_utc\", \"ts_utc\"], inplace=True)\n",
    "\n",
    "dates = idx[\"date_utc\"].drop_duplicates().tolist()\n",
    "if not dates:\n",
    "    raise RuntimeError(\"No SMAP files within CONFIG start/end window.\")\n",
    "\n",
    "print(\"SMAP Cell A starting\")\n",
    "print(\"  SMAP root      :\", SMAP_ROOT)\n",
    "print(\"  Files in window:\", len(idx))\n",
    "print(\"  Days in window :\", len(dates))\n",
    "print(\"  Group/vars     :\", GROUP, VAR_SURF, VAR_ROOT)\n",
    "print(\"  Output dir     :\", DAILY_DIR)\n",
    "\n",
    "processed = 0\n",
    "skipped = 0\n",
    "\n",
    "for i, d in enumerate(dates, start=1):\n",
    "    outp = out_path_for_date(d)\n",
    "    if outp.exists() and outp.stat().st_size > 0:\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "    day_files = idx.loc[idx[\"date_utc\"] == d, \"path\"].tolist()\n",
    "    if not day_files:\n",
    "        continue\n",
    "\n",
    "    surf_list, root_list = [], []\n",
    "\n",
    "    for fp in day_files:\n",
    "        ds = open_smap(fp)\n",
    "\n",
    "        # pull arrays\n",
    "        surf = ds[VAR_SURF].astype(\"float32\").values\n",
    "        root = ds[VAR_ROOT].astype(\"float32\").values\n",
    "\n",
    "        # replace fill (-9999) + enforce plausible range [0,1]\n",
    "        surf = np.where((surf >= 0.0) & (surf <= 1.0), surf, np.nan)\n",
    "        root = np.where((root >= 0.0) & (root <= 1.0), root, np.nan)\n",
    "\n",
    "        surf_list.append(surf)\n",
    "        root_list.append(root)\n",
    "\n",
    "        ds.close()\n",
    "\n",
    "    surf_stack = np.stack(surf_list, axis=0)\n",
    "    root_stack = np.stack(root_list, axis=0)\n",
    "\n",
    "    surf_day = agg_stack(surf_stack)\n",
    "    root_day = agg_stack(root_stack)\n",
    "\n",
    "    n_obs = np.sum(np.isfinite(surf_stack), axis=0).astype(\"int16\")\n",
    "\n",
    "    H, W = surf_day.shape\n",
    "    yy, xx = np.indices((H, W))\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"date_utc\": np.repeat(d, H * W),\n",
    "        \"y\": yy.ravel().astype(\"int32\"),\n",
    "        \"x\": xx.ravel().astype(\"int32\"),\n",
    "        \"sm_surface\": surf_day.ravel().astype(\"float32\"),\n",
    "        \"sm_rootzone\": root_day.ravel().astype(\"float32\"),\n",
    "        \"n_obs\": n_obs.ravel().astype(\"int16\"),\n",
    "    })\n",
    "\n",
    "    # Drop pixels with no data at all\n",
    "    df = df[~(df[\"sm_surface\"].isna() & df[\"sm_rootzone\"].isna())]\n",
    "\n",
    "    tmp = outp.with_suffix(\".parquet.tmp\")\n",
    "    df.to_parquet(tmp, index=False)\n",
    "    os.replace(tmp, outp)\n",
    "\n",
    "    processed += 1\n",
    "    if (processed % LOG_EVERY) == 0 or i == len(dates):\n",
    "        print(f\"  day {i}/{len(dates)} | wrote={processed} skipped={skipped} | last={d.date()} | rows={len(df):,}\")\n",
    "\n",
    "print(\"SMAP Cell A complete\")\n",
    "print(\"  wrote  :\", processed)\n",
    "print(\"  skipped:\", skipped)\n",
    "print(\"  daily dir:\", DAILY_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7dd8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some SMAP validation/QA on daily parquet outputs\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "OUT_DIR = Path(CONFIG[\"out_dir\"])\n",
    "DAILY_DIR = OUT_DIR / Path(CONFIG.get(\"SMAP_DAILY_DIRNAME\", \"derived/smap_daily\"))\n",
    "\n",
    "files = sorted(DAILY_DIR.glob(\"smap_daily_*.parquet\"))\n",
    "assert files, f\"No SMAP daily parquet files found in {DAILY_DIR}\"\n",
    "\n",
    "# Sample a few days + last day\n",
    "sample_files = [files[0], files[len(files)//2], files[-1]]\n",
    "rows = []\n",
    "\n",
    "for fp in sample_files:\n",
    "    df = pd.read_parquet(fp)\n",
    "\n",
    "    rows.append({\n",
    "        \"file\": fp.name,\n",
    "        \"rows\": len(df),\n",
    "        \"unique_pixels\": df[[\"y\",\"x\"]].drop_duplicates().shape[0],\n",
    "        \"sm_surface_min\": float(np.nanmin(df[\"sm_surface\"].values)) if df[\"sm_surface\"].notna().any() else np.nan,\n",
    "        \"sm_surface_p01\": float(np.nanpercentile(df[\"sm_surface\"].dropna(), 1)) if df[\"sm_surface\"].notna().any() else np.nan,\n",
    "        \"sm_surface_p50\": float(np.nanpercentile(df[\"sm_surface\"].dropna(), 50)) if df[\"sm_surface\"].notna().any() else np.nan,\n",
    "        \"sm_surface_p99\": float(np.nanpercentile(df[\"sm_surface\"].dropna(), 99)) if df[\"sm_surface\"].notna().any() else np.nan,\n",
    "        \"sm_surface_max\": float(np.nanmax(df[\"sm_surface\"].values)) if df[\"sm_surface\"].notna().any() else np.nan,\n",
    "        \"sm_rootzone_p50\": float(np.nanpercentile(df[\"sm_rootzone\"].dropna(), 50)) if df[\"sm_rootzone\"].notna().any() else np.nan,\n",
    "        \"n_obs_p50\": float(np.nanpercentile(df[\"n_obs\"].values, 50)),\n",
    "        \"n_obs_min\": int(df[\"n_obs\"].min()),\n",
    "        \"n_obs_max\": int(df[\"n_obs\"].max()),\n",
    "        \"nan_surface_frac\": float(df[\"sm_surface\"].isna().mean()),\n",
    "        \"nan_rootzone_frac\": float(df[\"sm_rootzone\"].isna().mean()),\n",
    "    })\n",
    "\n",
    "qa = pd.DataFrame(rows)\n",
    "qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841a9137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SMAP Cell B: daily SMAP pixels -> HydroPulse 3km grid_id daily table (resume-safe) ===\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import xarray as xr\n",
    "\n",
    "OUT_DIR = Path(CONFIG[\"out_dir\"])\n",
    "\n",
    "# Inputs\n",
    "GRID_PATH = Path(resolve_out_path(CONFIG.get(\"GRID_FILENAME\", \"grid_3000m_CA_epsg3310.parquet\")))\n",
    "SMAP_ROOT = OUT_DIR / Path(CONFIG.get(\"SMAP_L4_DIRNAME\", \"manual/smap/SPL4SMGP\"))\n",
    "SMAP_DAILY_DIR = OUT_DIR / Path(CONFIG.get(\"SMAP_DAILY_DIRNAME\", \"derived/smap_daily\"))\n",
    "\n",
    "if not GRID_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Missing grid parquet: {GRID_PATH}\")\n",
    "if not SMAP_ROOT.exists():\n",
    "    raise FileNotFoundError(f\"Missing SMAP root: {SMAP_ROOT}\")\n",
    "if not SMAP_DAILY_DIR.exists():\n",
    "    raise FileNotFoundError(f\"Missing SMAP daily dir: {SMAP_DAILY_DIR}\")\n",
    "\n",
    "# Outputs\n",
    "GRIDMAP_PATH = OUT_DIR / CONFIG.get(\"SMAP_GRIDMAP_FILENAME\", \"smap_pixel_to_grid_3310.parquet\")\n",
    "SHARDS_DIR = OUT_DIR / Path(CONFIG.get(\"SMAP_GRID_DAILY_SHARDS_DIRNAME\", \"derived/smap_grid_shards\"))\n",
    "SHARDS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "final_name_tmpl = CONFIG.get(\"SMAP_GRID_FINAL_FILENAME\", \"smap_daily_grid_CA_3000m_epsg3310_{start}_{end}.parquet\")\n",
    "FINAL_PATH = OUT_DIR / final_name_tmpl.format(\n",
    "    start=CONFIG[\"start_date\"].replace(\"-\", \"\"),\n",
    "    end=CONFIG[\"end_date\"].replace(\"-\", \"\")\n",
    ")\n",
    "\n",
    "AGG = (CONFIG.get(\"SMAP_GRID_AGG\", \"mean\") or \"mean\").lower()\n",
    "MIN_PIX = int(CONFIG.get(\"SMAP_GRID_MIN_PIXELS\", 1))\n",
    "LOG_EVERY = int(CONFIG.get(\"SMAP_LOG_EVERY_N_DAYS\", 20))\n",
    "NEAR_KM = float(CONFIG.get(\"SMAP_NEAREST_FALLBACK_KM\", 6.0))\n",
    "NEAR_M = NEAR_KM * 1000.0\n",
    "\n",
    "if AGG not in {\"mean\", \"median\"}:\n",
    "    raise ValueError(\"SMAP_GRID_AGG must be 'mean' or 'median'\")\n",
    "\n",
    "# -------------------------\n",
    "# 1) Load HydroPulse grid\n",
    "# -------------------------\n",
    "grid = gpd.read_parquet(GRID_PATH)\n",
    "if \"grid_id\" not in grid.columns:\n",
    "    raise KeyError(\"Grid parquet must contain 'grid_id' column.\")\n",
    "if \"geometry\" not in grid.columns:\n",
    "    raise KeyError(\"Grid parquet must contain 'geometry' column.\")\n",
    "\n",
    "# Ensure CRS is EPSG:3310 (your OPS_EPSG)\n",
    "OPS_EPSG = int(CONFIG.get(\"OPS_EPSG\", 3310))\n",
    "if grid.crs is None:\n",
    "    grid = grid.set_crs(epsg=OPS_EPSG)\n",
    "else:\n",
    "    grid = grid.to_crs(epsg=OPS_EPSG)\n",
    "\n",
    "grid = grid[[\"grid_id\", \"geometry\"]].copy()\n",
    "\n",
    "# -------------------------\n",
    "# 2) Build (y,x)->grid_id map (one-time), using cell_lat/cell_lon from a sample .nc4\n",
    "# -------------------------\n",
    "def find_sample_nc4() -> Path:\n",
    "    ok_ext = {\".nc4\", \".nc\", \".cdf\"}\n",
    "    cands = sorted(\n",
    "        p for p in SMAP_ROOT.rglob(\"*\")\n",
    "        if p.is_file()\n",
    "        and \"_done\" not in p.parts\n",
    "        and p.stat().st_size > 0\n",
    "        and p.suffix.lower() in ok_ext\n",
    "    )\n",
    "    if not cands:\n",
    "        raise FileNotFoundError(f\"No .nc/.nc4 files found under {SMAP_ROOT}\")\n",
    "    # pick largest, tends to be real science granule\n",
    "    return max(cands, key=lambda p: p.stat().st_size)\n",
    "\n",
    "def build_gridmap() -> pd.DataFrame:\n",
    "    sample = find_sample_nc4()\n",
    "\n",
    "    # Root group contains cell_lat/cell_lon per your earlier debug\n",
    "    ds_root = xr.open_dataset(sample, engine=\"netcdf4\", decode_times=False, mask_and_scale=True)\n",
    "\n",
    "    # Expect 2D arrays named cell_lat / cell_lon\n",
    "    if \"cell_lat\" not in ds_root.variables or \"cell_lon\" not in ds_root.variables:\n",
    "        raise KeyError(f\"Expected cell_lat/cell_lon in root group. Vars: {list(ds_root.variables.keys())[:50]}\")\n",
    "\n",
    "    lat = ds_root[\"cell_lat\"].values\n",
    "    lon = ds_root[\"cell_lon\"].values\n",
    "\n",
    "    # y/x coordinate arrays exist too; but we use integer indices y,x matching your daily parquet\n",
    "    H, W = lat.shape\n",
    "    yy, xx = np.indices((H, W))\n",
    "\n",
    "    # Flatten\n",
    "    df = pd.DataFrame({\n",
    "        \"y\": yy.ravel().astype(\"int32\"),\n",
    "        \"x\": xx.ravel().astype(\"int32\"),\n",
    "        \"lat\": lat.ravel().astype(\"float64\"),\n",
    "        \"lon\": lon.ravel().astype(\"float64\"),\n",
    "    })\n",
    "    ds_root.close()\n",
    "\n",
    "    # Drop any missing or insane coords\n",
    "    df = df.replace([np.inf, -np.inf], np.nan).dropna(subset=[\"lat\", \"lon\"])\n",
    "    df = df[(df[\"lat\"] >= -90) & (df[\"lat\"] <= 90) & (df[\"lon\"] >= -180) & (df[\"lon\"] <= 180)]\n",
    "\n",
    "    # Make points in WGS84 then project to 3310\n",
    "    gdf = gpd.GeoDataFrame(\n",
    "        df[[\"y\", \"x\"]].copy(),\n",
    "        geometry=gpd.points_from_xy(df[\"lon\"], df[\"lat\"]),\n",
    "        crs=\"EPSG:4326\"\n",
    "    ).to_crs(epsg=OPS_EPSG)\n",
    "\n",
    "    # Spatial join to grid polygons\n",
    "    joined = gpd.sjoin(gdf, grid, how=\"left\", predicate=\"within\")[[\"y\", \"x\", \"grid_id\", \"geometry\"]].copy()\n",
    "\n",
    "    missing = joined[\"grid_id\"].isna().sum()\n",
    "    if missing > 0:\n",
    "        # Fallback: nearest join within radius (meters)\n",
    "        # Keep only missing points for nearest join\n",
    "        miss = joined[joined[\"grid_id\"].isna()].drop(columns=[\"grid_id\"]).copy()\n",
    "        # sjoin_nearest is available in geopandas >=0.10; use max_distance to avoid nonsense matches\n",
    "        nearest = gpd.sjoin_nearest(miss, grid, how=\"left\", max_distance=NEAR_M, distance_col=\"dist_m\")\n",
    "        joined.loc[joined[\"grid_id\"].isna(), \"grid_id\"] = nearest[\"grid_id\"].values\n",
    "\n",
    "    joined = joined.drop(columns=[\"geometry\"])\n",
    "    joined = joined.dropna(subset=[\"grid_id\"]).copy()\n",
    "\n",
    "    # Enforce uniqueness\n",
    "    joined = joined.drop_duplicates(subset=[\"y\", \"x\"])\n",
    "    return joined\n",
    "\n",
    "if GRIDMAP_PATH.exists() and GRIDMAP_PATH.stat().st_size > 0:\n",
    "    gridmap = pd.read_parquet(GRIDMAP_PATH)\n",
    "else:\n",
    "    gridmap = build_gridmap()\n",
    "    tmp = GRIDMAP_PATH.with_suffix(\".tmp.parquet\")\n",
    "    gridmap.to_parquet(tmp, index=False)\n",
    "    os.replace(tmp, GRIDMAP_PATH)\n",
    "\n",
    "if not {\"y\",\"x\",\"grid_id\"}.issubset(set(gridmap.columns)):\n",
    "    raise RuntimeError(f\"Bad gridmap schema: {gridmap.columns}\")\n",
    "\n",
    "print(\"SMAP Cell B starting\")\n",
    "print(\"  Grid:\", GRID_PATH)\n",
    "print(\"  SMAP daily dir:\", SMAP_DAILY_DIR)\n",
    "print(\"  Gridmap:\", GRIDMAP_PATH, f\"(rows={len(gridmap):,})\")\n",
    "print(\"  Shards dir:\", SHARDS_DIR)\n",
    "print(\"  FINAL:\", FINAL_PATH)\n",
    "\n",
    "# -------------------------\n",
    "# 3) Process day shards (resume-safe)\n",
    "# -------------------------\n",
    "daily_files = sorted(SMAP_DAILY_DIR.glob(\"smap_daily_*.parquet\"))\n",
    "if not daily_files:\n",
    "    raise FileNotFoundError(f\"No smap_daily parquet files found in {SMAP_DAILY_DIR}\")\n",
    "\n",
    "written = 0\n",
    "skipped = 0\n",
    "\n",
    "def agg_group(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # df has grid_id and moisture fields\n",
    "    if AGG == \"mean\":\n",
    "        out = df.groupby(\"grid_id\", as_index=False).agg(\n",
    "            sm_surface=(\"sm_surface\", \"mean\"),\n",
    "            sm_rootzone=(\"sm_rootzone\", \"mean\"),\n",
    "            n_pixels=(\"grid_id\", \"size\"),\n",
    "            n_obs_mean=(\"n_obs\", \"mean\"),\n",
    "        )\n",
    "    else:\n",
    "        out = df.groupby(\"grid_id\", as_index=False).agg(\n",
    "            sm_surface=(\"sm_surface\", \"median\"),\n",
    "            sm_rootzone=(\"sm_rootzone\", \"median\"),\n",
    "            n_pixels=(\"grid_id\", \"size\"),\n",
    "            n_obs_mean=(\"n_obs\", \"mean\"),\n",
    "        )\n",
    "    return out\n",
    "\n",
    "for i, fp in enumerate(daily_files, start=1):\n",
    "    date_str = fp.stem.split(\"_\")[-1]  # YYYY-MM-DD\n",
    "    outp = SHARDS_DIR / f\"smap_grid_{date_str}.parquet\"\n",
    "\n",
    "    if outp.exists() and outp.stat().st_size > 0:\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "    df = pd.read_parquet(fp)  # columns: date_utc,y,x,sm_surface,sm_rootzone,n_obs\n",
    "    # Join mapping\n",
    "    df = df.merge(gridmap, on=[\"y\",\"x\"], how=\"inner\")\n",
    "\n",
    "    # Aggregate to grid\n",
    "    g = agg_group(df)\n",
    "\n",
    "    # Apply min pixels filter\n",
    "    g = g[g[\"n_pixels\"] >= MIN_PIX].copy()\n",
    "    g.insert(1, \"date_utc\", pd.Timestamp(date_str, tz=\"UTC\"))\n",
    "\n",
    "    tmp = outp.with_suffix(\".tmp.parquet\")\n",
    "    g.to_parquet(tmp, index=False)\n",
    "    os.replace(tmp, outp)\n",
    "\n",
    "    written += 1\n",
    "    if (written % LOG_EVERY) == 0 or i == len(daily_files):\n",
    "        print(f\"  day {i}/{len(daily_files)} | wrote={written} skipped={skipped} | last={date_str} | rows={len(g):,}\")\n",
    "\n",
    "print(\"SMAP Cell B shards complete\")\n",
    "print(\"  wrote  :\", written)\n",
    "print(\"  skipped:\", skipped)\n",
    "\n",
    "# -------------------------\n",
    "# 4) Optional: stitch shards into one final parquet (resume-safe)\n",
    "# -------------------------\n",
    "if FINAL_PATH.exists() and FINAL_PATH.stat().st_size > 0:\n",
    "    print(\"[SKIP] Final exists:\", FINAL_PATH)\n",
    "else:\n",
    "    shard_files = sorted(SHARDS_DIR.glob(\"smap_grid_*.parquet\"))\n",
    "    if not shard_files:\n",
    "        raise RuntimeError(\"No SMAP grid shards found to stitch.\")\n",
    "    parts = [pd.read_parquet(p) for p in shard_files]\n",
    "    final = pd.concat(parts, ignore_index=True)\n",
    "    tmp = FINAL_PATH.with_suffix(\".tmp.parquet\")\n",
    "    final.to_parquet(tmp, index=False)\n",
    "    os.replace(tmp, FINAL_PATH)\n",
    "    print(\"Saved FINAL:\", FINAL_PATH, \"rows=\", len(final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91087cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# SNOTEL — Cell A\n",
    "# Parse, normalize, and sanity-check\n",
    "# First, we download data manually from https://wcc.sc.egov.usda.gov/reportGenerator/\n",
    "# Parameters are: click on \"Advanced search\", select Network - SNOTEL, all stations in CA - this is about 500 stations,\n",
    "# date range 1991-01-01 to 2026-01-12 (most recent available as of writing),\n",
    "# and select a few variables\n",
    "# The buildable report URL looks like this: \n",
    "# https://wcc.sc.egov.usda.gov/reportGenerator/view_csv/customMultiTimeSeriesGroupByStationReport/daily/start_of_period/county=%2522Alameda%2522,%2522Alpine%2522,%2522Amador%2522,%2522Butte%2522,%2522Calaveras%2522,%2522Colusa%2522,%2522Contra%2520Costa%2522,%2522Del%2520Norte%2522,%2522El%2520Dorado%2522,%2522Fresno%2522,%2522Glenn%2522,%2522Humboldt%2522,%2522Imperial%2522,%2522Inyo%2522,%2522Kern%2522,%2522Kings%2522,%2522Lake%2522,%2522Lassen%2522,%2522Los%2520Angeles%2522,%2522Madera%2522,%2522Marin%2522,%2522Mariposa%2522,%2522Mendocino%2522,%2522Merced%2522,%2522Modoc%2522,%2522Mono%2522,%2522Monterey%2522,%2522Napa%2522,%2522Nevada%2522,%2522Orange%2522,%2522Placer%2522,%2522Plumas%2522,%2522Riverside%2522,%2522Sacramento%2522,%2522San%2520Benito%2522,%2522San%2520Bernardino%2522,%2522San%2520Diego%2522,%2522San%2520Francisco%2522,%2522San%2520Joaquin%2522,%2522San%2520Luis%2520Obispo%2522,%2522San%2520Mateo%2522,%2522Santa%2520Barbara%2522,%2522Santa%2520Clara%2522,%2522Santa%2520Cruz%2522,%2522Shasta%2522,%2522Sierra%2522,%2522Siskiyou%2522,%2522Solano%2522,%2522Sonoma%2522,%2522Stanislaus%2522,%2522Sutter%2522,%2522Tehama%2522,%2522Trinity%2522,%2522Tulare%2522,%2522Tuolumne%2522,%2522Ventura%2522,%2522Yolo%2522,%2522Yuba%2522,%2522UNKNOWN%2522%2520AND%2520network=%2522SNTL%2522%2520AND%2520outServiceDate=%25222100-01-01%2522%257Cname/1991-01-01,2026-12-31/stationId,name,WTEQ::value,SNWD::value,PREC::value,PRCP:-2:value,PRCP::value,TAVG::value,TMAX::value,TMIN::value?fitToScreen=false\n",
    "# For making more changes, one can go to the main URL, experiment with parameters, and then copy the resulting CSV URL.\n",
    "# Manually save the file as \"manual/snotel/snotel-19910101-20260112.txt\"\n",
    "# and then of course, run the below code to parse and normalize it.\n",
    "# ============================\n",
    "\n",
    "# ============================\n",
    "# SNOTEL — Cell A (FINAL, resolve_out_path)\n",
    "# - Reads one wide NRCS report export (1991 → present)\n",
    "# - Melts to long tidy format (station_id × date × variable)\n",
    "# - Prefers non \"-2in\" variant when duplicates exist\n",
    "# - Writes parquet directly into results/ (out_dir)\n",
    "# ============================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------- config-driven paths ----------\n",
    "OUT_DIR = Path(CONFIG[\"out_dir\"])\n",
    "SNOTEL_MANUAL_DIR = Path(resolve_out_path(CONFIG[\"SNOTEL_MANUAL_DIR\"]))         # e.g. \"manual/snotel\"\n",
    "RAW_FILE = SNOTEL_MANUAL_DIR / Path(CONFIG[\"SNOTEL_RAW_FILENAME\"]).name\n",
    "OUT_PARQUET = OUT_DIR / CONFIG[\"SNOTEL_DAILY_LONG_PARQUET_NAME\"]               # e.g. \"snotel_daily_long.parquet\"\n",
    "\n",
    "print(\"SNOTEL Cell A starting\")\n",
    "print(\"  out_dir   :\", OUT_DIR)\n",
    "print(\"  raw file  :\", RAW_FILE)\n",
    "print(\"  out parquet:\", OUT_PARQUET)\n",
    "\n",
    "if not RAW_FILE.exists():\n",
    "    raise FileNotFoundError(f\"Missing SNOTEL raw file: {RAW_FILE}\")\n",
    "\n",
    "# ---------- read raw (robust: skip WCC metadata preamble) ----------\n",
    "def find_header_row(path: Path, max_lines: int = 500) -> int:\n",
    "    \"\"\"\n",
    "    Find the first line index (0-based) that looks like the actual CSV header.\n",
    "    WCC report exports often include a preamble before the header.\n",
    "    \"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i >= max_lines:\n",
    "                break\n",
    "            s = line.strip().lower()\n",
    "            # header usually starts with 'date' and contains at least one '(' stationId ')'\n",
    "            if s.startswith(\"date\") and \"(\" in s and \")\" in s and \",\" in s:\n",
    "                return i\n",
    "            # sometimes it's literally just 'date,' without station ids on the same line (rare)\n",
    "            if s.startswith(\"date,\"):\n",
    "                return i\n",
    "    raise ValueError(f\"Could not find header row in first {max_lines} lines of {path}\")\n",
    "\n",
    "header_row = find_header_row(RAW_FILE)\n",
    "print(f\"SNOTEL: detected CSV header at line {header_row+1} (1-based)\")\n",
    "\n",
    "df_raw = pd.read_csv(\n",
    "    RAW_FILE,\n",
    "    header=header_row,\n",
    "    low_memory=False,\n",
    "    encoding=\"utf-8\",\n",
    "    encoding_errors=\"replace\",\n",
    ")\n",
    "# First column is date in this NRCS export\n",
    "df_raw.rename(columns={df_raw.columns[0]: \"date\"}, inplace=True)\n",
    "df_raw[\"date\"] = pd.to_datetime(df_raw[\"date\"], errors=\"coerce\", utc=True)\n",
    "if df_raw[\"date\"].isna().any():\n",
    "    bad = df_raw[df_raw[\"date\"].isna()].head(5)\n",
    "    raise ValueError(f\"Found NaT dates after parsing date column; sample:\\n{bad}\")\n",
    "\n",
    "# ---------- helper to parse column names ----------\n",
    "def parse_col(col: str):\n",
    "    \"\"\"\n",
    "    Expected column pattern:\n",
    "      '<station name> (<stationId>) <variable label...>'\n",
    "    Returns (station_name, station_id, variable_key, is_minus2_variant) or None\n",
    "    \"\"\"\n",
    "    m = re.match(r\"(.+?)\\s+\\((\\d+)\\)\\s+(.+)\", col)\n",
    "    if not m:\n",
    "        return None\n",
    "\n",
    "    station_name = m.group(1).strip()\n",
    "    station_id = int(m.group(2))\n",
    "    rest = m.group(3)\n",
    "\n",
    "    # Canonical variables\n",
    "    if \"Snow Water Equivalent\" in rest:\n",
    "        var = \"swe_in\"\n",
    "    elif \"Snow Depth\" in rest:\n",
    "        var = \"snow_depth_in\"\n",
    "    elif \"Precipitation Increment\" in rest:\n",
    "        var = \"precip_increment_in\"\n",
    "    elif \"Precipitation Accumulation\" in rest:\n",
    "        var = \"precip_accum_in\"\n",
    "    elif \"Air Temperature Average\" in rest:\n",
    "        var = \"tavg_f\"\n",
    "    elif \"Air Temperature Maximum\" in rest:\n",
    "        var = \"tmax_f\"\n",
    "    elif \"Air Temperature Minimum\" in rest:\n",
    "        var = \"tmin_f\"\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    is_minus2 = \"-2in\" in rest\n",
    "    return station_name, station_id, var, is_minus2\n",
    "\n",
    "# ---------- melt wide -> long ----------\n",
    "records = []\n",
    "for col in df_raw.columns[1:]:\n",
    "    parsed = parse_col(col)\n",
    "    if parsed is None:\n",
    "        continue\n",
    "\n",
    "    station_name, station_id, var, is_minus2 = parsed\n",
    "\n",
    "    s = pd.to_numeric(df_raw[col], errors=\"coerce\").astype(\"float32\")\n",
    "\n",
    "    # Basic validity cleaning (keep conservative; do more later if needed)\n",
    "    if var.startswith(\"t\"):\n",
    "        # Fahrenheit: discard extreme negatives (sentinels)\n",
    "        s = s.where(s > -50)\n",
    "    else:\n",
    "        # SWE/Depth/Precip should not be negative\n",
    "        s = s.where(s >= 0)\n",
    "\n",
    "    records.append(pd.DataFrame({\n",
    "        \"date\": df_raw[\"date\"],\n",
    "        \"station_id\": station_id,\n",
    "        \"station_name\": station_name,\n",
    "        \"variable\": var,\n",
    "        \"value\": s,\n",
    "        \"minus2_variant\": is_minus2\n",
    "    }))\n",
    "\n",
    "if not records:\n",
    "    raise RuntimeError(\n",
    "        \"Parsed zero SNOTEL data columns. The column naming pattern likely changed. \"\n",
    "        \"Inspect df_raw.columns[:50] to update parse_col().\"\n",
    "    )\n",
    "\n",
    "df_long = pd.concat(records, ignore_index=True)\n",
    "\n",
    "# ---------- resolve duplicate variants (prefer non '-2in') ----------\n",
    "df_long.sort_values(\n",
    "    by=[\"date\", \"station_id\", \"variable\", \"minus2_variant\"],  # False first -> non -2in preferred\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "df_long = (\n",
    "    df_long\n",
    "    .drop_duplicates(subset=[\"date\", \"station_id\", \"variable\"], keep=\"first\")\n",
    "    .drop(columns=[\"minus2_variant\"])\n",
    ")\n",
    "\n",
    "df_long[\"source\"] = \"SNOTEL\"\n",
    "\n",
    "# ---------- sanity summary ----------\n",
    "print(\"SNOTEL Cell A summary\")\n",
    "print(\"  rows      :\", len(df_long))\n",
    "print(\"  stations  :\", df_long[\"station_id\"].nunique())\n",
    "print(\"  variables :\", sorted(df_long[\"variable\"].unique()))\n",
    "print(\"  date min  :\", df_long[\"date\"].min())\n",
    "print(\"  date max  :\", df_long[\"date\"].max())\n",
    "\n",
    "# ---------- write (atomic) ----------\n",
    "tmp_path = OUT_PARQUET.with_suffix(\".tmp.parquet\")\n",
    "df_long.to_parquet(tmp_path, index=False)\n",
    "tmp_path.replace(OUT_PARQUET)\n",
    "\n",
    "print(\"Saved:\", OUT_PARQUET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24b138b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# SNOTEL — Station metadata (AWDB)\n",
    "# Builds station_id -> lat/lon lookup for gridding\n",
    "# ============================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from urllib.parse import quote\n",
    "\n",
    "OUT_DIR = Path(CONFIG[\"out_dir\"])\n",
    "SNOTEL_LONG_PATH = OUT_DIR / CONFIG.get(\"SNOTEL_DAILY_LONG_PARQUET_NAME\", \"snotel_daily_long.parquet\")\n",
    "META_OUT = OUT_DIR / CONFIG.get(\"SNOTEL_STATION_META_PARQUET_NAME\", \"snotel_station_metadata.parquet\")\n",
    "\n",
    "AWDB_URL_TMPL = CONFIG.get(\n",
    "    \"AWDB_META_URL_TEMPLATE\",\n",
    "    \"https://wcc.sc.egov.usda.gov/awdbRestApi/services/v1/stations?stationTriplets={triplets}&elements=\"\n",
    ")\n",
    "\n",
    "if not SNOTEL_LONG_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Missing SNOTEL long parquet: {SNOTEL_LONG_PATH}\")\n",
    "\n",
    "df_long = pd.read_parquet(SNOTEL_LONG_PATH, columns=[\"station_id\", \"station_name\"])\n",
    "station_ids = sorted(df_long[\"station_id\"].dropna().astype(int).unique().tolist())\n",
    "\n",
    "print(\"SNOTEL station metadata starting\")\n",
    "print(\"  stations:\", len(station_ids))\n",
    "print(\"  out     :\", META_OUT)\n",
    "\n",
    "# If cached, load and only fetch missing\n",
    "if META_OUT.exists():\n",
    "    meta = pd.read_parquet(META_OUT)\n",
    "    have = set(meta[\"station_id\"].astype(int).unique().tolist())\n",
    "    missing = [sid for sid in station_ids if sid not in have]\n",
    "    print(f\"  cached  : {len(have)} stations | missing: {len(missing)}\")\n",
    "else:\n",
    "    meta = pd.DataFrame()\n",
    "    missing = station_ids\n",
    "\n",
    "def fetch_awdb(triplets: list[str]) -> pd.DataFrame:\n",
    "    # AWDB expects comma-separated triplets like \"1234:CA:SNTL,5678:CA:SNTL\"\n",
    "    trip_str = \",\".join(triplets)\n",
    "    url = AWDB_URL_TMPL.format(triplets=quote(trip_str, safe=\",:\"))\n",
    "    r = requests.get(url, timeout=120)\n",
    "    r.raise_for_status()\n",
    "    js = r.json()\n",
    "    # AWDB returns {'stations': [...]} or direct list depending on endpoint version\n",
    "    rows = js.get(\"stations\", js) if isinstance(js, dict) else js\n",
    "    return pd.json_normalize(rows)\n",
    "\n",
    "rows = []\n",
    "batch_size = 80  # conservative; avoids long URLs\n",
    "for i in range(0, len(missing), batch_size):\n",
    "    batch = missing[i:i+batch_size]\n",
    "    triplets = [f\"{sid}:CA:SNTL\" for sid in batch]  # CA SNOTEL triplet format\n",
    "    try:\n",
    "        got = fetch_awdb(triplets)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"AWDB metadata fetch failed for batch {i//batch_size}: {e}\")\n",
    "    rows.append(got)\n",
    "    if (i//batch_size + 1) % 5 == 0 or (i + batch_size) >= len(missing):\n",
    "        print(f\"  fetched batch {i//batch_size + 1} | total rows so far: {sum(len(x) for x in rows):,}\")\n",
    "\n",
    "new = pd.concat(rows, ignore_index=True) if rows else pd.DataFrame()\n",
    "\n",
    "# Normalize / keep only what we need; AWDB fields can vary slightly.\n",
    "# We key on 'stationTriplet' and/or 'stationId' when present.\n",
    "if not new.empty:\n",
    "    # stationId sometimes present; otherwise parse from stationTriplet\n",
    "    if \"stationId\" in new.columns:\n",
    "        new[\"station_id\"] = pd.to_numeric(new[\"stationId\"], errors=\"coerce\")\n",
    "    elif \"stationTriplet\" in new.columns:\n",
    "        new[\"station_id\"] = pd.to_numeric(new[\"stationTriplet\"].str.split(\":\").str[0], errors=\"coerce\")\n",
    "    else:\n",
    "        raise KeyError(f\"AWDB response missing stationId/stationTriplet; columns={list(new.columns)[:50]}\")\n",
    "\n",
    "    # lat/lon fields\n",
    "    lat_col = \"latitude\" if \"latitude\" in new.columns else (\"lat\" if \"lat\" in new.columns else None)\n",
    "    lon_col = \"longitude\" if \"longitude\" in new.columns else (\"lon\" if \"lon\" in new.columns else None)\n",
    "    if lat_col is None or lon_col is None:\n",
    "        raise KeyError(f\"AWDB response missing lat/lon columns; columns={list(new.columns)[:50]}\")\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        \"station_id\": new[\"station_id\"].astype(\"Int64\"),\n",
    "        \"lat\": pd.to_numeric(new[lat_col], errors=\"coerce\"),\n",
    "        \"lon\": pd.to_numeric(new[lon_col], errors=\"coerce\"),\n",
    "        \"elev_m\": pd.to_numeric(new.get(\"elevation\", np.nan), errors=\"coerce\"),\n",
    "        \"state\": new.get(\"state\", \"CA\"),\n",
    "        \"station_name_meta\": new.get(\"name\", pd.NA),\n",
    "        \"station_triplet\": new.get(\"stationTriplet\", pd.NA),\n",
    "    }).dropna(subset=[\"station_id\"])\n",
    "else:\n",
    "    out = pd.DataFrame(columns=[\"station_id\",\"lat\",\"lon\",\"elev_m\",\"state\",\"station_name_meta\",\"station_triplet\"])\n",
    "\n",
    "# Merge with existing cache\n",
    "if meta.empty:\n",
    "    meta2 = out\n",
    "else:\n",
    "    meta2 = pd.concat([meta, out], ignore_index=True)\n",
    "    meta2 = meta2.drop_duplicates(subset=[\"station_id\"], keep=\"last\")\n",
    "\n",
    "# Basic sanity\n",
    "meta2[\"station_id\"] = meta2[\"station_id\"].astype(int)\n",
    "ok = meta2.dropna(subset=[\"lat\",\"lon\"])\n",
    "print(\"  meta rows:\", len(meta2), \"| with lat/lon:\", len(ok))\n",
    "\n",
    "tmp = META_OUT.with_suffix(\".tmp.parquet\")\n",
    "meta2.to_parquet(tmp, index=False)\n",
    "tmp.replace(META_OUT)\n",
    "print(\"Saved:\", META_OUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1e8553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# SNOTEL — Cell B\n",
    "# Grid stations to 3km grid; write daily shards; stitch final parquet (analysis window)\n",
    "# ============================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import geopandas as gpd\n",
    "\n",
    "# ---------- paths ----------\n",
    "OUT_DIR = Path(CONFIG[\"out_dir\"])\n",
    "\n",
    "GRID_PATH = Path(resolve_out_path(CONFIG.get(\"GRID_FILENAME\", \"grid_3000m_CA_epsg3310.parquet\")))\n",
    "SNOTEL_LONG_PATH = OUT_DIR / CONFIG.get(\"SNOTEL_DAILY_LONG_PARQUET_NAME\", \"snotel_daily_long.parquet\")\n",
    "\n",
    "GRIDMAP_PATH = OUT_DIR / CONFIG.get(\"SNOTEL_GRIDMAP_PARQUET_NAME\", \"snotel_station_to_grid_3310.parquet\")\n",
    "SHARDS_DIR = OUT_DIR / CONFIG.get(\"SNOTEL_GRID_SHARDS_DIRNAME\", \"derived/snotel_grid_shards\")\n",
    "SHARDS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "start = pd.to_datetime(CONFIG[\"start_date\"]).tz_localize(\"UTC\")\n",
    "end   = pd.to_datetime(CONFIG[\"end_date\"]).tz_localize(\"UTC\")\n",
    "\n",
    "final_name_tmpl = CONFIG.get(\n",
    "    \"SNOTEL_DAILY_GRID_PARQUET_NAME\",\n",
    "    \"snotel_daily_grid_CA_3000m_epsg3310_{start}_{end}.parquet\"\n",
    ")\n",
    "FINAL_PATH = OUT_DIR / final_name_tmpl.format(\n",
    "    start=start.strftime(\"%Y%m%d\"),\n",
    "    end=end.strftime(\"%Y%m%d\")\n",
    ")\n",
    "\n",
    "print(\"SNOTEL Cell B starting\")\n",
    "print(\"  Grid     :\", GRID_PATH)\n",
    "print(\"  SNOTEL A :\", SNOTEL_LONG_PATH)\n",
    "print(\"  Gridmap  :\", GRIDMAP_PATH)\n",
    "print(\"  Shards   :\", SHARDS_DIR)\n",
    "print(\"  FINAL    :\", FINAL_PATH)\n",
    "\n",
    "if not GRID_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Missing grid parquet: {GRID_PATH}\")\n",
    "if not SNOTEL_LONG_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Missing SNOTEL long parquet (Cell A output): {SNOTEL_LONG_PATH}\")\n",
    "\n",
    "# ---------- load grid ----------\n",
    "ggrid = gpd.read_parquet(GRID_PATH)\n",
    "if \"grid_id\" not in ggrid.columns:\n",
    "    raise KeyError(\"Grid file must contain grid_id\")\n",
    "if ggrid.crs is None:\n",
    "    raise ValueError(\"Grid GeoDataFrame missing CRS\")\n",
    "# ensure EPSG 3310 ops\n",
    "ggrid = ggrid.to_crs(epsg=int(CONFIG.get(\"OPS_EPSG\", 3310)))\n",
    "\n",
    "# precompute centroids for nearest mapping\n",
    "ggrid_cent = ggrid.copy()\n",
    "ggrid_cent[\"geometry\"] = ggrid_cent.geometry.centroid\n",
    "ggrid_cent = ggrid_cent[[\"grid_id\", \"geometry\"]]\n",
    "\n",
    "# ---------- load SNOTEL long ----------\n",
    "df = pd.read_parquet(SNOTEL_LONG_PATH)\n",
    "\n",
    "# Slice to analysis window early (massive speedup)\n",
    "df = df[(df[\"date\"] >= start) & (df[\"date\"] <= end)].copy()\n",
    "if df.empty:\n",
    "    raise ValueError(\"SNOTEL long table has no rows in analysis window. Check date parsing/timezones.\")\n",
    "\n",
    "META_PATH = OUT_DIR / CONFIG.get(\"SNOTEL_STATION_META_PARQUET_NAME\", \"snotel_station_metadata.parquet\")\n",
    "if not META_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Missing station metadata parquet: {META_PATH}. Run the SNOTEL metadata cell first.\")\n",
    "\n",
    "meta = pd.read_parquet(META_PATH)\n",
    "meta[\"station_id\"] = meta[\"station_id\"].astype(int)\n",
    "\n",
    "# Build station table from metadata + names from long table\n",
    "stations = (\n",
    "    df[[\"station_id\", \"station_name\"]]\n",
    "    .drop_duplicates(subset=[\"station_id\"])\n",
    "    .merge(meta[[\"station_id\",\"lat\",\"lon\"]], on=\"station_id\", how=\"left\")\n",
    ")\n",
    "\n",
    "missing_ll = stations[\"lat\"].isna().sum()\n",
    "if missing_ll:\n",
    "    print(f\"[WARN] {missing_ll} stations missing lat/lon; they will be dropped for gridding.\")\n",
    "stations = stations.dropna(subset=[\"lat\",\"lon\"])\n",
    "\n",
    "\n",
    "# ---------- station -> grid mapping (cached) ----------\n",
    "if GRIDMAP_PATH.exists():\n",
    "    gridmap = pd.read_parquet(GRIDMAP_PATH)\n",
    "    print(f\"Loaded existing gridmap ({len(gridmap):,} stations)\")\n",
    "else:\n",
    "    gstations = gpd.GeoDataFrame(\n",
    "        stations,\n",
    "        geometry=gpd.points_from_xy(stations[\"lon\"], stations[\"lat\"]),\n",
    "        crs=\"EPSG:4326\"\n",
    "    ).to_crs(ggrid_cent.crs)\n",
    "\n",
    "    # nearest grid centroid\n",
    "    joined = gpd.sjoin_nearest(\n",
    "        gstations,\n",
    "        ggrid_cent,\n",
    "        how=\"left\",\n",
    "        distance_col=\"dist_m\"\n",
    "    )\n",
    "\n",
    "    gridmap = joined[[\"station_id\", \"grid_id\", \"dist_m\"]].copy()\n",
    "    gridmap.to_parquet(GRIDMAP_PATH, index=False)\n",
    "    print(f\"Saved gridmap ({len(gridmap):,} stations) -> {GRIDMAP_PATH}\")\n",
    "\n",
    "# attach grid_id to all records\n",
    "df = df.merge(gridmap[[\"station_id\", \"grid_id\"]], on=\"station_id\", how=\"inner\")\n",
    "if df.empty:\n",
    "    raise ValueError(\"After station->grid join, SNOTEL has zero rows. Check station_id alignment.\")\n",
    "\n",
    "# ---------- pick variables of interest (you can expand later) ----------\n",
    "# Keep everything for now; but when aggregating, we compute grid-level stats per variable.\n",
    "keep_vars = [\n",
    "    \"precip_increment_in\",  # primary precip\n",
    "    \"swe_in\",\n",
    "    \"snow_depth_in\",\n",
    "    \"tavg_f\",\n",
    "    \"tmax_f\",\n",
    "    \"tmin_f\",\n",
    "    \"precip_accum_in\",      # kept but not primary\n",
    "]\n",
    "df = df[df[\"variable\"].isin(keep_vars)].copy()\n",
    "\n",
    "# ---------- aggregate per grid_id x day x variable ----------\n",
    "# We'll compute mean across stations in same grid cell, plus count.\n",
    "df[\"date_day\"] = df[\"date\"].dt.floor(\"D\")\n",
    "\n",
    "all_days = pd.date_range(start=start.floor(\"D\"), end=end.floor(\"D\"), freq=\"D\", tz=\"UTC\")\n",
    "\n",
    "wrote = skipped = 0\n",
    "for i, day in enumerate(all_days, start=1):\n",
    "    day_tag = day.strftime(\"%Y-%m-%d\")\n",
    "    shard_path = SHARDS_DIR / f\"snotel_grid_{day_tag}.parquet\"\n",
    "\n",
    "    if shard_path.exists():\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "    dday = df[df[\"date_day\"] == day]\n",
    "    if dday.empty:\n",
    "        # still write an empty shard to mark completion (resume-safe)\n",
    "        empty = pd.DataFrame(columns=[\"grid_id\", \"date\", \"variable\", \"value_mean\", \"n_stations\"])\n",
    "        empty.to_parquet(shard_path, index=False)\n",
    "        wrote += 1\n",
    "        continue\n",
    "\n",
    "    agg = (\n",
    "        dday.groupby([\"grid_id\", \"date_day\", \"variable\"], as_index=False)\n",
    "        .agg(\n",
    "            value_mean=(\"value\", \"mean\"),\n",
    "            n_stations=(\"value\", \"count\")\n",
    "        )\n",
    "        .rename(columns={\"date_day\": \"date\"})\n",
    "    )\n",
    "\n",
    "    # Optional: pivot to wide per day for easier merges later\n",
    "    wide = agg.pivot_table(\n",
    "        index=[\"grid_id\", \"date\"],\n",
    "        columns=\"variable\",\n",
    "        values=\"value_mean\"\n",
    "    ).reset_index()\n",
    "\n",
    "    # attach station counts in wide form too\n",
    "    counts = agg.pivot_table(\n",
    "        index=[\"grid_id\", \"date\"],\n",
    "        columns=\"variable\",\n",
    "        values=\"n_stations\"\n",
    "    ).add_prefix(\"n_\").reset_index()\n",
    "\n",
    "    out = wide.merge(counts, on=[\"grid_id\", \"date\"], how=\"left\")\n",
    "\n",
    "    out.to_parquet(shard_path, index=False)\n",
    "    wrote += 1\n",
    "\n",
    "    if i % 20 == 0 or i == len(all_days):\n",
    "        print(f\"  day {i}/{len(all_days)} | wrote={wrote} skipped={skipped} | last={day_tag} | rows={len(out):,}\")\n",
    "\n",
    "print(\"SNOTEL Cell B shards complete\")\n",
    "print(\"  wrote  :\", wrote)\n",
    "print(\"  skipped:\", skipped)\n",
    "\n",
    "# ---------- stitch final ----------\n",
    "shards = sorted(SHARDS_DIR.glob(\"snotel_grid_*.parquet\"))\n",
    "parts = [pd.read_parquet(p) for p in shards]\n",
    "final = pd.concat(parts, ignore_index=True)\n",
    "\n",
    "# Some days may be empty shards; drop them\n",
    "final = final.dropna(subset=[\"grid_id\", \"date\"], how=\"any\")\n",
    "\n",
    "final.to_parquet(FINAL_PATH, index=False)\n",
    "print(\"Saved FINAL:\", FINAL_PATH, \"rows=\", len(final))\n",
    "print(final.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b088b5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# ERA5 MASTER CELL (CDS API) — hardened for ZIP/Nc4 bundles + resume-safe\n",
    "# - Downloads CA-subset ERA5-Land data into results/{era5_raw,era5_clean}\n",
    "# - Handles CDS returning ZIP bundles even when requesting \"netcdf\"\n",
    "# - Skips downloads if a usable output already exists (either final .nc or existing .zip that has been extracted)\n",
    "# ============================\n",
    "\n",
    "from pathlib import Path\n",
    "import calendar\n",
    "import time\n",
    "import zipfile\n",
    "import cdsapi\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "# ---------- toggles (edit these per run) ----------\n",
    "DO_DAILY_ANALYSIS_WINDOW = True          # 2024-06-01 .. 2024-10-31 (or CONFIG window)\n",
    "DO_MONTHLY_BASELINE = True               # baseline monthly means (1991-2020 by default)\n",
    "\n",
    "# If you want to limit daily downloads to only some months, set e.g. [6,7,8,9,10]\n",
    "DAILY_MONTHS_OVERRIDE = None  # e.g. [6,7,8,9,10] or None to auto from CONFIG start/end\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def bbox_to_area(bbox: dict) -> list[float]:\n",
    "    # CDS expects [North, West, South, East]\n",
    "    return [float(bbox[\"nwlat\"]), float(bbox[\"nwlng\"]), float(bbox[\"selat\"]), float(bbox[\"selng\"])]\n",
    "\n",
    "def ensure_dir(p: Path) -> Path:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "    return p\n",
    "\n",
    "def is_zip(path: Path) -> bool:\n",
    "    if not path.exists() or path.stat().st_size < 4:\n",
    "        return False\n",
    "    head = path.read_bytes()[:4]\n",
    "    return head.startswith(b\"PK\\x03\\x04\") or head.startswith(b\"PK\\x05\\x06\") or head.startswith(b\"PK\\x07\\x08\")\n",
    "\n",
    "def looks_like_html(path: Path) -> bool:\n",
    "    if not path.exists() or path.stat().st_size < 16:\n",
    "        return False\n",
    "    head = path.read_bytes()[:64].lstrip().lower()\n",
    "    return head.startswith(b\"<!doctype\") or head.startswith(b\"<html\") or head.startswith(b\"{\")  # html/json-ish\n",
    "\n",
    "def month_range_from_dates(d1, d2):\n",
    "    months = []\n",
    "    y, m = d1.year, d1.month\n",
    "    while (y, m) <= (d2.year, d2.month):\n",
    "        months.append((y, m))\n",
    "        if m == 12:\n",
    "            y += 1\n",
    "            m = 1\n",
    "        else:\n",
    "            m += 1\n",
    "    return months\n",
    "\n",
    "def retry_sleep(base: float, attempt: int) -> float:\n",
    "    return base * attempt\n",
    "\n",
    "def safe_unlink(p: Path) -> None:\n",
    "    try:\n",
    "        if p.exists():\n",
    "            p.unlink()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# --- variable aliases (CDS request names vs common ERA5 variable names in NetCDF)\n",
    "ALIASES = {\n",
    "    \"2m_temperature\": [\"t2m\", \"2m_temperature\"],\n",
    "    \"snow_depth_water_equivalent\": [\"sd\", \"snow_depth_water_equivalent\", \"sdwe\"],\n",
    "    \"volumetric_soil_water_layer_1\": [\"swvl1\", \"volumetric_soil_water_layer_1\"],\n",
    "}\n",
    "\n",
    "def open_nc_quick(path: Path):\n",
    "    # Keep this minimal to avoid loading actual arrays; just metadata\n",
    "    return xr.open_dataset(path, engine=\"netcdf4\", decode_times=False)\n",
    "\n",
    "def validate_era5_netcdf(path: Path, requested_vars: list[str]) -> bool:\n",
    "    \"\"\"\n",
    "    True if path is a readable NetCDF and appears to contain expected structure.\n",
    "    We do NOT enforce exact variable names (CDS stream files vary), but we do:\n",
    "    - reject HTML/JSON payloads\n",
    "    - reject ZIPs masquerading as .nc\n",
    "    - require: at least one plausible time dim AND lat/lon coordinate presence\n",
    "    - require: at least one data var that matches one of the requested var aliases (soft check)\n",
    "    \"\"\"\n",
    "    path = Path(path)\n",
    "    if not path.exists() or path.stat().st_size < 5_000:\n",
    "        return False\n",
    "    if is_zip(path) or looks_like_html(path):\n",
    "        return False\n",
    "    try:\n",
    "        ds = open_nc_quick(path)\n",
    "        # time dim can be time or valid_time\n",
    "        has_time = (\"time\" in ds.dims) or (\"valid_time\" in ds.dims)\n",
    "        # coord names often latitude/longitude, sometimes lat/lon\n",
    "        has_latlon = ((\"latitude\" in ds.variables) and (\"longitude\" in ds.variables)) or ((\"lat\" in ds.variables) and (\"lon\" in ds.variables))\n",
    "        if not (has_time and has_latlon):\n",
    "            return False\n",
    "\n",
    "        present = set(ds.data_vars.keys()) | set(ds.variables.keys())\n",
    "        # soft match: at least 1 of requested vars appears via aliases\n",
    "        want_aliases = []\n",
    "        for v in requested_vars:\n",
    "            want_aliases.extend(ALIASES.get(v, [v]))\n",
    "        if any(a in present for a in want_aliases):\n",
    "            return True\n",
    "\n",
    "        # If no alias hits, still allow if dataset has \"reasonable\" number of data_vars (non-empty)\n",
    "        return len(ds.data_vars) > 0\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def choose_best_nc(nc_files: list[Path], requested_vars: list[str]) -> Path:\n",
    "    \"\"\"\n",
    "    From a set of extracted NetCDF files, pick the one most likely to contain the full schema.\n",
    "    Score: alias hits (heavy) + number of data vars (light).\n",
    "    \"\"\"\n",
    "    want_aliases = []\n",
    "    for v in requested_vars:\n",
    "        want_aliases.extend(ALIASES.get(v, [v]))\n",
    "\n",
    "    best = None\n",
    "    best_score = -1\n",
    "    for fp in nc_files:\n",
    "        try:\n",
    "            if not validate_era5_netcdf(fp, requested_vars):\n",
    "                continue\n",
    "            ds = open_nc_quick(fp)\n",
    "            present = set(ds.data_vars.keys()) | set(ds.variables.keys())\n",
    "            alias_hits = sum(1 for a in want_aliases if a in present)\n",
    "            score = alias_hits * 100 + len(ds.data_vars)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best = fp\n",
    "        except Exception:\n",
    "            continue\n",
    "    if best is None:\n",
    "        raise RuntimeError(f\"Could not find a valid NetCDF among extracted files: {[p.name for p in nc_files]}\")\n",
    "    return best\n",
    "\n",
    "def extract_zip_to_final(zip_path: Path, final_nc: Path, requested_vars: list[str]) -> str:\n",
    "    \"\"\"\n",
    "    Extract zip, select best .nc, write/replace final_nc.\n",
    "    Keeps the zip in place for provenance/resume.\n",
    "    \"\"\"\n",
    "    zip_path = Path(zip_path)\n",
    "    final_nc = Path(final_nc)\n",
    "\n",
    "    if not zip_path.exists():\n",
    "        raise FileNotFoundError(f\"ZIP missing: {zip_path}\")\n",
    "\n",
    "    extract_dir = zip_path.parent / f\"_{zip_path.stem}_extracted\"\n",
    "    ensure_dir(extract_dir)\n",
    "\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as z:\n",
    "        z.extractall(extract_dir)\n",
    "\n",
    "    nc_files = sorted(extract_dir.glob(\"*.nc\"))\n",
    "    if not nc_files:\n",
    "        raise RuntimeError(f\"ZIP contained no .nc files: {zip_path.name}\")\n",
    "\n",
    "    best = choose_best_nc(nc_files, requested_vars)\n",
    "\n",
    "    # Replace final_nc atomically-ish\n",
    "    tmp = final_nc.with_suffix(\".tmp.nc\")\n",
    "    safe_unlink(tmp)\n",
    "    best.replace(tmp)\n",
    "    safe_unlink(final_nc)\n",
    "    tmp.replace(final_nc)\n",
    "\n",
    "    if not validate_era5_netcdf(final_nc, requested_vars):\n",
    "        raise RuntimeError(f\"Extracted NetCDF failed validation: {final_nc}\")\n",
    "\n",
    "    return \"extracted\"\n",
    "\n",
    "# ---------- config ----------\n",
    "OUT_DIR = Path(CONFIG[\"out_dir\"])\n",
    "RAW_DIR = ensure_dir(OUT_DIR / CONFIG.get(\"ERA5_RAW_DIRNAME\", \"era5_raw\"))\n",
    "CLEAN_DIR = ensure_dir(OUT_DIR / CONFIG.get(\"ERA5_CLEAN_DIRNAME\", \"era5_clean\"))  # placeholder for later\n",
    "\n",
    "BBOX = CONFIG[\"bbox\"]\n",
    "AREA = bbox_to_area(BBOX)\n",
    "\n",
    "DAILY_DATASET = CONFIG.get(\"ERA5_DAILY_DATASET\", \"derived-era5-land-daily-statistics\")\n",
    "MONTHLY_DATASET = CONFIG.get(\"ERA5_MONTHLY_DATASET\", \"reanalysis-era5-land-monthly-means\")\n",
    "\n",
    "VARS_DAILY = CONFIG.get(\"ERA5_VARIABLES_DAILY\", [\n",
    "    \"2m_temperature\",\n",
    "    \"snow_depth_water_equivalent\",\n",
    "    \"volumetric_soil_water_layer_1\",\n",
    "])\n",
    "\n",
    "MAX_RETRIES = int(CONFIG.get(\"ERA5_MAX_RETRIES\", 6))\n",
    "BACKOFF_BASE = float(CONFIG.get(\"ERA5_BACKOFF_BASE_S\", 25))\n",
    "TIMEOUT_S = int(CONFIG.get(\"ERA5_TIMEOUT_S\", 600))\n",
    "\n",
    "analysis_start = pd.to_datetime(CONFIG[\"start_date\"]).date()\n",
    "analysis_end   = pd.to_datetime(CONFIG[\"end_date\"]).date()\n",
    "\n",
    "baseline_start = pd.to_datetime(CONFIG.get(\"BASELINE_START_DATE\", \"1991-01-01\")).date()\n",
    "baseline_end   = pd.to_datetime(CONFIG.get(\"BASELINE_END_DATE\", \"2020-12-31\")).date()\n",
    "\n",
    "print(\"ERA5 master cell starting\")\n",
    "print(\"  out_dir :\", OUT_DIR)\n",
    "print(\"  RAW_DIR :\", RAW_DIR)\n",
    "print(\"  CLEAN_DIR (future use):\", CLEAN_DIR)\n",
    "print(\"  AREA   :\", AREA, \"(N,W,S,E)\")\n",
    "print(\"  Daily dataset  :\", DAILY_DATASET)\n",
    "print(\"  Monthly dataset:\", MONTHLY_DATASET)\n",
    "print(\"  Daily vars:\", VARS_DAILY)\n",
    "print(\"  Analysis window :\", analysis_start, \"→\", analysis_end)\n",
    "print(\"  Baseline window :\", baseline_start, \"→\", baseline_end)\n",
    "\n",
    "client = cdsapi.Client()\n",
    "\n",
    "def cds_download_zip_or_nc(dataset: str, request: dict, final_nc: Path, zip_path: Path) -> str:\n",
    "    \"\"\"\n",
    "    Resume-safe logic:\n",
    "    - If final_nc exists and validates -> SKIP\n",
    "    - Else if zip exists -> extract to final_nc (if needed)\n",
    "    - Else download to zip_path (temporary .part), then extract to final_nc\n",
    "    \"\"\"\n",
    "    final_nc = Path(final_nc)\n",
    "    zip_path = Path(zip_path)\n",
    "\n",
    "    # 1) If final output already exists and is valid, skip everything.\n",
    "    if validate_era5_netcdf(final_nc, VARS_DAILY):\n",
    "        print(f\"[SKIP] {final_nc.name} already present and valid\")\n",
    "        return \"skipped\"\n",
    "\n",
    "    # 2) If there's an existing .nc that is actually a ZIP (common if previously saved as .nc), handle it.\n",
    "    if final_nc.exists() and is_zip(final_nc):\n",
    "        renamed = final_nc.with_suffix(\".zip\")\n",
    "        if not renamed.exists():\n",
    "            final_nc.rename(renamed)\n",
    "            print(f\"[INFO] Renamed ZIP-masquerading file: {final_nc.name} -> {renamed.name}\")\n",
    "        zip_path = renamed  # prefer the renamed zip going forward\n",
    "\n",
    "    # 3) If we already have a zip, try extracting (and then validate).\n",
    "    if zip_path.exists() and is_zip(zip_path):\n",
    "        print(f\"[EXTRACT] {zip_path.name} -> {final_nc.name}\")\n",
    "        return extract_zip_to_final(zip_path, final_nc, VARS_DAILY)\n",
    "\n",
    "    # 4) Download (to .part first), then decide whether we got zip or nc, then validate/extract.\n",
    "    tmp = zip_path.with_suffix(zip_path.suffix + \".part\")\n",
    "    safe_unlink(tmp)\n",
    "\n",
    "    for attempt in range(1, MAX_RETRIES + 1):\n",
    "        try:\n",
    "            print(f\"[DL] {zip_path.name}  (dataset={dataset})\")\n",
    "            # download into tmp path\n",
    "            client.retrieve(dataset, request, str(tmp))\n",
    "\n",
    "            if not tmp.exists() or tmp.stat().st_size < 5_000:\n",
    "                raise RuntimeError(f\"Download produced an empty/small file: {tmp}\")\n",
    "\n",
    "            # move tmp into zip_path (even if it’s actually .nc content; we’ll detect below)\n",
    "            safe_unlink(zip_path)\n",
    "            tmp.replace(zip_path)\n",
    "\n",
    "            # if it's a zip, extract -> final_nc\n",
    "            if is_zip(zip_path):\n",
    "                print(f\"[OK] downloaded ZIP ({zip_path.stat().st_size/1e6:.1f} MB), extracting...\")\n",
    "                res = extract_zip_to_final(zip_path, final_nc, VARS_DAILY)\n",
    "                print(f\"[OK] {final_nc.name} ready ({final_nc.stat().st_size/1e6:.1f} MB)\")\n",
    "                return \"downloaded\"\n",
    "\n",
    "            # if it's not a zip, it might actually be a netcdf payload; rename to final_nc\n",
    "            if looks_like_html(zip_path):\n",
    "                raise RuntimeError(f\"CDS returned HTML/JSON payload instead of data: {zip_path}\")\n",
    "\n",
    "            # treat as NetCDF-like payload\n",
    "            safe_unlink(final_nc)\n",
    "            zip_path.replace(final_nc)\n",
    "\n",
    "            if not validate_era5_netcdf(final_nc, VARS_DAILY):\n",
    "                raise RuntimeError(f\"Downloaded payload is not a valid ERA5 NetCDF: {final_nc}\")\n",
    "\n",
    "            print(f\"[OK] {final_nc.name} ({final_nc.stat().st_size/1e6:.1f} MB)\")\n",
    "            return \"downloaded\"\n",
    "\n",
    "        except Exception as e:\n",
    "            # cleanup tmp if present\n",
    "            safe_unlink(tmp)\n",
    "            if attempt == MAX_RETRIES:\n",
    "                raise\n",
    "            sleep_s = retry_sleep(BACKOFF_BASE, attempt)\n",
    "            print(f\"[WARN] attempt {attempt}/{MAX_RETRIES} failed: {e}\")\n",
    "            print(f\"       sleeping {sleep_s:.0f}s, then retrying...\")\n",
    "            time.sleep(sleep_s)\n",
    "\n",
    "def download_daily_month(year: int, month: int):\n",
    "    ndays = calendar.monthrange(year, month)[1]\n",
    "    request = {\n",
    "        \"variable\": VARS_DAILY,\n",
    "        \"year\": f\"{year:d}\",\n",
    "        \"month\": f\"{month:02d}\",\n",
    "        \"day\": [f\"{d:02d}\" for d in range(1, ndays + 1)],\n",
    "        \"daily_statistic\": \"daily_mean\",\n",
    "        \"time_zone\": \"utc+00:00\",\n",
    "        \"frequency\": \"1_hourly\",\n",
    "        \"area\": AREA,\n",
    "        \"format\": \"netcdf\",\n",
    "    }\n",
    "    final_nc = RAW_DIR / f\"era5l_daily_CA_{year}{month:02d}_mean.nc\"\n",
    "    zip_path = RAW_DIR / f\"era5l_daily_CA_{year}{month:02d}_mean.zip\"\n",
    "    return cds_download_zip_or_nc(DAILY_DATASET, request, final_nc, zip_path)\n",
    "\n",
    "def download_monthly_baseline(year: int):\n",
    "    request = {\n",
    "        \"variable\": VARS_DAILY,\n",
    "        \"year\": f\"{year:d}\",\n",
    "        \"month\": [f\"{m:02d}\" for m in range(1, 13)],\n",
    "        \"time\": \"00:00\",\n",
    "        \"area\": AREA,\n",
    "        \"format\": \"netcdf\",\n",
    "    }\n",
    "    # You already renamed the tripping file to .zip — this logic supports both.\n",
    "    final_nc = RAW_DIR / f\"era5l_monthly_CA_{year}.nc\"\n",
    "    zip_path = RAW_DIR / f\"era5l_monthly_CA_{year}.zip\"\n",
    "    return cds_download_zip_or_nc(MONTHLY_DATASET, request, final_nc, zip_path)\n",
    "\n",
    "# ---------- run daily downloads ----------\n",
    "if DO_DAILY_ANALYSIS_WINDOW:\n",
    "    if DAILY_MONTHS_OVERRIDE is not None:\n",
    "        months = [(analysis_start.year, m) for m in DAILY_MONTHS_OVERRIDE]\n",
    "    else:\n",
    "        months = month_range_from_dates(analysis_start, analysis_end)\n",
    "\n",
    "    print(\"\\nERA5 daily downloads:\")\n",
    "    stats = {\"downloaded\": 0, \"skipped\": 0, \"extracted\": 0}\n",
    "    for (y, m) in months:\n",
    "        res = download_daily_month(y, m)\n",
    "        stats[res] += 1\n",
    "    print(\"Daily summary:\", stats)\n",
    "\n",
    "# ---------- run monthly baseline downloads ----------\n",
    "if DO_MONTHLY_BASELINE:\n",
    "    print(\"\\nERA5 monthly baseline downloads:\")\n",
    "    stats = {\"downloaded\": 0, \"skipped\": 0, \"extracted\": 0}\n",
    "    for year in range(baseline_start.year, baseline_end.year + 1):\n",
    "        res = download_monthly_baseline(year)\n",
    "        stats[res] += 1\n",
    "    print(\"Monthly baseline summary:\", stats)\n",
    "\n",
    "print(\"\\nERA5 master cell complete.\")\n",
    "print(\"Raw files are in:\", RAW_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7a62a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# ERA5-Land | Cell A: raw NetCDFs (+ extracted per-variable NCs) -> clean long-format parquet shards\n",
    "#\n",
    "# Fixes:\n",
    "# - Daily top-level .nc may contain only t2m; merges in companion variables from _extracted/*.nc\n",
    "# - Same for monthly year files if needed\n",
    "# - Assembly step never hard-fails if no shards exist; it prints diagnostics and skips assembly\n",
    "#\n",
    "# Outputs (under results/):\n",
    "#   - derived/era5_cellA_shards_daily/era5_daily_long_YYYY-MM-DD.parquet\n",
    "#   - derived/era5_cellA_shards_monthly/era5_monthly_long_YYYY-MM.parquet\n",
    "#   - era5_daily_long_analysis.parquet    (optional assembly)\n",
    "#   - era5_monthly_long_baseline.parquet  (optional assembly; OFF by default)\n",
    "#\n",
    "# Notes:\n",
    "# - Cell A only: parse, standardize variable names, convert units (t2m -> degC), write long-format.\n",
    "# - No grid mapping happens here. Cell B will map lat/lon -> HydroPulse grid and aggregate.\n",
    "# - Resume-safe: if shard exists, skip; if assembled parquet exists, skip.\n",
    "# - Reprocessing is manual: delete outputs to rebuild.\n",
    "# ============================\n",
    "\n",
    "from pathlib import Path\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "# ----------------------------\n",
    "# Config + paths\n",
    "# ----------------------------\n",
    "OUT_DIR = Path(CONFIG[\"out_dir\"])\n",
    "RAW_DIR = OUT_DIR / CONFIG.get(\"ERA5_RAW_DIRNAME\", \"era5_raw\")\n",
    "assert RAW_DIR.exists(), f\"ERA5 RAW_DIR not found: {RAW_DIR}\"\n",
    "\n",
    "analysis_start = pd.to_datetime(CONFIG[\"start_date\"]).date()\n",
    "analysis_end   = pd.to_datetime(CONFIG[\"end_date\"]).date()\n",
    "\n",
    "baseline_start = pd.to_datetime(CONFIG.get(\"BASELINE_START_DATE\", \"1991-01-01\")).date()\n",
    "baseline_end   = pd.to_datetime(CONFIG.get(\"BASELINE_END_DATE\", \"2020-12-31\")).date()\n",
    "\n",
    "VARS = CONFIG.get(\"ERA5_VARIABLES_DAILY\", [\n",
    "    \"2m_temperature\",\n",
    "    \"snow_depth_water_equivalent\",\n",
    "    \"volumetric_soil_water_layer_1\",\n",
    "])\n",
    "\n",
    "DERIVED_DIR = OUT_DIR / \"derived\"\n",
    "DERIVED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DAILY_SHARDS_DIR = DERIVED_DIR / CONFIG.get(\"ERA5_CELL_A_DAILY_SHARDS_DIRNAME\", \"era5_cellA_shards_daily\")\n",
    "MONTHLY_SHARDS_DIR = DERIVED_DIR / CONFIG.get(\"ERA5_CELL_A_MONTHLY_SHARDS_DIRNAME\", \"era5_cellA_shards_monthly\")\n",
    "DAILY_SHARDS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MONTHLY_SHARDS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DAILY_LONG_OUT = OUT_DIR / CONFIG.get(\"ERA5_DAILY_LONG_PARQUET_NAME\", \"era5_daily_long_analysis.parquet\")\n",
    "MONTHLY_LONG_OUT = OUT_DIR / CONFIG.get(\"ERA5_MONTHLY_LONG_PARQUET_NAME\", \"era5_monthly_long_baseline.parquet\")\n",
    "\n",
    "ASSEMBLE_DAILY_ANALYSIS = bool(CONFIG.get(\"ERA5_ASSEMBLE_DAILY_ANALYSIS\", True))\n",
    "ASSEMBLE_MONTHLY_BASELINE = bool(CONFIG.get(\"ERA5_ASSEMBLE_MONTHLY_BASELINE\", False))\n",
    "\n",
    "print(\"ERA5 Cell A starting\")\n",
    "print(\"  RAW_DIR   :\", RAW_DIR)\n",
    "print(\"  Analysis  :\", analysis_start, \"→\", analysis_end)\n",
    "print(\"  Baseline  :\", baseline_start, \"→\", baseline_end)\n",
    "print(\"  VARS      :\", VARS)\n",
    "print(\"  Daily shards  :\", DAILY_SHARDS_DIR)\n",
    "print(\"  Monthly shards:\", MONTHLY_SHARDS_DIR)\n",
    "print(\"  Daily long out :\", DAILY_LONG_OUT, \"| assemble:\", ASSEMBLE_DAILY_ANALYSIS)\n",
    "print(\"  Monthly long out:\", MONTHLY_LONG_OUT, \"| assemble:\", ASSEMBLE_MONTHLY_BASELINE)\n",
    "\n",
    "# ----------------------------\n",
    "# Variable alias resolution\n",
    "# ----------------------------\n",
    "ALIASES = {\n",
    "    \"2m_temperature\": [\"t2m\", \"2m_temperature\"],\n",
    "    \"snow_depth_water_equivalent\": [\"sd\", \"snow_depth_water_equivalent\", \"sdwe\"],\n",
    "    \"volumetric_soil_water_layer_1\": [\"swvl1\", \"volumetric_soil_water_layer_1\"],\n",
    "}\n",
    "\n",
    "def _pick_coord_name(ds, candidates):\n",
    "    for c in candidates:\n",
    "        if c in ds.coords or c in ds.variables:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def get_time_name(ds):\n",
    "    return _pick_coord_name(ds, [\"time\", \"valid_time\"])\n",
    "\n",
    "def get_latlon_names(ds):\n",
    "    lat = _pick_coord_name(ds, [\"latitude\", \"lat\"])\n",
    "    lon = _pick_coord_name(ds, [\"longitude\", \"lon\"])\n",
    "    if lat is None or lon is None:\n",
    "        raise KeyError(f\"Could not find lat/lon coords in dataset. coords={list(ds.coords)} vars={list(ds.variables)[:30]}\")\n",
    "    return lat, lon\n",
    "\n",
    "def resolve_data_var(ds, canonical_key: str) -> str:\n",
    "    wants = ALIASES.get(canonical_key, [canonical_key])\n",
    "    present = set(ds.data_vars.keys())\n",
    "    for w in wants:\n",
    "        if w in present:\n",
    "            return w\n",
    "    present2 = set(ds.variables.keys())\n",
    "    for w in wants:\n",
    "        if w in present2:\n",
    "            return w\n",
    "    raise KeyError(\n",
    "        f\"Could not resolve variable '{canonical_key}'. Wants={wants} \"\n",
    "        f\"Present(data_vars)={sorted(list(ds.data_vars.keys()))[:50]}\"\n",
    "    )\n",
    "\n",
    "# ----------------------------\n",
    "# File discovery\n",
    "# ----------------------------\n",
    "def daily_months_from_window(d1, d2):\n",
    "    out = []\n",
    "    y, m = d1.year, d1.month\n",
    "    while (y, m) <= (d2.year, d2.month):\n",
    "        out.append((y, m))\n",
    "        if m == 12:\n",
    "            y, m = y + 1, 1\n",
    "        else:\n",
    "            m += 1\n",
    "    return out\n",
    "\n",
    "def open_nc(path: Path) -> xr.Dataset:\n",
    "    # netcdf4 is usually installed in your env; keep it explicit\n",
    "    return xr.open_dataset(path, engine=\"netcdf4\")\n",
    "\n",
    "def infer_daily_extracted_dir(year: int, month: int) -> Path:\n",
    "    return RAW_DIR / f\"_era5l_daily_CA_{year}{month:02d}_mean_extracted\"\n",
    "\n",
    "def infer_monthly_extracted_dir(year: int) -> Path:\n",
    "    return RAW_DIR / f\"_era5l_monthly_CA_{year}_extracted\"\n",
    "\n",
    "def find_daily_primary_nc(year: int, month: int) -> Path | None:\n",
    "    fp = RAW_DIR / f\"era5l_daily_CA_{year}{month:02d}_mean.nc\"\n",
    "    return fp if fp.exists() else None\n",
    "\n",
    "def find_monthly_primary_nc(year: int) -> Path | None:\n",
    "    fp = RAW_DIR / f\"era5l_monthly_CA_{year}.nc\"\n",
    "    return fp if fp.exists() else None\n",
    "\n",
    "def list_extracted_ncs(extracted_dir: Path) -> list[Path]:\n",
    "    if not extracted_dir.exists():\n",
    "        return []\n",
    "    return sorted([p for p in extracted_dir.glob(\"*.nc\") if p.is_file() and p.stat().st_size > 0])\n",
    "\n",
    "# ----------------------------\n",
    "# Build a merged dataset for a period\n",
    "# ----------------------------\n",
    "def merge_sources_for_period(primary_nc: Path | None, extracted_dir: Path) -> tuple[xr.Dataset | None, dict]:\n",
    "    \"\"\"\n",
    "    Returns (merged_ds, diag).\n",
    "    Merges primary_nc + all extracted_dir/*.nc into one Dataset, preferring to include all required vars.\n",
    "    \"\"\"\n",
    "    sources = []\n",
    "    diag = {\"primary\": str(primary_nc) if primary_nc else None, \"extracted_dir\": str(extracted_dir), \"extracted_n\": 0, \"vars_present\": []}\n",
    "\n",
    "    if primary_nc and primary_nc.exists():\n",
    "        sources.append(primary_nc)\n",
    "\n",
    "    extracted_ncs = list_extracted_ncs(extracted_dir)\n",
    "    diag[\"extracted_n\"] = len(extracted_ncs)\n",
    "    sources.extend(extracted_ncs)\n",
    "\n",
    "    if not sources:\n",
    "        return None, diag\n",
    "\n",
    "    # Open all sources, keep only data vars, then merge\n",
    "    dsets = []\n",
    "    for fp in sources:\n",
    "        try:\n",
    "            ds = open_nc(fp)\n",
    "            # Some stream files contain extra meta vars; keep data_vars only\n",
    "            keep = list(ds.data_vars.keys())\n",
    "            if keep:\n",
    "                ds = ds[keep]\n",
    "            dsets.append(ds)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] failed to open {fp.name}: {e}\")\n",
    "\n",
    "    if not dsets:\n",
    "        return None, diag\n",
    "\n",
    "    try:\n",
    "        merged = xr.merge(dsets, compat=\"override\", join=\"outer\")\n",
    "    except Exception:\n",
    "        # fallback: merging can fail on attrs conflicts; strip attrs\n",
    "        dsets2 = []\n",
    "        for ds in dsets:\n",
    "            ds2 = ds.copy()\n",
    "            ds2.attrs = {}\n",
    "            for v in ds2.data_vars:\n",
    "                ds2[v].attrs = {}\n",
    "            dsets2.append(ds2)\n",
    "        merged = xr.merge(dsets2, compat=\"override\", join=\"outer\")\n",
    "\n",
    "    diag[\"vars_present\"] = sorted(list(merged.data_vars.keys()))\n",
    "    return merged, diag\n",
    "\n",
    "# ----------------------------\n",
    "# Shard writer\n",
    "# ----------------------------\n",
    "def write_time_slice_shard(ds, tname, latname, lonname, tval, out_path: Path, src_label: str):\n",
    "    if out_path.exists() and out_path.stat().st_size > 1_000_000:\n",
    "        print(f\"[SKIP] shard exists: {out_path.name} ({out_path.stat().st_size/1e6:.1f} MB)\")\n",
    "        return \"skipped\"\n",
    "\n",
    "    dss = ds.sel({tname: tval})\n",
    "\n",
    "    # Resolve each required variable (fail only if truly absent from merged dataset)\n",
    "    v_t2m  = resolve_data_var(dss, \"2m_temperature\")\n",
    "    v_sdwe = resolve_data_var(dss, \"snow_depth_water_equivalent\")\n",
    "    v_swvl1 = resolve_data_var(dss, \"volumetric_soil_water_layer_1\")\n",
    "\n",
    "    t2m = dss[v_t2m].values\n",
    "    sdwe = dss[v_sdwe].values\n",
    "    swvl1 = dss[v_swvl1].values\n",
    "\n",
    "    lat = dss[latname].values\n",
    "    lon = dss[lonname].values\n",
    "\n",
    "    if lat.ndim == 1 and lon.ndim == 1 and t2m.ndim == 2:\n",
    "        lon2, lat2 = np.meshgrid(lon, lat)\n",
    "    else:\n",
    "        lat2, lon2 = lat, lon\n",
    "\n",
    "    latf = lat2.reshape(-1).astype(np.float32)\n",
    "    lonf = lon2.reshape(-1).astype(np.float32)\n",
    "\n",
    "    t2m_c = (t2m.reshape(-1).astype(np.float32) - 273.15)\n",
    "    sdwe_f = sdwe.reshape(-1).astype(np.float32)\n",
    "    swvl1_f = swvl1.reshape(-1).astype(np.float32)\n",
    "\n",
    "    valid = np.isfinite(latf) & np.isfinite(lonf) & np.isfinite(t2m_c) & np.isfinite(sdwe_f) & np.isfinite(swvl1_f)\n",
    "    if valid.sum() == 0:\n",
    "        print(f\"[WARN] no valid points for shard {out_path.name}; skipping write.\")\n",
    "        return \"empty\"\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"date\": pd.to_datetime([pd.to_datetime(tval).date()] * int(valid.sum())),\n",
    "        \"lat\": latf[valid],\n",
    "        \"lon\": lonf[valid],\n",
    "        \"t2m_c\": t2m_c[valid],\n",
    "        \"sdwe\": sdwe_f[valid],\n",
    "        \"swvl1\": swvl1_f[valid],\n",
    "        \"src_file\": src_label,\n",
    "    })\n",
    "    df.to_parquet(out_path, index=False)\n",
    "    print(f\"[OK] wrote shard: {out_path.name} rows={len(df):,}\")\n",
    "    return \"wrote\"\n",
    "\n",
    "# ----------------------------\n",
    "# Daily analysis: merge monthly sources, then shard days in window\n",
    "# ----------------------------\n",
    "def process_daily_analysis():\n",
    "    months = daily_months_from_window(analysis_start, analysis_end)\n",
    "    wrote = skipped = empty = failed_months = 0\n",
    "\n",
    "    for (y, m) in months:\n",
    "        primary = find_daily_primary_nc(y, m)\n",
    "        extracted_dir = infer_daily_extracted_dir(y, m)\n",
    "\n",
    "        ds, diag = merge_sources_for_period(primary, extracted_dir)\n",
    "        if ds is None:\n",
    "            print(f\"[WARN] No daily sources found for {y}{m:02d} (primary={primary}, extracted={extracted_dir})\")\n",
    "            failed_months += 1\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n[DAILY] {y}{m:02d} sources:\")\n",
    "        print(\"  primary :\", diag['primary'])\n",
    "        print(\"  extracted_dir :\", diag['extracted_dir'], f\"(n_nc={diag['extracted_n']})\")\n",
    "        print(\"  merged vars   :\", diag[\"vars_present\"][:20], (\"...\" if len(diag[\"vars_present\"]) > 20 else \"\"))\n",
    "\n",
    "        try:\n",
    "            tname = get_time_name(ds)\n",
    "            latname, lonname = get_latlon_names(ds)\n",
    "            times = pd.to_datetime(ds[tname].values)\n",
    "\n",
    "            # Only the dates in analysis window\n",
    "            for tval in times:\n",
    "                d = pd.to_datetime(tval).date()\n",
    "                if d < analysis_start or d > analysis_end:\n",
    "                    continue\n",
    "                out_path = DAILY_SHARDS_DIR / f\"era5_daily_long_{d.isoformat()}.parquet\"\n",
    "                res = write_time_slice_shard(ds, tname, latname, lonname, tval, out_path, src_label=f\"daily_{y}{m:02d}\")\n",
    "                if res == \"wrote\":\n",
    "                    wrote += 1\n",
    "                elif res == \"skipped\":\n",
    "                    skipped += 1\n",
    "                else:\n",
    "                    empty += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] daily month failed: {y}{m:02d} | {e}\")\n",
    "            failed_months += 1\n",
    "\n",
    "    print(f\"\\n[DONE] ERA5 daily shards summary: wrote={wrote} skipped={skipped} empty={empty} failed_months={failed_months}\")\n",
    "\n",
    "    if ASSEMBLE_DAILY_ANALYSIS:\n",
    "        if DAILY_LONG_OUT.exists() and DAILY_LONG_OUT.stat().st_size > 5_000_000:\n",
    "            print(f\"[SKIP] daily long parquet exists: {DAILY_LONG_OUT} ({DAILY_LONG_OUT.stat().st_size/1e6:.1f} MB)\")\n",
    "        else:\n",
    "            shard_files = sorted(DAILY_SHARDS_DIR.glob(\"era5_daily_long_*.parquet\"))\n",
    "            if not shard_files:\n",
    "                print(f\"[WARN] No daily shards found in {DAILY_SHARDS_DIR}; skipping assembly.\")\n",
    "                return\n",
    "            df = pd.concat([pd.read_parquet(p) for p in shard_files], ignore_index=True)\n",
    "            df[\"date\"] = pd.to_datetime(df[\"date\"]).dt.normalize()\n",
    "            df.sort_values([\"date\", \"lat\", \"lon\"], inplace=True)\n",
    "            df.to_parquet(DAILY_LONG_OUT, index=False)\n",
    "            print(f\"[OK] saved daily long parquet: {DAILY_LONG_OUT} rows={len(df):,}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Monthly baseline: merge per-year sources, then shard months in baseline window\n",
    "# ----------------------------\n",
    "def process_monthly_baseline():\n",
    "    wrote = skipped = empty = failed_years = 0\n",
    "    for year in range(baseline_start.year, baseline_end.year + 1):\n",
    "        primary = find_monthly_primary_nc(year)\n",
    "        extracted_dir = infer_monthly_extracted_dir(year)\n",
    "\n",
    "        ds, diag = merge_sources_for_period(primary, extracted_dir)\n",
    "        if ds is None:\n",
    "            print(f\"[WARN] No monthly sources found for {year} (primary={primary}, extracted={extracted_dir})\")\n",
    "            failed_years += 1\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n[MONTHLY] {year} sources:\")\n",
    "        print(\"  primary :\", diag['primary'])\n",
    "        print(\"  extracted_dir :\", diag['extracted_dir'], f\"(n_nc={diag['extracted_n']})\")\n",
    "        print(\"  merged vars   :\", diag[\"vars_present\"][:20], (\"...\" if len(diag[\"vars_present\"]) > 20 else \"\"))\n",
    "\n",
    "        try:\n",
    "            tname = get_time_name(ds)\n",
    "            latname, lonname = get_latlon_names(ds)\n",
    "            times = pd.to_datetime(ds[tname].values)\n",
    "\n",
    "            for tval in times:\n",
    "                d = pd.to_datetime(tval).date()\n",
    "                if d < baseline_start or d > baseline_end:\n",
    "                    continue\n",
    "                ym = f\"{d.year:04d}-{d.month:02d}\"\n",
    "                out_path = MONTHLY_SHARDS_DIR / f\"era5_monthly_long_{ym}.parquet\"\n",
    "                res = write_time_slice_shard(ds, tname, latname, lonname, tval, out_path, src_label=f\"monthly_{year}\")\n",
    "                if res == \"wrote\":\n",
    "                    wrote += 1\n",
    "                elif res == \"skipped\":\n",
    "                    skipped += 1\n",
    "                else:\n",
    "                    empty += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] monthly year failed: {year} | {e}\")\n",
    "            failed_years += 1\n",
    "\n",
    "    print(f\"\\n[DONE] ERA5 monthly shards summary: wrote={wrote} skipped={skipped} empty={empty} failed_years={failed_years}\")\n",
    "\n",
    "    if ASSEMBLE_MONTHLY_BASELINE:\n",
    "        if MONTHLY_LONG_OUT.exists() and MONTHLY_LONG_OUT.stat().st_size > 5_000_000:\n",
    "            print(f\"[SKIP] monthly long parquet exists: {MONTHLY_LONG_OUT} ({MONTHLY_LONG_OUT.stat().st_size/1e6:.1f} MB)\")\n",
    "        else:\n",
    "            shard_files = sorted(MONTHLY_SHARDS_DIR.glob(\"era5_monthly_long_*.parquet\"))\n",
    "            if not shard_files:\n",
    "                print(f\"[WARN] No monthly shards found in {MONTHLY_SHARDS_DIR}; skipping assembly.\")\n",
    "                return\n",
    "            df = pd.concat([pd.read_parquet(p) for p in shard_files], ignore_index=True)\n",
    "            df[\"date\"] = pd.to_datetime(df[\"date\"]).dt.normalize()\n",
    "            df.sort_values([\"date\", \"lat\", \"lon\"], inplace=True)\n",
    "            df.to_parquet(MONTHLY_LONG_OUT, index=False)\n",
    "            print(f\"[OK] saved monthly long parquet: {MONTHLY_LONG_OUT} rows={len(df):,}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Run\n",
    "# ----------------------------\n",
    "process_daily_analysis()\n",
    "process_monthly_baseline()\n",
    "\n",
    "print(\"\\nERA5 Cell A complete.\")\n",
    "print(\"  Daily shards dir  :\", DAILY_SHARDS_DIR)\n",
    "print(\"  Monthly shards dir:\", MONTHLY_SHARDS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b370a1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# ERA5-Land | Cell B: lat/lon points -> HydroPulse grid mapping + daily/monthly gridding shards\n",
    "#\n",
    "# Inputs (from Cell A):\n",
    "#   results/derived/era5_cellA_shards_daily/era5_daily_long_YYYY-MM-DD.parquet\n",
    "#   results/derived/era5_cellA_shards_monthly/era5_monthly_long_YYYY-MM.parquet\n",
    "#   results/<GRID_FILENAME>  (GeoParquet w/ grid_id + geometry in EPSG:OPS_EPSG)\n",
    "#\n",
    "# Outputs:\n",
    "#   results/derived/era5_grid_daily_shards/era5_daily_grid_YYYY-MM-DD.parquet\n",
    "#   results/derived/era5_grid_monthly_shards/era5_monthly_grid_YYYY-MM.parquet\n",
    "#   results/<ERA5_DAILY_GRID_PARQUET_NAME>      (assembled, analysis window)\n",
    "#   results/<ERA5_MONTHLY_GRID_PARQUET_NAME>    (assembled, baseline window; optional)\n",
    "#\n",
    "# Design:\n",
    "# - Precompute a stable mapping: (lat, lon) -> grid_id once, cache it to results/\n",
    "# - Then each shard becomes: read shard -> merge mapping -> groupby grid_id/date -> aggregate\n",
    "#\n",
    "# Resume-safe:\n",
    "# - Mapping parquet reused if present\n",
    "# - Per-day/per-month grid shards skipped if present\n",
    "# - Final assembled parquet skipped if present\n",
    "# - Reprocessing is manual: delete outputs to rebuild\n",
    "# ============================\n",
    "\n",
    "from pathlib import Path\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from shapely import wkb\n",
    "\n",
    "# ----------------------------\n",
    "# Config + paths\n",
    "# ----------------------------\n",
    "OUT_DIR = Path(CONFIG[\"out_dir\"])\n",
    "DERIVED_DIR = OUT_DIR / \"derived\"\n",
    "DERIVED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OPS_EPSG = int(CONFIG.get(\"OPS_EPSG\", 3310))\n",
    "\n",
    "GRID_PATH = Path(resolve_out_path(CONFIG.get(\"GRID_FILENAME\", \"grid_3000m_CA_epsg3310.parquet\")))\n",
    "\n",
    "ERA5_DAILY_CELL_A_DIRNAME = CONFIG.get(\"ERA5_CELL_A_DAILY_SHARDS_DIRNAME\", \"era5_cellA_shards_daily\")\n",
    "ERA5_MONTHLY_CELL_A_DIRNAME = CONFIG.get(\"ERA5_CELL_A_MONTHLY_SHARDS_DIRNAME\", \"era5_cellA_shards_monthly\")\n",
    "\n",
    "CELL_A_DAILY_DIR = DERIVED_DIR / ERA5_DAILY_CELL_A_DIRNAME\n",
    "CELL_A_MONTHLY_DIR = DERIVED_DIR / ERA5_MONTHLY_CELL_A_DIRNAME\n",
    "\n",
    "DAILY_GRID_SHARDS_DIR = DERIVED_DIR / CONFIG.get(\"ERA5_GRID_DAILY_SHARDS_DIRNAME\", \"era5_grid_daily_shards\")\n",
    "MONTHLY_GRID_SHARDS_DIR = DERIVED_DIR / CONFIG.get(\"ERA5_GRID_MONTHLY_SHARDS_DIRNAME\", \"era5_grid_monthly_shards\")\n",
    "DAILY_GRID_SHARDS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MONTHLY_GRID_SHARDS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ERA5_POINT_GRIDMAP_NAME = CONFIG.get(\"ERA5_POINT_TO_GRIDMAP_PARQUET_NAME\", \"era5_point_to_grid_3310.parquet\")\n",
    "GRIDMAP_PATH = OUT_DIR / ERA5_POINT_GRIDMAP_NAME\n",
    "\n",
    "analysis_start = pd.to_datetime(CONFIG[\"start_date\"]).date()\n",
    "analysis_end   = pd.to_datetime(CONFIG[\"end_date\"]).date()\n",
    "\n",
    "baseline_start = pd.to_datetime(CONFIG.get(\"BASELINE_START_DATE\", \"1991-01-01\")).date()\n",
    "baseline_end   = pd.to_datetime(CONFIG.get(\"BASELINE_END_DATE\", \"2020-12-31\")).date()\n",
    "\n",
    "ERA5_DAILY_GRID_NAME = CONFIG.get(\n",
    "    \"ERA5_DAILY_GRID_PARQUET_NAME\",\n",
    "    f\"era5_daily_grid_CA_{CONFIG.get('grid_resolution_m',3000)}m_epsg{OPS_EPSG}_{str(analysis_start).replace('-','')}_{str(analysis_end).replace('-','')}.parquet\"\n",
    ")\n",
    "ERA5_MONTHLY_GRID_NAME = CONFIG.get(\n",
    "    \"ERA5_MONTHLY_GRID_PARQUET_NAME\",\n",
    "    f\"era5_monthly_grid_CA_{CONFIG.get('grid_resolution_m',3000)}m_epsg{OPS_EPSG}_{str(baseline_start).replace('-','')[:4]}01_{str(baseline_end).replace('-','')[:4]}12.parquet\"\n",
    ")\n",
    "\n",
    "FINAL_DAILY_PATH = OUT_DIR / ERA5_DAILY_GRID_NAME\n",
    "FINAL_MONTHLY_PATH = OUT_DIR / ERA5_MONTHLY_GRID_NAME\n",
    "\n",
    "ASSEMBLE_DAILY = bool(CONFIG.get(\"ERA5_ASSEMBLE_DAILY_GRID\", True))\n",
    "ASSEMBLE_MONTHLY = bool(CONFIG.get(\"ERA5_ASSEMBLE_MONTHLY_GRID\", False))\n",
    "\n",
    "ERA5_LATLON_ROUND_DECIMALS = int(CONFIG.get(\"ERA5_LATLON_ROUND_DECIMALS\", 6))\n",
    "\n",
    "print(\"ERA5 Cell B starting\")\n",
    "print(\"  GRID_PATH   :\", GRID_PATH)\n",
    "print(\"  OPS_EPSG    :\", OPS_EPSG)\n",
    "print(\"  CellA daily :\", CELL_A_DAILY_DIR)\n",
    "print(\"  CellA monthly:\", CELL_A_MONTHLY_DIR)\n",
    "print(\"  Gridmap cache:\", GRIDMAP_PATH)\n",
    "print(\"  Daily grid shards :\", DAILY_GRID_SHARDS_DIR)\n",
    "print(\"  Monthly grid shards:\", MONTHLY_GRID_SHARDS_DIR)\n",
    "print(\"  Final daily grid :\", FINAL_DAILY_PATH, \"| assemble:\", ASSEMBLE_DAILY)\n",
    "print(\"  Final monthly grid:\", FINAL_MONTHLY_PATH, \"| assemble:\", ASSEMBLE_MONTHLY)\n",
    "print(\"  lat/lon rounding decimals:\", ERA5_LATLON_ROUND_DECIMALS)\n",
    "\n",
    "if not GRID_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Missing grid parquet: {GRID_PATH}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Robust grid loader (handles WKB bytes geometry)\n",
    "# ----------------------------\n",
    "def load_grid_geodataframe(grid_path: Path, ops_epsg: int) -> gpd.GeoDataFrame:\n",
    "    grid = pd.read_parquet(grid_path)\n",
    "\n",
    "    if \"grid_id\" not in grid.columns:\n",
    "        raise KeyError(\"Grid parquet must include grid_id\")\n",
    "    if \"geometry\" not in grid.columns:\n",
    "        raise KeyError(\"Grid parquet must include geometry column\")\n",
    "\n",
    "    # ensure grid_id is string (your schema is string IDs like CA3310_...)\n",
    "    grid = grid.copy()\n",
    "    grid[\"grid_id\"] = grid[\"grid_id\"].astype(str)\n",
    "\n",
    "    geom = grid[\"geometry\"]\n",
    "    if len(geom) > 0 and isinstance(geom.iloc[0], (bytes, bytearray, memoryview)):\n",
    "        grid[\"geometry\"] = grid[\"geometry\"].apply(lambda b: wkb.loads(bytes(b)) if b is not None else None)\n",
    "\n",
    "    ggrid = gpd.GeoDataFrame(grid, geometry=\"geometry\", crs=f\"EPSG:{ops_epsg}\").copy()\n",
    "    ggrid = ggrid[[\"grid_id\", \"geometry\"]].dropna(subset=[\"geometry\"]).copy()\n",
    "    return ggrid\n",
    "\n",
    "ggrid = load_grid_geodataframe(GRID_PATH, OPS_EPSG)\n",
    "print(f\"[OK] Loaded grid: cells={len(ggrid):,}\")\n",
    "print(\"[DEBUG] grid_id example:\", ggrid[\"grid_id\"].iloc[0])\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers\n",
    "# ----------------------------\n",
    "def list_cellA_shards_daily():\n",
    "    if not CELL_A_DAILY_DIR.exists():\n",
    "        return []\n",
    "    return sorted(CELL_A_DAILY_DIR.glob(\"era5_daily_long_*.parquet\"))\n",
    "\n",
    "def list_cellA_shards_monthly():\n",
    "    if not CELL_A_MONTHLY_DIR.exists():\n",
    "        return []\n",
    "    return sorted(CELL_A_MONTHLY_DIR.glob(\"era5_monthly_long_*.parquet\"))\n",
    "\n",
    "def normalize_latlon(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    out[\"lat\"] = pd.to_numeric(out[\"lat\"], errors=\"coerce\").astype(np.float32)\n",
    "    out[\"lon\"] = pd.to_numeric(out[\"lon\"], errors=\"coerce\").astype(np.float32)\n",
    "    out[\"lat_k\"] = out[\"lat\"].round(ERA5_LATLON_ROUND_DECIMALS)\n",
    "    out[\"lon_k\"] = out[\"lon\"].round(ERA5_LATLON_ROUND_DECIMALS)\n",
    "    return out\n",
    "\n",
    "def build_or_load_point_gridmap() -> pd.DataFrame:\n",
    "    if GRIDMAP_PATH.exists() and GRIDMAP_PATH.stat().st_size > 50_000:\n",
    "        gm = pd.read_parquet(GRIDMAP_PATH)\n",
    "        # enforce expected types\n",
    "        gm[\"grid_id\"] = gm[\"grid_id\"].astype(str)\n",
    "        print(f\"[SKIP] Using existing gridmap: {GRIDMAP_PATH} rows={len(gm):,}\")\n",
    "        return gm\n",
    "\n",
    "    daily_shards = list_cellA_shards_daily()\n",
    "    monthly_shards = list_cellA_shards_monthly()\n",
    "\n",
    "    if daily_shards:\n",
    "        sample_path = daily_shards[0]\n",
    "    elif monthly_shards:\n",
    "        sample_path = monthly_shards[0]\n",
    "    else:\n",
    "        raise FileNotFoundError(\"No ERA5 Cell A shards found (daily or monthly). Run ERA5 Cell A first.\")\n",
    "\n",
    "    print(f\"[MAP] Building ERA5 point->grid map from sample shard: {sample_path.name}\")\n",
    "    samp = pd.read_parquet(sample_path, columns=[\"lat\", \"lon\"])\n",
    "    samp = normalize_latlon(samp)\n",
    "\n",
    "    pts = samp[[\"lat_k\", \"lon_k\"]].drop_duplicates().dropna().copy()\n",
    "    print(f\"[MAP] Unique points: {len(pts):,}\")\n",
    "\n",
    "    gpts = gpd.GeoDataFrame(\n",
    "        pts,\n",
    "        geometry=[Point(xy) for xy in zip(pts[\"lon_k\"].astype(float), pts[\"lat_k\"].astype(float))],\n",
    "        crs=\"EPSG:4326\",\n",
    "    ).to_crs(epsg=OPS_EPSG)\n",
    "\n",
    "    joined = gpd.sjoin(gpts, ggrid, how=\"left\", predicate=\"within\")\n",
    "\n",
    "    missing = joined[\"grid_id\"].isna().sum()\n",
    "    if missing > 0:\n",
    "        print(f\"[WARN] {missing:,} points not within any grid cell; using nearest join for those.\")\n",
    "        miss = joined[joined[\"grid_id\"].isna()].drop(columns=[\"grid_id\", \"index_right\"], errors=\"ignore\")\n",
    "        near = gpd.sjoin_nearest(miss, ggrid, how=\"left\", distance_col=\"dist_m\")\n",
    "        joined.loc[joined[\"grid_id\"].isna(), \"grid_id\"] = near[\"grid_id\"].values\n",
    "\n",
    "    joined = joined.drop(columns=[\"geometry\", \"index_right\"], errors=\"ignore\")\n",
    "    gm = joined[[\"lat_k\", \"lon_k\", \"grid_id\"]].dropna().copy()\n",
    "\n",
    "    # Critical fix: grid_id is a STRING in your grid parquet; do not cast to int.\n",
    "    gm[\"grid_id\"] = gm[\"grid_id\"].astype(str)\n",
    "\n",
    "    # Atomic-ish write to avoid corrupt cache on failure\n",
    "    tmp = GRIDMAP_PATH.with_suffix(\".tmp.parquet\")\n",
    "    if tmp.exists():\n",
    "        tmp.unlink()\n",
    "    gm.to_parquet(tmp, index=False)\n",
    "    if GRIDMAP_PATH.exists():\n",
    "        GRIDMAP_PATH.unlink()\n",
    "    tmp.rename(GRIDMAP_PATH)\n",
    "\n",
    "    print(f\"[OK] Saved gridmap: {GRIDMAP_PATH} rows={len(gm):,}\")\n",
    "    return gm\n",
    "\n",
    "gridmap = build_or_load_point_gridmap()\n",
    "\n",
    "# ----------------------------\n",
    "# Gridding\n",
    "# ----------------------------\n",
    "def grid_one_shard(shard_path: Path, out_path: Path, kind: str):\n",
    "    if out_path.exists() and out_path.stat().st_size > 50_000:\n",
    "        print(f\"[SKIP] Grid shard exists: {out_path.name} ({out_path.stat().st_size/1e6:.1f} MB)\")\n",
    "        return \"skipped\"\n",
    "\n",
    "    df = pd.read_parquet(shard_path)\n",
    "    req = {\"date\", \"lat\", \"lon\", \"t2m_c\", \"sdwe\", \"swvl1\"}\n",
    "    miss = req - set(df.columns)\n",
    "    if miss:\n",
    "        raise KeyError(f\"{shard_path.name} missing required columns: {sorted(miss)}\")\n",
    "\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"]).dt.normalize()\n",
    "    df = normalize_latlon(df)\n",
    "\n",
    "    tmp = df.merge(gridmap, on=[\"lat_k\", \"lon_k\"], how=\"left\")\n",
    "    tmp = tmp.dropna(subset=[\"grid_id\"]).copy()\n",
    "    if tmp.empty:\n",
    "        print(f\"[WARN] After merge, no rows remain for {shard_path.name}; skipping.\")\n",
    "        return \"empty\"\n",
    "\n",
    "    tmp[\"grid_id\"] = tmp[\"grid_id\"].astype(str)\n",
    "\n",
    "    out = (\n",
    "        tmp.groupby([\"grid_id\", \"date\"], as_index=False)\n",
    "        .agg(\n",
    "            t2m_c=(\"t2m_c\", \"mean\"),\n",
    "            sdwe=(\"sdwe\", \"mean\"),\n",
    "            swvl1=(\"swvl1\", \"mean\"),\n",
    "            n_points=(\"t2m_c\", \"count\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    out.to_parquet(out_path, index=False)\n",
    "    print(f\"[OK] Wrote {kind} grid shard: {out_path.name} rows={len(out):,} cells={out['grid_id'].nunique():,}\")\n",
    "    return \"wrote\"\n",
    "\n",
    "def process_daily_grid():\n",
    "    shards = list_cellA_shards_daily()\n",
    "    if not shards:\n",
    "        print(\"[WARN] No ERA5 daily Cell A shards found; skipping daily Cell B.\")\n",
    "        return\n",
    "\n",
    "    wrote = skipped = empty = 0\n",
    "    for sp in shards:\n",
    "        m = re.search(r\"era5_daily_long_(\\d{4}-\\d{2}-\\d{2})\\.parquet$\", sp.name)\n",
    "        if not m:\n",
    "            continue\n",
    "        d = pd.to_datetime(m.group(1)).date()\n",
    "        if d < analysis_start or d > analysis_end:\n",
    "            continue\n",
    "\n",
    "        outp = DAILY_GRID_SHARDS_DIR / f\"era5_daily_grid_{d.isoformat()}.parquet\"\n",
    "        res = grid_one_shard(sp, outp, kind=\"daily\")\n",
    "        wrote += (res == \"wrote\")\n",
    "        skipped += (res == \"skipped\")\n",
    "        empty += (res == \"empty\")\n",
    "\n",
    "    print(f\"[DONE] ERA5 daily Cell B: wrote={wrote} skipped={skipped} empty={empty}\")\n",
    "\n",
    "    if ASSEMBLE_DAILY:\n",
    "        if FINAL_DAILY_PATH.exists() and FINAL_DAILY_PATH.stat().st_size > 5_000_000:\n",
    "            print(f\"[SKIP] Final daily grid exists: {FINAL_DAILY_PATH} ({FINAL_DAILY_PATH.stat().st_size/1e6:.1f} MB)\")\n",
    "            return\n",
    "        files = sorted(DAILY_GRID_SHARDS_DIR.glob(\"era5_daily_grid_*.parquet\"))\n",
    "        if not files:\n",
    "            print(f\"[WARN] No daily grid shards found in {DAILY_GRID_SHARDS_DIR}; skipping assembly.\")\n",
    "            return\n",
    "        final = pd.concat([pd.read_parquet(p) for p in files], ignore_index=True)\n",
    "        final[\"date\"] = pd.to_datetime(final[\"date\"]).dt.normalize()\n",
    "        final[\"grid_id\"] = final[\"grid_id\"].astype(str)\n",
    "        final.sort_values([\"grid_id\", \"date\"], inplace=True)\n",
    "        final.to_parquet(FINAL_DAILY_PATH, index=False)\n",
    "        print(f\"[OK] Saved FINAL daily grid: {FINAL_DAILY_PATH} rows={len(final):,} cells={final['grid_id'].nunique():,} dates={final['date'].nunique():,}\")\n",
    "\n",
    "def process_monthly_grid():\n",
    "    shards = list_cellA_shards_monthly()\n",
    "    if not shards:\n",
    "        print(\"[WARN] No ERA5 monthly Cell A shards found; skipping monthly Cell B.\")\n",
    "        return\n",
    "\n",
    "    wrote = skipped = empty = 0\n",
    "    for sp in shards:\n",
    "        m = re.search(r\"era5_monthly_long_(\\d{4}-\\d{2})\\.parquet$\", sp.name)\n",
    "        if not m:\n",
    "            continue\n",
    "        ym = m.group(1)\n",
    "        d = pd.to_datetime(f\"{ym}-01\").date()\n",
    "        if d < baseline_start or d > baseline_end:\n",
    "            continue\n",
    "\n",
    "        outp = MONTHLY_GRID_SHARDS_DIR / f\"era5_monthly_grid_{ym}.parquet\"\n",
    "        res = grid_one_shard(sp, outp, kind=\"monthly\")\n",
    "        wrote += (res == \"wrote\")\n",
    "        skipped += (res == \"skipped\")\n",
    "        empty += (res == \"empty\")\n",
    "\n",
    "    print(f\"[DONE] ERA5 monthly Cell B: wrote={wrote} skipped={skipped} empty={empty}\")\n",
    "\n",
    "    if ASSEMBLE_MONTHLY:\n",
    "        if FINAL_MONTHLY_PATH.exists() and FINAL_MONTHLY_PATH.stat().st_size > 5_000_000:\n",
    "            print(f\"[SKIP] Final monthly grid exists: {FINAL_MONTHLY_PATH} ({FINAL_MONTHLY_PATH.stat().st_size/1e6:.1f} MB)\")\n",
    "            return\n",
    "        files = sorted(MONTHLY_GRID_SHARDS_DIR.glob(\"era5_monthly_grid_*.parquet\"))\n",
    "        if not files:\n",
    "            print(f\"[WARN] No monthly grid shards found in {MONTHLY_GRID_SHARDS_DIR}; skipping assembly.\")\n",
    "            return\n",
    "        final = pd.concat([pd.read_parquet(p) for p in files], ignore_index=True)\n",
    "        final[\"date\"] = pd.to_datetime(final[\"date\"]).dt.normalize()\n",
    "        final[\"grid_id\"] = final[\"grid_id\"].astype(str)\n",
    "        final.sort_values([\"grid_id\", \"date\"], inplace=True)\n",
    "        final.to_parquet(FINAL_MONTHLY_PATH, index=False)\n",
    "        print(f\"[OK] Saved FINAL monthly grid: {FINAL_MONTHLY_PATH} rows={len(final):,} cells={final['grid_id'].nunique():,} dates={final['date'].nunique():,}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Run\n",
    "# ----------------------------\n",
    "process_daily_grid()\n",
    "process_monthly_grid()\n",
    "print(\"ERA5 Cell B complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43209c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next dataset is the USGS-StreamFlow dataset\n",
    "# ============================\n",
    "# Base endpoint (Daily Values / DV):\n",
    "# \t•\thttps://waterservices.usgs.gov/nwis/dv/\n",
    "\n",
    "# Key parameters you’ll use (these are the ones that matter):\n",
    "# \t•\tformat=rdb (tab-delimited, easy to parse)\n",
    "# \t•\tstateCd=ca (California-only site filter)\n",
    "# \t•\tparameterCd=00060 (discharge, cubic feet per second)\n",
    "# \t•\tsiteType=ST (streams)\n",
    "# \t•\tstartDT=1991-01-01 (baseline start)\n",
    "# \t•\tendDT=2020-12-31 (baseline end) or endDT=2024-10-31 (analysis end)\n",
    "# \t•\tsiteStatus=active (optional)\n",
    "# \t•\tstatCd=00003 (daily mean) (supported by DV service outputs)  ￼\n",
    "# ============================\n",
    "\n",
    "# URLs look like this: https://waterservices.usgs.gov/nwis/dv/?format=rdb&stateCd=CA&siteType=ST&parameterCd=00060&startDT=1991-01-01&endDT=2025-12-31\n",
    "# This is a large download! If this fails, then try manually\n",
    "# downloading from the above URL and saving as \"manual/usgs-streamflow/streamflow-19910101-20251231.txt\"\n",
    "# and then run the parsing/normalization cell below.\n",
    "# ============================\n",
    "\n",
    "\n",
    "# ============================\n",
    "# USGS NWIS Daily Values downloader (chunked, resumable)\n",
    "# Writes to:\n",
    "#   results/{USGS_RAW_DIRNAME}/  (yearly .rdb chunks + optional combined raw)\n",
    "#   results/{USGS_CLEAN_DIRNAME}/ (reserved for later)\n",
    "# ============================\n",
    "\n",
    "from pathlib import Path\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# ---- config ----\n",
    "OUT_DIR = Path(CONFIG[\"out_dir\"])\n",
    "\n",
    "RAW_DIR = OUT_DIR / CONFIG.get(\"USGS_RAW_DIRNAME\", \"usgs_raw\")\n",
    "CLEAN_DIR = OUT_DIR / CONFIG.get(\"USGS_CLEAN_DIRNAME\", \"usgs_clean\")\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CLEAN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "STATE_CD = CONFIG.get(\"USGS_STATE_CD\", \"CA\")\n",
    "SITE_TYPE = CONFIG.get(\"USGS_SITE_TYPE\", \"ST\")\n",
    "PARAM = CONFIG.get(\"USGS_PARAM_DISCHARGE\", \"00060\")\n",
    "\n",
    "START = pd.to_datetime(CONFIG.get(\"USGS_START_DATE\", \"1991-01-01\")).date()\n",
    "END   = pd.to_datetime(CONFIG.get(\"USGS_END_DATE\", \"2025-12-31\")).date()\n",
    "\n",
    "SLEEP_S = float(CONFIG.get(\"USGS_SLEEP_S\", 2.5))\n",
    "MAX_RETRIES = int(CONFIG.get(\"USGS_MAX_RETRIES\", 6))\n",
    "BACKOFF_BASE = float(CONFIG.get(\"USGS_BACKOFF_BASE_S\", 10))\n",
    "TIMEOUT_S = int(CONFIG.get(\"USGS_TIMEOUT_S\", 180))\n",
    "\n",
    "BASE_URL = \"https://waterservices.usgs.gov/nwis/dv/\"\n",
    "\n",
    "def ok_file(path: Path, min_bytes: int = 50_000) -> bool:\n",
    "    return path.exists() and path.is_file() and path.stat().st_size >= min_bytes\n",
    "\n",
    "def fetch_year(year: int) -> Path:\n",
    "    startDT = f\"{year:04d}-01-01\"\n",
    "    endDT   = f\"{year:04d}-12-31\"\n",
    "    out = RAW_DIR / f\"usgs_dv_{STATE_CD.lower()}_{SITE_TYPE.lower()}_{PARAM}_{year:04d}.rdb\"\n",
    "\n",
    "    if ok_file(out):\n",
    "        print(f\"[SKIP] {out.name} exists ({out.stat().st_size/1e6:.2f} MB)\")\n",
    "        return out\n",
    "\n",
    "    params = {\n",
    "        \"format\": \"rdb\",\n",
    "        \"stateCd\": STATE_CD,\n",
    "        \"siteType\": SITE_TYPE,\n",
    "        \"parameterCd\": PARAM,\n",
    "        \"startDT\": startDT,\n",
    "        \"endDT\": endDT,\n",
    "    }\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": CONFIG.get(\"USER_AGENT_HEADERS\", {}).get(\"User-Agent\", \"BlueLeafLabs/HydroPulse\"),\n",
    "        \"Accept-Encoding\": \"gzip, deflate\",\n",
    "    }\n",
    "\n",
    "    for attempt in range(1, MAX_RETRIES + 1):\n",
    "        try:\n",
    "            print(f\"[DL] {year} -> {out.name}\")\n",
    "            r = requests.get(BASE_URL, params=params, headers=headers, timeout=TIMEOUT_S)\n",
    "            r.raise_for_status()\n",
    "\n",
    "            tmp = out.with_suffix(\".tmp\")\n",
    "            tmp.write_bytes(r.content)\n",
    "\n",
    "            if not ok_file(tmp):\n",
    "                raise RuntimeError(f\"Downloaded file too small for {year}: {tmp.stat().st_size} bytes\")\n",
    "\n",
    "            tmp.replace(out)\n",
    "            print(f\"[OK] {out.name} ({out.stat().st_size/1e6:.2f} MB)\")\n",
    "            time.sleep(SLEEP_S)\n",
    "            return out\n",
    "\n",
    "        except Exception as e:\n",
    "            if attempt == MAX_RETRIES:\n",
    "                raise\n",
    "            sleep = BACKOFF_BASE * attempt\n",
    "            print(f\"[WARN] {year} attempt {attempt}/{MAX_RETRIES} failed: {e}\")\n",
    "            print(f\"       sleeping {sleep:.0f}s then retrying...\")\n",
    "            time.sleep(sleep)\n",
    "\n",
    "# ---- run yearly downloads ----\n",
    "years = list(range(START.year, END.year + 1))\n",
    "\n",
    "print(\"USGS download starting (yearly chunks)\")\n",
    "print(\"  RAW_DIR  :\", RAW_DIR)\n",
    "print(\"  CLEAN_DIR:\", CLEAN_DIR, \"(reserved)\")\n",
    "print(\"  state/site/param:\", STATE_CD, SITE_TYPE, PARAM)\n",
    "print(\"  date range:\", START, \"→\", END)\n",
    "print(\"  years:\", years[0], \"→\", years[-1])\n",
    "\n",
    "paths = []\n",
    "for y in years:\n",
    "    paths.append(fetch_year(y))\n",
    "\n",
    "print(\"\\nDownloaded/verified chunks:\", len(paths))\n",
    "\n",
    "# ---- optional: combined raw file under RAW_DIR ----\n",
    "combined = RAW_DIR / f\"usgs_dv_{STATE_CD.lower()}_{SITE_TYPE.lower()}_{PARAM}_{START.year:04d}-{END.year:04d}.rdb\"\n",
    "if not ok_file(combined, min_bytes=200_000):\n",
    "    print(\"[COMBINE] Writing combined file:\", combined.name)\n",
    "    with combined.open(\"wb\") as w:\n",
    "        first = True\n",
    "        for p in paths:\n",
    "            data = p.read_bytes().splitlines(keepends=True)\n",
    "            if first:\n",
    "                w.writelines(data)\n",
    "                first = False\n",
    "            else:\n",
    "                # drop leading comment lines from each subsequent yearly shard\n",
    "                i = 0\n",
    "                while i < len(data) and data[i].lstrip().startswith(b\"#\"):\n",
    "                    i += 1\n",
    "                w.writelines(data[i:])\n",
    "    print(\"[OK] Combined:\", combined.name, f\"({combined.stat().st_size/1e6:.2f} MB)\")\n",
    "else:\n",
    "    print(\"[SKIP] Combined file already exists:\", combined.name)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f30292b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# USGS Cell A (NWIS DV) -> cleaned long parquet (HARDENED + RESUME-SAFE)\n",
    "# - Robust line parsing, ignore comments, handle repeated headers/spec lines\n",
    "# - IMPORTANT FIX: parse per-header-block (do NOT build one wide DataFrame for the whole year)\n",
    "# - Select discharge column per header block using numeric-prefix scoring (handles '14n')\n",
    "# - Writes per-year shard parquet files so reruns do NOT redo hours of work\n",
    "# - Assembles final parquet ONLY if final output is missing (manual delete to rebuild)\n",
    "# ============================\n",
    "\n",
    "from pathlib import Path\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "OUT_DIR = Path(CONFIG[\"out_dir\"])\n",
    "RAW_DIR = OUT_DIR / CONFIG.get(\"USGS_RAW_DIRNAME\", \"usgs_raw\")\n",
    "OUT_PARQUET = OUT_DIR / CONFIG.get(\"USGS_DAILY_LONG_PARQUET_NAME\", \"usgs_dv_daily_long.parquet\")\n",
    "\n",
    "DERIVED_DIR = OUT_DIR / \"derived\"\n",
    "DERIVED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "YEAR_SHARDS_DIR = DERIVED_DIR / CONFIG.get(\"USGS_CELL_A_YEAR_SHARDS_DIRNAME\", \"usgs_cellA_year_shards\")\n",
    "YEAR_SHARDS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"USGS Cell A starting (resume-safe hardened parser, block-wise)\")\n",
    "print(\"  RAW_DIR      :\", RAW_DIR)\n",
    "print(\"  OUT_PARQUET  :\", OUT_PARQUET)\n",
    "print(\"  YEAR_SHARDS  :\", YEAR_SHARDS_DIR)\n",
    "\n",
    "def year_from_filename(name: str) -> int | None:\n",
    "    m = re.search(r\"_(\\d{4})\\.rdb$\", name)\n",
    "    return int(m.group(1)) if m else None\n",
    "\n",
    "_num_prefix_re = re.compile(r\"^\\s*([-+]?\\d+(?:\\.\\d+)?(?:[eE][-+]?\\d+)?)\")\n",
    "\n",
    "def parse_numeric_prefix_and_suffix(s: str):\n",
    "    \"\"\"\n",
    "    '14n' -> (14.0, 'n')\n",
    "    '0.08' -> (0.08, NA)\n",
    "    ''/None -> (NaN, NA)\n",
    "    \"\"\"\n",
    "    if s is None:\n",
    "        return (np.nan, pd.NA)\n",
    "    s = str(s).strip()\n",
    "    if not s:\n",
    "        return (np.nan, pd.NA)\n",
    "    m = _num_prefix_re.match(s)\n",
    "    if not m:\n",
    "        return (np.nan, pd.NA)\n",
    "    num = pd.to_numeric(m.group(1), errors=\"coerce\")\n",
    "    suffix = s[m.end():].strip()\n",
    "    if suffix == \"\":\n",
    "        suffix = pd.NA\n",
    "    return (float(num) if pd.notna(num) else np.nan, suffix)\n",
    "\n",
    "def is_spec_line(line: str) -> bool:\n",
    "    # Typical RDB spec line: \"5s\\t15s\\t20d\\t...\"\n",
    "    return bool(re.match(r\"^[0-9A-Za-z]+s(\\t[0-9A-Za-z]+s)+$\", line))\n",
    "\n",
    "def choose_value_col_for_header(header: list[str], sample_lines: list[str], dt_col: str):\n",
    "    \"\"\"\n",
    "    Choose best discharge col within THIS header.\n",
    "    Candidates are columns containing '_00060_' excluding *_cd and excluding obvious non-data cols.\n",
    "    Score candidates by numeric-prefix rate on sample_lines.\n",
    "    \"\"\"\n",
    "    candidates = [c for c in header if (\"_00060_\" in c) and (not c.endswith(\"_cd\"))]\n",
    "    if not candidates:\n",
    "        candidates = [c for c in header if (\"00060\" in c) and (not c.endswith(\"_cd\")) and (c not in (\"agency_cd\",\"site_no\",dt_col))]\n",
    "\n",
    "    if not candidates:\n",
    "        raise KeyError(f\"No discharge candidates in header: {header[:20]} ... (n={len(header)})\")\n",
    "\n",
    "    idx = {c: header.index(c) for c in candidates}\n",
    "    hlen = len(header)\n",
    "\n",
    "    def score_col(col: str) -> float:\n",
    "        j = idx[col]\n",
    "        good = 0\n",
    "        tot = 0\n",
    "        for ln in sample_lines:\n",
    "            if not ln or ln.startswith(\"#\"):\n",
    "                continue\n",
    "            parts = ln.split(\"\\t\")\n",
    "            # normalize width\n",
    "            if len(parts) < hlen:\n",
    "                parts = parts + [\"\"] * (hlen - len(parts))\n",
    "            elif len(parts) > hlen:\n",
    "                parts = parts[:hlen]\n",
    "            v = parts[j]\n",
    "            m = _num_prefix_re.match(str(v).strip())\n",
    "            tot += 1\n",
    "            if m:\n",
    "                good += 1\n",
    "        return good / tot if tot else 0.0\n",
    "\n",
    "    best = None\n",
    "    best_rate = -1.0\n",
    "    for c in candidates:\n",
    "        r = score_col(c)\n",
    "        if r > best_rate:\n",
    "            best_rate = r\n",
    "            best = c\n",
    "\n",
    "    qual_col = (best + \"_cd\") if (best and (best + \"_cd\") in header) else None\n",
    "    return best, qual_col, best_rate, candidates\n",
    "\n",
    "def parse_rdb_year_blockwise(path: Path):\n",
    "    \"\"\"\n",
    "    Stream parse an RDB file, handling repeated headers.\n",
    "    Produces standardized rows only (no wide dataframe).\n",
    "    \"\"\"\n",
    "    rows_out = []\n",
    "    headers_seen = 0\n",
    "    bad_lines = 0\n",
    "\n",
    "    header = None\n",
    "    dt_col = None\n",
    "    value_col = None\n",
    "    qual_col = None\n",
    "\n",
    "    # We use a small buffer after each header to pick the best value column for that header.\n",
    "    buffer_lines = []\n",
    "    buffer_target = 300  # small lookahead; cheap but robust\n",
    "\n",
    "    def flush_buffer_with_chosen_cols():\n",
    "        nonlocal buffer_lines, value_col, qual_col, bad_lines, rows_out\n",
    "        if not buffer_lines:\n",
    "            return\n",
    "        # process buffered data lines using current header/value_col/qual_col\n",
    "        hlen = len(header)\n",
    "        i_ag = header.index(\"agency_cd\") if \"agency_cd\" in header else None\n",
    "        i_site = header.index(\"site_no\") if \"site_no\" in header else None\n",
    "        i_dt = header.index(dt_col)\n",
    "        i_val = header.index(value_col) if value_col in header else None\n",
    "        i_q = header.index(qual_col) if (qual_col and qual_col in header) else None\n",
    "\n",
    "        for ln in buffer_lines:\n",
    "            if not ln or ln.startswith(\"#\"):\n",
    "                continue\n",
    "            if is_spec_line(ln):\n",
    "                continue\n",
    "            parts = ln.split(\"\\t\")\n",
    "            if len(parts) != hlen:\n",
    "                if len(parts) > hlen:\n",
    "                    parts = parts[:hlen]\n",
    "                else:\n",
    "                    parts = parts + [\"\"] * (hlen - len(parts))\n",
    "                bad_lines += 1\n",
    "\n",
    "            # required fields\n",
    "            if i_site is None or i_val is None:\n",
    "                continue\n",
    "            site_no = parts[i_site].strip()\n",
    "            if not site_no:\n",
    "                continue\n",
    "\n",
    "            date_raw = parts[i_dt].strip()\n",
    "            if not date_raw:\n",
    "                continue\n",
    "\n",
    "            v_raw = parts[i_val]\n",
    "            v_num, v_flag = parse_numeric_prefix_and_suffix(v_raw)\n",
    "\n",
    "            q_raw = parts[i_q].strip() if i_q is not None else None\n",
    "            agency = parts[i_ag].strip() if i_ag is not None else None\n",
    "\n",
    "            rows_out.append({\n",
    "                \"agency_cd\": agency,\n",
    "                \"site_no\": site_no,\n",
    "                \"date\": date_raw,\n",
    "                \"discharge_cfs\": v_num,\n",
    "                \"discharge_qual\": q_raw if q_raw != \"\" else None,\n",
    "                \"discharge_flag\": v_flag,\n",
    "                \"__source_file\": path.name,\n",
    "            })\n",
    "\n",
    "        buffer_lines = []\n",
    "\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "        for raw in f:\n",
    "            line = raw.rstrip(\"\\n\")\n",
    "            if not line or line.startswith(\"#\"):\n",
    "                continue\n",
    "\n",
    "            # header detected\n",
    "            if line.startswith(\"agency_cd\\t\") and (\"\\tdatetime\\t\" in line or \"\\tdateTime\\t\" in line):\n",
    "                # If we were in the middle of a block, flush any buffered lines before switching header\n",
    "                if header is not None and value_col is not None:\n",
    "                    flush_buffer_with_chosen_cols()\n",
    "\n",
    "                header = line.split(\"\\t\")\n",
    "                dt_col = \"datetime\" if \"datetime\" in header else \"dateTime\"\n",
    "                headers_seen += 1\n",
    "                value_col = None\n",
    "                qual_col = None\n",
    "                buffer_lines = []\n",
    "                continue\n",
    "\n",
    "            if header is None:\n",
    "                continue\n",
    "\n",
    "            # skip spec lines\n",
    "            if is_spec_line(line):\n",
    "                continue\n",
    "\n",
    "            # if we haven't chosen the discharge column for this header yet, buffer some lines\n",
    "            if value_col is None:\n",
    "                buffer_lines.append(line)\n",
    "                if len(buffer_lines) >= buffer_target:\n",
    "                    value_col, qual_col, rate, cand = choose_value_col_for_header(header, buffer_lines, dt_col)\n",
    "                    # now process the buffer\n",
    "                    flush_buffer_with_chosen_cols()\n",
    "                continue\n",
    "\n",
    "            # otherwise process immediately (fast path)\n",
    "            buffer_lines.append(line)\n",
    "            if len(buffer_lines) >= 2000:\n",
    "                flush_buffer_with_chosen_cols()\n",
    "\n",
    "    # end of file: finalize\n",
    "    if header is not None and value_col is None and buffer_lines:\n",
    "        # choose with whatever we have\n",
    "        value_col, qual_col, rate, cand = choose_value_col_for_header(header, buffer_lines, dt_col)\n",
    "    if header is not None and value_col is not None:\n",
    "        flush_buffer_with_chosen_cols()\n",
    "\n",
    "    df = pd.DataFrame(rows_out)\n",
    "    return df, headers_seen, bad_lines\n",
    "\n",
    "def finalize_diagnostics(path: Path):\n",
    "    df = pd.read_parquet(path)\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\", utc=True)\n",
    "    df[\"discharge_cfs\"] = pd.to_numeric(df[\"discharge_cfs\"], errors=\"coerce\")\n",
    "\n",
    "    null_q = int(df[\"discharge_cfs\"].isna().sum())\n",
    "    frac_null = null_q / max(1, len(df))\n",
    "\n",
    "    print(\"\\n[FINALIZE] USGS Cell A output diagnostics\")\n",
    "    print(\"  rows      :\", f\"{len(df):,}\")\n",
    "    print(\"  sites     :\", f\"{df['site_no'].astype(str).nunique():,}\")\n",
    "    print(\"  date min  :\", df[\"date\"].min())\n",
    "    print(\"  date max  :\", df[\"date\"].max())\n",
    "    print(\"  null discharge_cfs :\", f\"{null_q:,}\", f\"({frac_null*100:.2f}%)\")\n",
    "    display(df.head(5))\n",
    "\n",
    "# --- No rebuild if final exists (your rule) ---\n",
    "if OUT_PARQUET.exists() and OUT_PARQUET.stat().st_size > 1_000_000:\n",
    "    print(f\"[SKIP] Final output exists ({OUT_PARQUET.stat().st_size/1e6:.1f} MB). Not rebuilding.\")\n",
    "    finalize_diagnostics(OUT_PARQUET)\n",
    "\n",
    "else:\n",
    "    if not RAW_DIR.exists():\n",
    "        raise FileNotFoundError(f\"Missing RAW_DIR: {RAW_DIR}\")\n",
    "\n",
    "    in_files = sorted([\n",
    "        p for p in RAW_DIR.glob(\"usgs_dv_ca_st_00060_*.rdb\")\n",
    "        if re.search(r\"_\\d{4}\\.rdb$\", p.name)\n",
    "    ])\n",
    "    if not in_files:\n",
    "        in_files = sorted([p for p in RAW_DIR.glob(\"*.rdb\") if \"1991-2025\" not in p.name])\n",
    "    if not in_files:\n",
    "        raise FileNotFoundError(f\"No per-year .rdb files found under {RAW_DIR}\")\n",
    "\n",
    "    print(f\"  Found yearly chunks: {len(in_files)}\")\n",
    "    print(\"  First/last:\", in_files[0].name, \"→\", in_files[-1].name)\n",
    "\n",
    "    for fp in in_files:\n",
    "        year = year_from_filename(fp.name)\n",
    "        if year is None:\n",
    "            continue\n",
    "\n",
    "        shard_path = YEAR_SHARDS_DIR / f\"usgs_dv_daily_long_{year}.parquet\"\n",
    "        if shard_path.exists() and shard_path.stat().st_size > 1_000_000:\n",
    "            print(f\"[SKIP] year {year} shard exists: {shard_path.name}\")\n",
    "            continue\n",
    "\n",
    "        df, headers_seen, bad_lines = parse_rdb_year_blockwise(fp)\n",
    "\n",
    "        # Normalize date and drop invalids\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\", utc=True)\n",
    "        df[\"site_no\"] = df[\"site_no\"].astype(str)\n",
    "\n",
    "        df = df.dropna(subset=[\"date\", \"site_no\"]).copy()\n",
    "\n",
    "        nn = int(pd.to_numeric(df[\"discharge_cfs\"], errors=\"coerce\").notna().sum())\n",
    "        pct = (nn / max(1, len(df))) * 100.0\n",
    "        print(f\"[OK] year {year} shard: rows={len(df):,} nonnull_discharge={nn:,} ({pct:.2f}%) \"\n",
    "              f\"| headers_seen={headers_seen} | ragged_fixed={bad_lines:,}\")\n",
    "\n",
    "        # Fail-fast: if still catastrophically low, stop before burning more time\n",
    "        if len(df) > 1000 and (nn / len(df)) < 0.05:\n",
    "            raise RuntimeError(\n",
    "                f\"Year {year} still produced <5% non-null discharge after block-wise parsing. \"\n",
    "                f\"This indicates the RDB value strings are not numeric-prefix parsable or the wrong column is being chosen.\"\n",
    "            )\n",
    "\n",
    "        df.to_parquet(shard_path, index=False)\n",
    "\n",
    "    # Assemble final from shards\n",
    "    shard_files = sorted(YEAR_SHARDS_DIR.glob(\"usgs_dv_daily_long_*.parquet\"))\n",
    "    if not shard_files:\n",
    "        raise RuntimeError(\"No year shards found to assemble final output.\")\n",
    "\n",
    "    print(f\"[ASSEMBLE] Building final from {len(shard_files)} year shards...\")\n",
    "    parts = [pd.read_parquet(p) for p in shard_files]\n",
    "    out = pd.concat(parts, ignore_index=True).sort_values([\"site_no\",\"date\"]).reset_index(drop=True)\n",
    "    out.to_parquet(OUT_PARQUET, index=False)\n",
    "    print(f\"[OK] Wrote final: {OUT_PARQUET} rows={len(out):,}\")\n",
    "\n",
    "    finalize_diagnostics(OUT_PARQUET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea803376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# USGS NWIS DV (daily values) — Cell B (site -> grid mapping + daily gridding shards)\n",
    "# - Inputs:\n",
    "#   1) Grid parquet (EPSG:3310) from your earlier grid build\n",
    "#   2) USGS daily parquet from Cell A (site_no, date, discharge_cfs, ...)\n",
    "# - Goal:\n",
    "#   Create a station->grid lookup (site centroids), then produce daily shards for your analysis window,\n",
    "#   and a final parquet of grid_id x date x discharge.\n",
    "#\n",
    "# IMPORTANT:\n",
    "#   USGS DV downloads do NOT reliably include lat/lon. This cell therefore:\n",
    "#     A) Tries to use lat/lon if already present in Cell A output, else\n",
    "#     B) Fetches site metadata (lat/lon) via USGS site service, cached locally, then proceeds.\n",
    "#\n",
    "# Resume-safe:\n",
    "#   - mapping parquet is reused if exists\n",
    "#   - daily shard files are skipped if already exist\n",
    "# ============================\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import time\n",
    "import re\n",
    "\n",
    "FORCE_REMAP = True   # True -> rebuild station->grid mapping even if exists\n",
    "FORCE_SHARDS = True  # True -> rebuild daily shards even if they exist\n",
    "\n",
    "# ---------- config paths ----------\n",
    "OUT_DIR = Path(CONFIG[\"out_dir\"])\n",
    "DERIVED_DIR = OUT_DIR / \"derived\"\n",
    "DERIVED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "GRID_PATH = Path(resolve_out_path(CONFIG.get(\"GRID_FILENAME\", \"grid_3000m_CA_epsg3310.parquet\")))\n",
    "USGS_DAILY_PATH = OUT_DIR / CONFIG.get(\"USGS_DAILY_LONG_PARQUET_NAME\", \"usgs_dv_daily_long.parquet\")\n",
    "\n",
    "USGS_GRIDMAP_NAME = CONFIG.get(\"USGS_SITE_TO_GRID_PARQUET_NAME\", \"usgs_site_to_grid_3310.parquet\")\n",
    "USGS_SITE_META_NAME = CONFIG.get(\"USGS_SITE_META_PARQUET_NAME\", \"usgs_site_meta_ca_st.parquet\")\n",
    "\n",
    "GRIDMAP_PATH = OUT_DIR / USGS_GRIDMAP_NAME\n",
    "SITE_META_PATH = OUT_DIR / USGS_SITE_META_NAME\n",
    "\n",
    "SHARDS_DIR = DERIVED_DIR / CONFIG.get(\"USGS_GRID_SHARDS_DIRNAME\", \"usgs_grid_shards\")\n",
    "SHARDS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "FINAL_NAME = CONFIG.get(\n",
    "    \"USGS_DAILY_GRID_PARQUET_NAME\",\n",
    "    f\"usgs_daily_grid_CA_{CONFIG.get('grid_resolution_m',3000)}m_epsg{int(CONFIG.get('OPS_EPSG',3310))}_{CONFIG['start_date'].replace('-','')}_{CONFIG['end_date'].replace('-','')}.parquet\"\n",
    ")\n",
    "FINAL_PATH = OUT_DIR / FINAL_NAME\n",
    "\n",
    "# ---------- CRS ----------\n",
    "OPS_EPSG = int(CONFIG.get(\"OPS_EPSG\", 3310))\n",
    "\n",
    "# ---------- time window ----------\n",
    "analysis_start = pd.to_datetime(CONFIG[\"start_date\"], utc=True).floor(\"D\")\n",
    "analysis_end   = pd.to_datetime(CONFIG[\"end_date\"],   utc=True).floor(\"D\")\n",
    "\n",
    "print(\"USGS Cell B starting\")\n",
    "print(\"  Grid     :\", GRID_PATH)\n",
    "print(\"  USGS A   :\", USGS_DAILY_PATH)\n",
    "print(\"  Site meta:\", SITE_META_PATH, \"(cache)\")\n",
    "print(\"  Gridmap  :\", GRIDMAP_PATH)\n",
    "print(\"  Shards   :\", SHARDS_DIR)\n",
    "print(\"  FINAL    :\", FINAL_PATH)\n",
    "print(\"  Config window:\", analysis_start.date(), \"→\", analysis_end.date())\n",
    "\n",
    "if not GRID_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Missing grid parquet: {GRID_PATH}\")\n",
    "if not USGS_DAILY_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Missing USGS daily parquet (Cell A output): {USGS_DAILY_PATH}\")\n",
    "\n",
    "# ---------- load grid ----------\n",
    "grid = pd.read_parquet(GRID_PATH)\n",
    "if \"grid_id\" not in grid.columns:\n",
    "    raise KeyError(\"Grid parquet must include grid_id\")\n",
    "if \"geometry\" not in grid.columns:\n",
    "    raise KeyError(\"Grid parquet must include geometry column (GeoParquet)\")\n",
    "\n",
    "# IMPORTANT: some parquet stores geometry as WKB bytes; coerce if needed\n",
    "try:\n",
    "    geom = grid[\"geometry\"]\n",
    "    if len(geom) and isinstance(geom.iloc[0], (bytes, bytearray, memoryview)):\n",
    "        ggrid = gpd.GeoDataFrame(grid.copy(), geometry=gpd.GeoSeries.from_wkb(grid[\"geometry\"]), crs=f\"EPSG:{OPS_EPSG}\")\n",
    "    else:\n",
    "        ggrid = gpd.GeoDataFrame(grid.copy(), geometry=\"geometry\", crs=f\"EPSG:{OPS_EPSG}\")\n",
    "except Exception:\n",
    "    ggrid = gpd.GeoDataFrame(grid.copy(), geometry=gpd.GeoSeries.from_wkb(grid[\"geometry\"]), crs=f\"EPSG:{OPS_EPSG}\")\n",
    "\n",
    "ggrid = ggrid[[\"grid_id\", \"geometry\"]].copy()\n",
    "ggrid[\"grid_id\"] = ggrid[\"grid_id\"].astype(str)\n",
    "\n",
    "# ---------- load USGS daily ----------\n",
    "usgs = pd.read_parquet(USGS_DAILY_PATH)\n",
    "\n",
    "req_cols = {\"site_no\", \"date\", \"discharge_cfs\"}\n",
    "missing = req_cols - set(usgs.columns)\n",
    "if missing:\n",
    "    raise KeyError(f\"USGS daily parquet missing required columns: {sorted(missing)}\")\n",
    "\n",
    "usgs[\"site_no\"] = usgs[\"site_no\"].astype(str).str.strip()\n",
    "usgs[\"date\"] = pd.to_datetime(usgs[\"date\"], utc=True, errors=\"coerce\").dt.floor(\"D\")\n",
    "\n",
    "if usgs[\"date\"].isna().all():\n",
    "    raise RuntimeError(\"All USGS dates parsed to NaT. Check Cell A output 'date' column formatting.\")\n",
    "\n",
    "print(\"  USGS parquet date span:\", usgs[\"date\"].min().date(), \"→\", usgs[\"date\"].max().date(), f\"(rows={len(usgs):,})\")\n",
    "\n",
    "mask = (usgs[\"date\"] >= analysis_start) & (usgs[\"date\"] <= analysis_end)\n",
    "usgs_win = usgs.loc[mask].copy()\n",
    "\n",
    "print(\"  Effective window:\", analysis_start.date(), \"→\", analysis_end.date(), f\"(rows={len(usgs_win):,})\")\n",
    "\n",
    "if usgs_win.empty:\n",
    "    raise RuntimeError(\n",
    "        \"USGS daily data has zero rows in analysis window. \"\n",
    "        \"This indicates either CONFIG start/end does not overlap the parquet, or date parsing went wrong.\"\n",
    "    )\n",
    "\n",
    "# ---------- site number normalization ----------\n",
    "def normalize_site_no(s: str) -> str:\n",
    "    s = str(s).strip()\n",
    "    if s.isdigit():\n",
    "        return s\n",
    "    digits = re.sub(r\"\\D+\", \"\", s)\n",
    "    return digits if digits else s\n",
    "\n",
    "usgs_win[\"site_no_norm\"] = usgs_win[\"site_no\"].map(normalize_site_no)\n",
    "sites_needed_norm = sorted(usgs_win[\"site_no_norm\"].dropna().unique().tolist())\n",
    "print(f\"[META] sites needed in window: {len(sites_needed_norm):,}\")\n",
    "\n",
    "# ---------- station metadata acquisition (robust cache merge + coverage checks) ----------\n",
    "def fetch_usgs_site_meta(site_nos: list[str]) -> pd.DataFrame:\n",
    "    base = \"https://waterservices.usgs.gov/nwis/site/\"\n",
    "    rows = []\n",
    "    chunk_size = int(CONFIG.get(\"USGS_SITE_META_CHUNK_SIZE\", 250))\n",
    "    timeout_s = int(CONFIG.get(\"USGS_TIMEOUT_S\", 120))\n",
    "    sleep_s = float(CONFIG.get(\"USGS_META_SLEEP_S\", 0.2))\n",
    "\n",
    "    for i in range(0, len(site_nos), chunk_size):\n",
    "        chunk = site_nos[i:i+chunk_size]\n",
    "        params = {\"format\": \"rdb\", \"sites\": \",\".join(chunk), \"siteStatus\": \"all\"}\n",
    "        r = requests.get(base, params=params, timeout=timeout_s)\n",
    "        r.raise_for_status()\n",
    "        lines = r.text.splitlines()\n",
    "\n",
    "        header_idx = None\n",
    "        for j, line in enumerate(lines[:800]):\n",
    "            if not line or line.startswith(\"#\"):\n",
    "                continue\n",
    "            if (\"site_no\" in line) and (\"dec_lat_va\" in line) and (\"dec_long_va\" in line):\n",
    "                header_idx = j\n",
    "                break\n",
    "        if header_idx is None:\n",
    "            print(f\"[WARN] site service response missing expected header for chunk starting {chunk[0]}\")\n",
    "            continue\n",
    "\n",
    "        header = lines[header_idx]\n",
    "        data_lines = [ln for ln in lines[header_idx+2:] if ln and not ln.startswith(\"#\")]\n",
    "        if not data_lines:\n",
    "            continue\n",
    "\n",
    "        tsv = \"\\n\".join([header] + data_lines)\n",
    "        df = pd.read_csv(pd.io.common.StringIO(tsv), sep=\"\\t\", dtype=str)\n",
    "\n",
    "        out = pd.DataFrame({\n",
    "            \"site_no\": df.get(\"site_no\"),\n",
    "            \"site_name\": df.get(\"station_nm\"),\n",
    "            \"lat\": pd.to_numeric(df.get(\"dec_lat_va\"), errors=\"coerce\"),\n",
    "            \"lon\": pd.to_numeric(df.get(\"dec_long_va\"), errors=\"coerce\"),\n",
    "        })\n",
    "        out[\"site_no\"] = out[\"site_no\"].astype(str).str.strip()\n",
    "        out[\"site_no_norm\"] = out[\"site_no\"].map(normalize_site_no)\n",
    "\n",
    "        out = out.dropna(subset=[\"site_no_norm\"]).copy()\n",
    "        rows.append(out)\n",
    "        time.sleep(sleep_s)\n",
    "\n",
    "    if not rows:\n",
    "        return pd.DataFrame(columns=[\"site_no\",\"site_name\",\"lat\",\"lon\",\"site_no_norm\"])\n",
    "\n",
    "    meta = pd.concat(rows, ignore_index=True)\n",
    "    meta[\"site_no_norm\"] = meta[\"site_no_norm\"].astype(str).str.strip()\n",
    "    meta = meta.drop_duplicates(subset=[\"site_no_norm\"])\n",
    "    return meta\n",
    "\n",
    "def load_site_meta_cache() -> pd.DataFrame:\n",
    "    if not SITE_META_PATH.exists():\n",
    "        return pd.DataFrame(columns=[\"site_no\",\"site_name\",\"lat\",\"lon\",\"site_no_norm\"])\n",
    "    sm = pd.read_parquet(SITE_META_PATH)\n",
    "\n",
    "    # backward compat\n",
    "    if \"site_no_norm\" not in sm.columns:\n",
    "        if \"site_no\" not in sm.columns:\n",
    "            sm[\"site_no\"] = np.nan\n",
    "        sm[\"site_no\"] = sm[\"site_no\"].astype(str).str.strip()\n",
    "        sm[\"site_no_norm\"] = sm[\"site_no\"].map(normalize_site_no)\n",
    "        sm.to_parquet(SITE_META_PATH, index=False)\n",
    "        print(f\"[META] upgraded cache schema (added site_no_norm): {SITE_META_PATH}\")\n",
    "\n",
    "    for c in [\"site_no\",\"site_name\",\"lat\",\"lon\",\"site_no_norm\"]:\n",
    "        if c not in sm.columns:\n",
    "            sm[c] = np.nan\n",
    "    sm[\"site_no_norm\"] = sm[\"site_no_norm\"].astype(str).str.strip()\n",
    "    return sm\n",
    "\n",
    "site_meta = load_site_meta_cache()\n",
    "\n",
    "needed_set = set(sites_needed_norm)\n",
    "have_any = set(site_meta[\"site_no_norm\"].dropna().astype(str))\n",
    "have_latlon = set(site_meta.loc[site_meta[\"lat\"].notna() & site_meta[\"lon\"].notna(), \"site_no_norm\"].dropna().astype(str))\n",
    "print(f\"[META] cache coverage: have_any={len(needed_set & have_any):,}/{len(needed_set):,} | have_latlon={len(needed_set & have_latlon):,}/{len(needed_set):,}\")\n",
    "\n",
    "missing_norm = [s for s in sites_needed_norm if s not in have_any]\n",
    "if missing_norm:\n",
    "    print(f\"[META] fetching missing sites: {len(missing_norm):,}\")\n",
    "    fetched = fetch_usgs_site_meta(missing_norm)\n",
    "\n",
    "    # append-only; never shrink\n",
    "    site_meta = pd.concat([site_meta, fetched], ignore_index=True)\n",
    "    site_meta[\"site_no_norm\"] = site_meta[\"site_no_norm\"].astype(str).str.strip()\n",
    "    site_meta = site_meta.drop_duplicates(subset=[\"site_no_norm\"])\n",
    "\n",
    "    site_meta.to_parquet(SITE_META_PATH, index=False)\n",
    "    print(f\"[META] updated cache: {SITE_META_PATH} (rows={len(site_meta):,})\")\n",
    "\n",
    "have_latlon = set(site_meta.loc[site_meta[\"lat\"].notna() & site_meta[\"lon\"].notna(), \"site_no_norm\"].dropna().astype(str))\n",
    "print(f\"[META] post-fetch coverage: have_latlon={len(needed_set & have_latlon):,}/{len(needed_set):,}\")\n",
    "\n",
    "site_meta_use = site_meta.loc[\n",
    "    site_meta[\"site_no_norm\"].isin(sites_needed_norm) &\n",
    "    site_meta[\"lat\"].notna() & site_meta[\"lon\"].notna()\n",
    "].copy()\n",
    "\n",
    "if site_meta_use.empty:\n",
    "    raise RuntimeError(\"No sites with lat/lon available after metadata acquisition; cannot grid USGS discharge.\")\n",
    "\n",
    "# ---------- build or reuse site->grid map ----------\n",
    "if GRIDMAP_PATH.exists() and not FORCE_REMAP:\n",
    "    gridmap = pd.read_parquet(GRIDMAP_PATH)\n",
    "    print(f\"[SKIP] Using existing gridmap: {GRIDMAP_PATH} (rows={len(gridmap):,})\")\n",
    "else:\n",
    "    gs = gpd.GeoDataFrame(\n",
    "        site_meta_use.copy(),\n",
    "        geometry=[Point(xy) for xy in zip(site_meta_use[\"lon\"].astype(float), site_meta_use[\"lat\"].astype(float))],\n",
    "        crs=\"EPSG:4326\"\n",
    "    ).to_crs(epsg=OPS_EPSG)\n",
    "\n",
    "    joined = gpd.sjoin_nearest(\n",
    "        gs[[\"site_no_norm\",\"site_no\",\"site_name\",\"lat\",\"lon\",\"geometry\"]],\n",
    "        ggrid[[\"grid_id\",\"geometry\"]],\n",
    "        how=\"left\",\n",
    "        distance_col=\"dist_m\"\n",
    "    )\n",
    "\n",
    "    gridmap = joined.drop(columns=[\"geometry\"], errors=\"ignore\").copy()\n",
    "    gridmap[\"grid_id\"] = gridmap[\"grid_id\"].astype(str)\n",
    "    gridmap[\"dist_km\"] = pd.to_numeric(gridmap.get(\"dist_m\"), errors=\"coerce\") / 1000.0\n",
    "    gridmap.drop(columns=[\"dist_m\"], inplace=True, errors=\"ignore\")\n",
    "\n",
    "    gridmap.to_parquet(GRIDMAP_PATH, index=False)\n",
    "    print(f\"[OK] Saved gridmap: {GRIDMAP_PATH} (rows={len(gridmap):,})\")\n",
    "\n",
    "# ---------- build daily shards (site->grid, then aggregate per grid/day) ----------\n",
    "# Normalize dates *again* on the working frame to guarantee exact equality match vs `days`.\n",
    "tmp = usgs_win.copy()\n",
    "tmp[\"date\"] = pd.to_datetime(tmp[\"date\"], utc=True, errors=\"coerce\").dt.floor(\"D\")\n",
    "tmp[\"site_no_norm\"] = tmp[\"site_no_norm\"].astype(str).str.strip()\n",
    "\n",
    "tmp = tmp.merge(gridmap[[\"site_no_norm\",\"grid_id\"]], on=\"site_no_norm\", how=\"inner\")\n",
    "if tmp.empty:\n",
    "    raise RuntimeError(\"After joining site->grid, no rows remain (check site_no_norm mapping + gridmap).\")\n",
    "\n",
    "days = pd.date_range(analysis_start, analysis_end, freq=\"D\", tz=\"UTC\")\n",
    "\n",
    "wrote = 0\n",
    "skipped = 0\n",
    "\n",
    "for i, day in enumerate(days, start=1):\n",
    "    shard = SHARDS_DIR / f\"usgs_grid_{day.date().isoformat()}.parquet\"\n",
    "    if shard.exists() and not FORCE_SHARDS:\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "    ddf = tmp.loc[tmp[\"date\"] == day.floor(\"D\")].copy()\n",
    "    if ddf.empty:\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "    out = (\n",
    "        ddf.groupby([\"grid_id\",\"date\"], as_index=False)\n",
    "        .agg(\n",
    "            discharge_cfs=(\"discharge_cfs\",\"mean\"),\n",
    "            n_gages=(\"discharge_cfs\",\"count\"),\n",
    "        )\n",
    "    )\n",
    "    out.to_parquet(shard, index=False)\n",
    "    wrote += 1\n",
    "\n",
    "    if (i % 20 == 0) or (i == len(days)):\n",
    "        print(f\"  day {i}/{len(days)} | wrote={wrote} skipped={skipped} | last={day.date()} | rows={len(out):,}\")\n",
    "\n",
    "print(\"USGS Cell B shards complete\")\n",
    "print(\"  wrote  :\", wrote)\n",
    "print(\"  skipped:\", skipped)\n",
    "\n",
    "# ---------- combine shards into FINAL parquet ----------\n",
    "shard_files = sorted(SHARDS_DIR.glob(\"usgs_grid_*.parquet\"))\n",
    "if not shard_files:\n",
    "    raise RuntimeError(f\"No shard files found in {SHARDS_DIR}. This indicates the shard loop never wrote any day with data.\")\n",
    "\n",
    "final = pd.concat([pd.read_parquet(p) for p in shard_files], ignore_index=True)\n",
    "\n",
    "final[\"date\"] = pd.to_datetime(final[\"date\"], utc=True).dt.floor(\"D\")\n",
    "final[\"grid_id\"] = final[\"grid_id\"].astype(str)\n",
    "final.sort_values([\"grid_id\",\"date\"], inplace=True)\n",
    "\n",
    "final.to_parquet(FINAL_PATH, index=False)\n",
    "print(f\"[OK] Saved FINAL: {FINAL_PATH} rows={len(final):,} cells={final['grid_id'].nunique():,} dates={final['date'].nunique():,}\")\n",
    "display(final.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18b0062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# diagnostics: gridmap coverage\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "OUT_DIR = Path(CONFIG[\"out_dir\"])\n",
    "DERIVED_DIR = OUT_DIR / \"derived\"\n",
    "\n",
    "USGS_DAILY_PATH = OUT_DIR / CONFIG.get(\"USGS_DAILY_LONG_PARQUET_NAME\", \"usgs_dv_daily_long.parquet\")\n",
    "GRIDMAP_PATH    = OUT_DIR / CONFIG.get(\"USGS_SITE_TO_GRID_PARQUET_NAME\", \"usgs_site_to_grid_3310.parquet\")\n",
    "\n",
    "SHARDS_DIR = DERIVED_DIR / CONFIG.get(\"USGS_GRID_SHARDS_DIRNAME\", \"usgs_grid_shards\")\n",
    "FINAL_NAME = CONFIG.get(\n",
    "    \"USGS_DAILY_GRID_PARQUET_NAME\",\n",
    "    f\"usgs_daily_grid_CA_{CONFIG.get('grid_resolution_m',3000)}m_epsg{int(CONFIG.get('OPS_EPSG',3310))}_{CONFIG['start_date'].replace('-','')}_{CONFIG['end_date'].replace('-','')}.parquet\"\n",
    ")\n",
    "FINAL_PATH = OUT_DIR / FINAL_NAME\n",
    "\n",
    "analysis_start = pd.to_datetime(CONFIG[\"start_date\"], utc=True).floor(\"D\")\n",
    "analysis_end   = pd.to_datetime(CONFIG[\"end_date\"], utc=True).floor(\"D\")\n",
    "\n",
    "print(\"[USGS B DIAG] Paths\")\n",
    "print(\"  OUT_DIR     :\", OUT_DIR)\n",
    "print(\"  USGS_DAILY  :\", USGS_DAILY_PATH, \"| exists:\", USGS_DAILY_PATH.exists())\n",
    "print(\"  GRIDMAP     :\", GRIDMAP_PATH, \"| exists:\", GRIDMAP_PATH.exists())\n",
    "print(\"  SHARDS_DIR  :\", SHARDS_DIR, \"| exists:\", SHARDS_DIR.exists())\n",
    "print(\"  FINAL_PATH  :\", FINAL_PATH, \"| exists:\", FINAL_PATH.exists())\n",
    "print(\"  Window      :\", analysis_start.date(), \"→\", analysis_end.date())\n",
    "\n",
    "# 1) Confirm window rows exist\n",
    "usgs = pd.read_parquet(USGS_DAILY_PATH, columns=[\"site_no\", \"date\", \"discharge_cfs\"])\n",
    "usgs[\"date\"] = pd.to_datetime(usgs[\"date\"], utc=True).dt.floor(\"D\")\n",
    "mask = (usgs[\"date\"] >= analysis_start) & (usgs[\"date\"] <= analysis_end)\n",
    "usgs_win = usgs.loc[mask].copy()\n",
    "print(\"\\n[USGS B DIAG] Window rows in Cell A parquet\")\n",
    "print(\"  rows:\", len(usgs_win))\n",
    "print(\"  sites:\", usgs_win[\"site_no\"].astype(str).str.strip().nunique())\n",
    "\n",
    "# 2) Verify gridmap integrity\n",
    "gm = pd.read_parquet(GRIDMAP_PATH)\n",
    "print(\"\\n[USGS B DIAG] Gridmap columns:\", list(gm.columns))\n",
    "site_key = \"site_no_norm\" if \"site_no_norm\" in gm.columns else \"site_no\"\n",
    "print(\"  gridmap rows:\", len(gm))\n",
    "print(\"  unique sites:\", gm[site_key].astype(str).str.strip().nunique(), f\"(key={site_key})\")\n",
    "print(\"  unique grid_id:\", gm[\"grid_id\"].nunique())\n",
    "\n",
    "# 3) Locate any shard files anywhere under derived/\n",
    "print(\"\\n[USGS B DIAG] Shard discovery\")\n",
    "if DERIVED_DIR.exists():\n",
    "    all_usgs_shards = sorted(DERIVED_DIR.rglob(\"usgs_grid_*.parquet\"))\n",
    "else:\n",
    "    all_usgs_shards = []\n",
    "\n",
    "print(\"  derived dir exists:\", DERIVED_DIR.exists())\n",
    "print(\"  shards matching derived/**/usgs_grid_*.parquet:\", len(all_usgs_shards))\n",
    "if all_usgs_shards:\n",
    "    print(\"  first:\", all_usgs_shards[0])\n",
    "    print(\"  last :\", all_usgs_shards[-1])\n",
    "\n",
    "# 4) If shards exist, show a quick sample read\n",
    "if all_usgs_shards:\n",
    "    sample = pd.read_parquet(all_usgs_shards[0])\n",
    "    print(\"\\n[USGS B DIAG] Sample shard head:\")\n",
    "    print(sample.head())\n",
    "else:\n",
    "    print(\"\\n[USGS B DIAG] No shards found anywhere under derived/.\")\n",
    "    print(\"  That means Cell B never entered (or never completed) the shard-writing loop,\")\n",
    "    print(\"  or it crashed before writing the first shard.\")\n",
    "    print(\"  Next step: rerun Cell B and watch for any exception AFTER the gridmap is saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c11d662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODIS DATA DOWNLOAD\n",
    "# ============================\n",
    "# Manual data again\n",
    "# https://appeears.earthdatacloud.nasa.gov/task/area\n",
    "# parameters: name: hydropulse_ca_ndvi\n",
    "# GeoJSON file: results/california_boundary.geojson\n",
    "# date range: 2000-01-01 to 2020-12-31\n",
    "# products: MOD13Q1.061 – MODIS/Terra Vegetation Indices 16-Day L3 Global 250m\n",
    "# From this product, select only: NDVI, VI_Quality, EVI\n",
    "# Output format: GeoTIFF\n",
    "# Projection: Native Projection \n",
    "# ============================\n",
    "\n",
    "# Then repeat the same thing for 2024-06-01 to 2024-10-31\n",
    "# Emails were sent when the jobs were received\n",
    "# Now to wait for the files to be ready for download\n",
    "\n",
    "# Files are downloaded as follows:\n",
    "# results/\n",
    "# └── manual/\n",
    "#     └── modis_ndvi_analysis/\n",
    "#         │   ├── MOD13Q1_*.tif\n",
    "#         │   └── ...\n",
    "#     └── modis_ndvi_baseline/\n",
    "#         │   ├── MOD13Q1_*.tif\n",
    "#         │   └── ...\n",
    "\n",
    "\n",
    "# === MODIS NDVI (MOD13Q1) | Cell A: Manual AppEEARS GeoTIFFs -> clean long parquet (analysis window first) ===\n",
    "# Inputs:\n",
    "#   results/manual/modis_ndvi_analysis/*.tif  (AppEEARS Area task outputs)\n",
    "# Output:\n",
    "#   results/modis_vi_long_analysis.parquet\n",
    "#   results/derived/modis_cellA_shards_analysis/modis_vi_long_<YYYY-MM-DD>.parquet  (optional shards)\n",
    "#\n",
    "# Notes:\n",
    "# - This is Cell A only: parse, scale, QA-filter, and produce clean long-format records.\n",
    "# - No grid mapping happens here. Cell B will map pixels -> HydroPulse grid and aggregate per grid cell.\n",
    "# - Baseline volume is large; this cell targets analysis first via CONFIG[\"MODIS_ANALYSIS_DIR\"].\n",
    "\n",
    "import re\n",
    "from pathlib import Path\n",
    "from datetime import datetime as dt, timedelta\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "from rasterio.errors import RasterioIOError\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Core function\n",
    "# ----------------------------\n",
    "def run_modis_cellA(\n",
    "    *,\n",
    "    tag: str,\n",
    "    input_dir: Path,\n",
    "    out_dir: Path,\n",
    "    assemble_final: bool,\n",
    "    max_files: int | None = None,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    MODIS Cell A core processor.\n",
    "    - Writes per-date shard parquets under: out_dir/derived/modis_cellA_shards_<tag>/\n",
    "    - Optionally assembles a single long parquet under: out_dir/modis_vi_long_<tag>.parquet\n",
    "    - Resume-safe:\n",
    "        - If a shard exists, it is skipped with a diagnostic.\n",
    "        - If final parquet exists, it is skipped with a diagnostic.\n",
    "    - Reprocessing is manual: delete outputs to rebuild.\n",
    "    Returns a dict of diagnostics.\n",
    "    \"\"\"\n",
    "\n",
    "    # ---------- config knobs ----------\n",
    "    scale_factor = float(CONFIG.get(\"MODIS_SCALE_FACTOR\", 0.0001))\n",
    "\n",
    "    ENFORCE_QA = bool(CONFIG.get(\"MODIS_ENFORCE_QA\", False))\n",
    "    ACCEPT_SUMMARY_QA = set(CONFIG.get(\"MODIS_ACCEPT_SUMMARY_QA\", [0]))\n",
    "    ACCEPT_VI_QUALITY = set(CONFIG.get(\"MODIS_ACCEPT_VI_QUALITY\", []))\n",
    "\n",
    "    COMMON_FILLS = set(CONFIG.get(\"MODIS_COMMON_FILL_VALUES\", [-3000, -28672, -9999, 32767, 65535]))\n",
    "\n",
    "    # ---------- paths ----------\n",
    "    shards_dir = out_dir / \"derived\" / f\"modis_cellA_shards_{tag}\"\n",
    "    shards_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    out_long_parquet = out_dir / f\"modis_vi_long_{tag}.parquet\"\n",
    "\n",
    "    print(\"\\nMODIS Cell A starting\")\n",
    "    print(\"  tag            :\", tag)\n",
    "    print(\"  input_dir      :\", input_dir)\n",
    "    print(\"  shards_dir     :\", shards_dir)\n",
    "    print(\"  assemble_final :\", assemble_final)\n",
    "    print(\"  out_long       :\", out_long_parquet if assemble_final else \"(skipped)\")\n",
    "    print(\"  ENFORCE_QA     :\", ENFORCE_QA)\n",
    "\n",
    "    if not input_dir.exists():\n",
    "        raise FileNotFoundError(f\"MODIS input directory not found: {input_dir}\")\n",
    "\n",
    "    # ---------- helpers ----------\n",
    "    def parse_modis_date(name: str):\n",
    "        patterns = [\n",
    "            r\"_doy(\\d{4})(\\d{3})_\",     # AppEEARS canonical\n",
    "            r\"\\bA(\\d{4})(\\d{3})\\b\",     # MODIS token\n",
    "            r\"doy(\\d{4})(\\d{3})\",       # loose\n",
    "        ]\n",
    "        for pat in patterns:\n",
    "            m = re.search(pat, name)\n",
    "            if m:\n",
    "                y = int(m.group(1))\n",
    "                j = int(m.group(2))\n",
    "                d = dt(y, 1, 1) + timedelta(days=j - 1)\n",
    "                return d.date()\n",
    "        return None\n",
    "\n",
    "    def parse_feature_id_from_name(name: str):\n",
    "        m = re.search(r\"_(aid\\d{4})\", name)\n",
    "        return m.group(1) if m else None\n",
    "\n",
    "    def parse_var_from_name(name: str):\n",
    "        upper = name.upper()\n",
    "        if \"SUMMARYQA\" in upper or \"SUMMARY_QA\" in upper:\n",
    "            return \"SummaryQA\"\n",
    "        if \"VI_QUALITY\" in upper or \"VIQUALITY\" in upper:\n",
    "            return \"VI_Quality\"\n",
    "        if re.search(r\"\\bNDVI\\b\", upper):\n",
    "            return \"NDVI\"\n",
    "        if re.search(r\"\\bEVI\\b\", upper):\n",
    "            return \"EVI\"\n",
    "        return None\n",
    "\n",
    "    def list_geotiffs(root: Path):\n",
    "        tifs = []\n",
    "        for ext in (\"*.tif\", \"*.tiff\", \"*.TIF\", \"*.TIFF\"):\n",
    "            tifs.extend(root.rglob(ext))\n",
    "        tifs = [p for p in tifs if p.is_file() and p.stat().st_size > 0]\n",
    "        tifs = sorted(tifs)\n",
    "        if max_files is not None:\n",
    "            tifs = tifs[: int(max_files)]\n",
    "        return tifs\n",
    "\n",
    "    def pixel_centers_xy(transform, width: int, height: int):\n",
    "        cols = np.arange(width) + 0.5\n",
    "        rows = np.arange(height) + 0.5\n",
    "        xs = transform.c + transform.a * cols + transform.b * 0.5\n",
    "        ys = transform.f + transform.e * rows + transform.d * 0.5\n",
    "        return xs, ys\n",
    "\n",
    "    def read_single_geotiff_meta(path: Path):\n",
    "        name = path.name\n",
    "        date = parse_modis_date(name)\n",
    "        if date is None:\n",
    "            return None\n",
    "\n",
    "        feature_id = parse_feature_id_from_name(name) or CONFIG.get(\"MODIS_FEATURE_ID\")\n",
    "        var_hint = parse_var_from_name(name)\n",
    "\n",
    "        try:\n",
    "            with rasterio.open(path) as src:\n",
    "                crs_wkt = src.crs.to_wkt() if src.crs else None\n",
    "                transform = src.transform\n",
    "                width, height = src.width, src.height\n",
    "                nodata = src.nodata\n",
    "\n",
    "                arrays = {}\n",
    "\n",
    "                # Single-band (one variable per file)\n",
    "                if src.count == 1:\n",
    "                    arr = src.read(1)\n",
    "                    arrays[var_hint or \"NDVI\"] = arr\n",
    "\n",
    "                # Multi-band: AppEEARS sometimes packs NDVI/EVI/QA\n",
    "                elif src.count >= 3:\n",
    "                    b1, b2, b3 = src.read(1), src.read(2), src.read(3)\n",
    "                    if var_hint is not None:\n",
    "                        arrays[var_hint] = b1\n",
    "                    else:\n",
    "                        arrays[\"NDVI\"] = b1\n",
    "                        arrays[\"EVI\"] = b2\n",
    "                        arrays[\"VI_Quality\"] = b3\n",
    "\n",
    "                else:\n",
    "                    b1, b2 = src.read(1), src.read(2)\n",
    "                    if var_hint is not None:\n",
    "                        arrays[var_hint] = b1\n",
    "                    else:\n",
    "                        arrays[\"NDVI\"] = b1\n",
    "                        arrays[\"EVI\"] = b2\n",
    "\n",
    "                return {\n",
    "                    \"date\": date,\n",
    "                    \"feature_id\": feature_id,\n",
    "                    \"crs_wkt\": crs_wkt,\n",
    "                    \"transform\": transform,\n",
    "                    \"width\": width,\n",
    "                    \"height\": height,\n",
    "                    \"nodata\": nodata,\n",
    "                    \"arrays\": arrays,\n",
    "                    \"name\": name,\n",
    "                }\n",
    "\n",
    "        except RasterioIOError:\n",
    "            return None\n",
    "\n",
    "    def arrays_to_long_df(meta: dict):\n",
    "        arrays = meta[\"arrays\"]\n",
    "        if \"NDVI\" not in arrays:\n",
    "            return None, \"missing_ndvi\"\n",
    "\n",
    "        ndvi_raw = arrays[\"NDVI\"].astype(np.int32, copy=False)\n",
    "        valid = np.ones(ndvi_raw.shape, dtype=bool)\n",
    "\n",
    "        nodata = meta[\"nodata\"]\n",
    "        if nodata is not None:\n",
    "            valid &= (ndvi_raw != nodata)\n",
    "        else:\n",
    "            valid &= ~np.isin(ndvi_raw, list(COMMON_FILLS))\n",
    "\n",
    "        ndvi = ndvi_raw * scale_factor\n",
    "        valid &= np.isfinite(ndvi)\n",
    "        valid &= (ndvi >= -1.0) & (ndvi <= 1.0)\n",
    "\n",
    "        evi = None\n",
    "        if \"EVI\" in arrays:\n",
    "            evi_raw = arrays[\"EVI\"].astype(np.int32, copy=False)\n",
    "            if nodata is not None:\n",
    "                valid &= (evi_raw != nodata)\n",
    "            else:\n",
    "                valid &= ~np.isin(evi_raw, list(COMMON_FILLS))\n",
    "            evi = evi_raw * scale_factor\n",
    "            valid &= np.isfinite(evi)\n",
    "            valid &= (evi >= -1.0) & (evi <= 1.0)\n",
    "\n",
    "        if ENFORCE_QA:\n",
    "            if \"SummaryQA\" in arrays:\n",
    "                sqa = arrays[\"SummaryQA\"].astype(np.int32, copy=False)\n",
    "                if nodata is not None:\n",
    "                    valid &= (sqa != nodata)\n",
    "                if ACCEPT_SUMMARY_QA:\n",
    "                    valid &= np.isin(sqa, list(ACCEPT_SUMMARY_QA))\n",
    "            elif \"VI_Quality\" in arrays:\n",
    "                viq = arrays[\"VI_Quality\"].astype(np.int32, copy=False)\n",
    "                if nodata is not None:\n",
    "                    valid &= (viq != nodata)\n",
    "                if ACCEPT_VI_QUALITY:\n",
    "                    valid &= np.isin(viq, list(ACCEPT_VI_QUALITY))\n",
    "\n",
    "        if valid.sum() == 0:\n",
    "            return None, \"no_valid_pixels_after_filters\"\n",
    "\n",
    "        xs, ys = pixel_centers_xy(meta[\"transform\"], meta[\"width\"], meta[\"height\"])\n",
    "        rr, cc = np.where(valid)\n",
    "\n",
    "        df = pd.DataFrame({\n",
    "            \"date\": pd.to_datetime([meta[\"date\"]] * len(rr)),\n",
    "            \"feature_id\": [meta[\"feature_id\"]] * len(rr),\n",
    "            \"row\": rr.astype(np.int32),\n",
    "            \"col\": cc.astype(np.int32),\n",
    "            \"x\": xs[cc].astype(np.float64),\n",
    "            \"y\": ys[rr].astype(np.float64),\n",
    "            \"ndvi\": ndvi[rr, cc].astype(np.float32),\n",
    "            \"evi\": (evi[rr, cc].astype(np.float32) if evi is not None else np.nan),\n",
    "            \"src_file\": meta[\"name\"],\n",
    "            \"crs_wkt\": meta[\"crs_wkt\"],\n",
    "        })\n",
    "        return df, None\n",
    "\n",
    "    # ---------- main ----------\n",
    "    tifs = list_geotiffs(input_dir)\n",
    "    if len(tifs) == 0:\n",
    "        raise RuntimeError(f\"No GeoTIFFs found under: {input_dir}\")\n",
    "\n",
    "    print(f\"[INFO] Found {len(tifs)} GeoTIFFs under: {input_dir}\")\n",
    "    print(\"[DEBUG] First 10 GeoTIFF filenames:\")\n",
    "    for p in tifs[:10]:\n",
    "        print(\"  \", p.name)\n",
    "\n",
    "    skip_reasons = Counter()\n",
    "    groups = {}\n",
    "\n",
    "    for p in tifs:\n",
    "        meta = read_single_geotiff_meta(p)\n",
    "        if meta is None:\n",
    "            if re.search(r\"doy\\d{7}\", p.name, flags=re.IGNORECASE) is None and parse_modis_date(p.name) is None:\n",
    "                skip_reasons[\"date_parse_failed\"] += 1\n",
    "            else:\n",
    "                skip_reasons[\"read_failed_or_other\"] += 1\n",
    "            continue\n",
    "\n",
    "        key = (meta[\"date\"], meta[\"feature_id\"])\n",
    "        if key not in groups:\n",
    "            groups[key] = {\n",
    "                \"date\": meta[\"date\"],\n",
    "                \"feature_id\": meta[\"feature_id\"],\n",
    "                \"crs_wkt\": meta[\"crs_wkt\"],\n",
    "                \"transform\": meta[\"transform\"],\n",
    "                \"width\": meta[\"width\"],\n",
    "                \"height\": meta[\"height\"],\n",
    "                \"nodata\": meta[\"nodata\"],\n",
    "                \"arrays\": {},\n",
    "                \"names\": [],\n",
    "            }\n",
    "\n",
    "        for k, arr in meta[\"arrays\"].items():\n",
    "            groups[key][\"arrays\"][k] = arr\n",
    "        groups[key][\"names\"].append(meta[\"name\"])\n",
    "\n",
    "    print(f\"[INFO] Parsed into {len(groups)} date-groups\")\n",
    "    if skip_reasons:\n",
    "        print(\"[DEBUG] Early skip reasons:\", dict(skip_reasons))\n",
    "\n",
    "    wrote = 0\n",
    "    skipped = 0\n",
    "    shard_paths = []\n",
    "\n",
    "    for (date, fid), g in sorted(groups.items(), key=lambda x: x[0][0].isoformat()):\n",
    "        shard_path = shards_dir / f\"modis_vi_long_{pd.to_datetime(date).date().isoformat()}.parquet\"\n",
    "        if shard_path.exists() and shard_path.stat().st_size > 1_000_000:\n",
    "            skipped += 1\n",
    "            shard_paths.append(shard_path)\n",
    "            print(f\"[SKIP] Shard exists: {shard_path.name} ({shard_path.stat().st_size/1e6:.1f} MB)\")\n",
    "            continue\n",
    "\n",
    "        meta = {\n",
    "            \"date\": g[\"date\"],\n",
    "            \"feature_id\": g[\"feature_id\"],\n",
    "            \"crs_wkt\": g[\"crs_wkt\"],\n",
    "            \"transform\": g[\"transform\"],\n",
    "            \"width\": g[\"width\"],\n",
    "            \"height\": g[\"height\"],\n",
    "            \"nodata\": g[\"nodata\"],\n",
    "            \"arrays\": g[\"arrays\"],\n",
    "            \"name\": \"|\".join(g[\"names\"])[:500],\n",
    "        }\n",
    "        df, reason = arrays_to_long_df(meta)\n",
    "        if df is None:\n",
    "            skip_reasons[reason] += 1\n",
    "            continue\n",
    "\n",
    "        df.to_parquet(shard_path, index=False)\n",
    "        wrote += 1\n",
    "        shard_paths.append(shard_path)\n",
    "        print(f\"[OK] Wrote shard: {shard_path.name} rows={len(df):,}\")\n",
    "\n",
    "    if wrote == 0 and skipped == 0:\n",
    "        print(\"[ERROR] No shards produced. Diagnostics:\")\n",
    "        print(\"  groups:\", len(groups))\n",
    "        print(\"  skip_reasons:\", dict(skip_reasons))\n",
    "        print(\"  ENFORCE_QA:\", ENFORCE_QA)\n",
    "        raise RuntimeError(\"No MODIS shards were produced; check filename parsing, nodata, and QA settings.\")\n",
    "\n",
    "    print(f\"[DONE] MODIS Cell A shards: wrote={wrote:,} skipped={skipped:,} dir={shards_dir}\")\n",
    "\n",
    "    if assemble_final:\n",
    "        if out_long_parquet.exists() and out_long_parquet.stat().st_size > 5_000_000:\n",
    "            print(f\"[SKIP] Long parquet exists: {out_long_parquet} ({out_long_parquet.stat().st_size/1e6:.1f} MB)\")\n",
    "        else:\n",
    "            # Assemble only from shards we believe exist.\n",
    "            shard_files = sorted(shards_dir.glob(\"modis_vi_long_*.parquet\"))\n",
    "            if not shard_files:\n",
    "                raise RuntimeError(f\"No shards found to assemble in {shards_dir}\")\n",
    "\n",
    "            print(f\"[INFO] Assembling {len(shard_files)} shards into: {out_long_parquet}\")\n",
    "            dfs = [pd.read_parquet(p) for p in shard_files]\n",
    "            long_df = pd.concat(dfs, ignore_index=True)\n",
    "            long_df[\"date\"] = pd.to_datetime(long_df[\"date\"]).dt.normalize()\n",
    "            long_df = long_df.sort_values([\"date\", \"row\", \"col\"]).reset_index(drop=True)\n",
    "            long_df.to_parquet(out_long_parquet, index=False)\n",
    "            print(f\"[OK] Saved long parquet: {out_long_parquet} rows={len(long_df):,}\")\n",
    "\n",
    "        # Lightweight preview\n",
    "        df_preview = pd.read_parquet(out_long_parquet)\n",
    "        display(df_preview.head(5))\n",
    "        display(df_preview[[\"ndvi\", \"evi\"]].describe())\n",
    "\n",
    "    else:\n",
    "        # Baseline mode sanity check: read exactly one shard\n",
    "        shard_files = sorted(shards_dir.glob(\"modis_vi_long_*.parquet\"))\n",
    "        print(\"[INFO] assemble_final=False; skipping long parquet by design.\")\n",
    "        if shard_files:\n",
    "            example = shard_files[0]\n",
    "            print(\"  Example shard:\", example.name)\n",
    "            df_preview = pd.read_parquet(example)\n",
    "            display(df_preview.head(5))\n",
    "            display(df_preview[[\"ndvi\", \"evi\"]].describe())\n",
    "\n",
    "    return {\n",
    "        \"tag\": tag,\n",
    "        \"input_dir\": str(input_dir),\n",
    "        \"shards_dir\": str(shards_dir),\n",
    "        \"out_long_parquet\": (str(out_long_parquet) if assemble_final else None),\n",
    "        \"wrote_shards\": wrote,\n",
    "        \"skipped_shards\": skipped,\n",
    "        \"skip_reasons\": dict(skip_reasons),\n",
    "        \"n_groups\": len(groups),\n",
    "    }\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Two entrypoints (no manual flag flipping)\n",
    "# ----------------------------\n",
    "# delete files from the derived directory if we want to reprocess\n",
    "def run_modis_analysis():\n",
    "    out_dir = Path(CONFIG[\"out_dir\"])\n",
    "    # You can either use CONFIG[\"MODIS_ANALYSIS_DIR\"] (relative) or hard-wire manual/modis_ndvi_analysis.\n",
    "    # This follows your established convention: out_dir + relative dir from config if present.\n",
    "    rel = CONFIG.get(\"MODIS_ANALYSIS_DIR\", \"manual/modis_ndvi_analysis\")\n",
    "    input_dir = out_dir / rel\n",
    "    return run_modis_cellA(\n",
    "        tag=\"analysis\",\n",
    "        input_dir=input_dir,\n",
    "        out_dir=out_dir,\n",
    "        assemble_final=True,\n",
    "        max_files=None,\n",
    "    )\n",
    "\n",
    "# delete files from the derived directory if we want to reprocess\n",
    "def run_modis_baseline():\n",
    "    out_dir = Path(CONFIG[\"out_dir\"])\n",
    "    rel = CONFIG.get(\"MODIS_BASELINE_DIR\", \"manual/modis_ndvi_baseline\")\n",
    "    input_dir = out_dir / rel\n",
    "    return run_modis_cellA(\n",
    "        tag=\"baseline\",\n",
    "        input_dir=input_dir,\n",
    "        out_dir=out_dir,\n",
    "        assemble_final=False,   # baseline is shards-only to avoid accidental huge concatenation\n",
    "        max_files=None, # set to a number if we want to test this out\n",
    "    )\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Run (choose which to invoke)\n",
    "# ----------------------------\n",
    "# Run analysis (small; assembles final)\n",
    "diag = run_modis_analysis()\n",
    "\n",
    "# Run baseline (large; shards-only)\n",
    "diag = run_modis_baseline()\n",
    "\n",
    "print(\"\\nMODIS Cell A diagnostics summary:\")\n",
    "print(diag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ac19ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === HydroPulse | Final Daily Grid Builder (v0: GHCND PRCP only) ===\n",
    "# Produces: CONFIG[\"FINAL_DAILY_FILENAME\"] as a canonical daily grid table:\n",
    "#   grid_id × date → prcp_mm + QC\n",
    "#\n",
    "# Inputs:\n",
    "#   - Grid parquet (EPSG:3310): CONFIG[\"GRID_FILENAME\"]\n",
    "#   - GHCND cleaned station-day parquet: resolve_out_path(CONFIG[\"GHCND_CLEAN_PARQUET_NAME\"])\n",
    "#\n",
    "# Notes for later HeatShield refactor:\n",
    "#   - This cell establishes the common \"final builder\" contract: {grid_id, date, variables..., QC...}\n",
    "#   - Keep the interface stable; only swap/extend source adapters per repo.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from pathlib import Path\n",
    "from sklearn.neighbors import BallTree\n",
    "\n",
    "# -----------------------------\n",
    "# Paths\n",
    "# -----------------------------\n",
    "OUT_DIR = Path(CONFIG[\"out_dir\"])\n",
    "GRID_PATH = Path(resolve_out_path(CONFIG[\"GRID_FILENAME\"]))\n",
    "GHCND_PATH = Path(resolve_out_path(CONFIG[\"GHCND_CLEAN_PARQUET_NAME\"]))\n",
    "FINAL_PATH = Path(resolve_out_path(CONFIG[\"FINAL_DAILY_FILENAME\"]))\n",
    "\n",
    "# Resume via daily shards\n",
    "SHARDS_DIR = OUT_DIR / \"derived\" / \"final_daily_shards_prcp\"\n",
    "SHARDS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"GRID_PATH:\", GRID_PATH)\n",
    "print(\"GHCND_PATH:\", GHCND_PATH)\n",
    "print(\"SHARDS_DIR:\", SHARDS_DIR)\n",
    "print(\"FINAL_PATH:\", FINAL_PATH)\n",
    "\n",
    "# -----------------------------\n",
    "# Column contract (confirmed)\n",
    "# -----------------------------\n",
    "STATION_COL = \"station_core\"\n",
    "DATE_COL    = \"date\"\n",
    "LAT_COL     = \"lat\"\n",
    "LON_COL     = \"lon\"\n",
    "PRCP_COL_IN = \"precipitation_mm\"\n",
    "\n",
    "# Output column naming for the final table\n",
    "PRCP_COL_OUT = \"prcp_mm\"\n",
    "\n",
    "# -----------------------------\n",
    "# Tunables (can later move to config)\n",
    "# -----------------------------\n",
    "K = int(CONFIG.get(\"PRCP_IDW_K\", 8))                        # k nearest stations\n",
    "HARD_CAP_KM = float(CONFIG.get(\"PRCP_HARD_CAP_KM\", 100.0))  # ignore stations beyond this radius\n",
    "POWER = float(CONFIG.get(\"PRCP_IDW_POWER\", 2.0))            # IDW power\n",
    "\n",
    "OPS_EPSG = int(CONFIG.get(\"OPS_EPSG\", 3310))\n",
    "WGS84_EPSG = int(CONFIG.get(\"WGS84_EPSG\", 4326))\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers (keep these stable across repos)\n",
    "# -----------------------------\n",
    "def ensure_epsg(gdf: gpd.GeoDataFrame, epsg: int) -> gpd.GeoDataFrame:\n",
    "    if gdf.crs is None:\n",
    "        gdf = gdf.set_crs(f\"EPSG:{WGS84_EPSG}\")\n",
    "    if (gdf.crs.to_epsg() or 0) != epsg:\n",
    "        gdf = gdf.to_crs(epsg)\n",
    "    return gdf\n",
    "\n",
    "def ensure_grid_id(grid: gpd.GeoDataFrame) -> tuple[gpd.GeoDataFrame, str]:\n",
    "    for c in [\"grid_id\", \"cell_id\", \"id\"]:\n",
    "        if c in grid.columns:\n",
    "            return grid, c\n",
    "    grid = grid.copy()\n",
    "    grid[\"grid_id\"] = np.arange(len(grid), dtype=np.int32)\n",
    "    return grid, \"grid_id\"\n",
    "\n",
    "def build_balltree_from_points(geom: gpd.GeoSeries) -> BallTree:\n",
    "    xy = np.column_stack([geom.x.values, geom.y.values])\n",
    "    return BallTree(xy, metric=\"euclidean\")\n",
    "\n",
    "def idw(dist_m: np.ndarray, vals: np.ndarray, power: float) -> float:\n",
    "    if np.any(dist_m == 0):\n",
    "        return float(vals[np.argmin(dist_m)])\n",
    "    w = 1.0 / np.power(dist_m, power)\n",
    "    return float(np.sum(w * vals) / np.sum(w))\n",
    "\n",
    "def interpolate_prcp_for_day(grid_centroids: gpd.GeoSeries, stations_day: gpd.GeoDataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns a DF aligned to grid_centroids order with columns:\n",
    "      prcp_mm, n_used, maxdist_km, method\n",
    "    \"\"\"\n",
    "    n_cells = len(grid_centroids)\n",
    "    out = pd.DataFrame({\n",
    "        PRCP_COL_OUT: np.full(n_cells, np.nan, dtype=float),\n",
    "        \"n_used\": np.zeros(n_cells, dtype=np.int16),\n",
    "        \"maxdist_km\": np.full(n_cells, np.nan, dtype=float),\n",
    "        \"method\": np.full(n_cells, None, dtype=object),\n",
    "    })\n",
    "\n",
    "    if stations_day is None or len(stations_day) == 0:\n",
    "        return out\n",
    "\n",
    "    # Build tree on station points\n",
    "    tree = build_balltree_from_points(stations_day.geometry)\n",
    "    vals = stations_day[PRCP_COL_IN].to_numpy(dtype=float)\n",
    "\n",
    "    qxy = np.column_stack([grid_centroids.x.values, grid_centroids.y.values])\n",
    "    k_eff = min(K, len(stations_day))\n",
    "    dist_m, idx = tree.query(qxy, k=k_eff)\n",
    "\n",
    "    hard_cap_m = HARD_CAP_KM * 1000.0\n",
    "\n",
    "    for i in range(n_cells):\n",
    "        d = dist_m[i]\n",
    "        j = idx[i]\n",
    "\n",
    "        # Apply hard cap\n",
    "        mask = d <= hard_cap_m\n",
    "        if not np.any(mask):\n",
    "            continue\n",
    "\n",
    "        d_use = d[mask]\n",
    "        v_use = vals[j[mask]]\n",
    "\n",
    "        # Drop NaNs (defensive)\n",
    "        good = np.isfinite(v_use)\n",
    "        d_use = d_use[good]\n",
    "        v_use = v_use[good]\n",
    "        if len(v_use) == 0:\n",
    "            continue\n",
    "\n",
    "        if len(v_use) == 1:\n",
    "            out.at[i, PRCP_COL_OUT] = float(v_use[0])\n",
    "            out.at[i, \"method\"] = \"nearest\"\n",
    "        else:\n",
    "            out.at[i, PRCP_COL_OUT] = idw(d_use, v_use, power=POWER)\n",
    "            out.at[i, \"method\"] = f\"idw_k{len(v_use)}\"\n",
    "\n",
    "        out.at[i, \"n_used\"] = int(len(v_use))\n",
    "        out.at[i, \"maxdist_km\"] = float(np.max(d_use) / 1000.0)\n",
    "\n",
    "    return out\n",
    "\n",
    "# -----------------------------\n",
    "# Load grid (EPSG:3310) and compute centroids\n",
    "# -----------------------------\n",
    "grid = gpd.read_parquet(GRID_PATH)\n",
    "grid = ensure_epsg(grid, OPS_EPSG)\n",
    "grid, GID_COL = ensure_grid_id(grid)\n",
    "centroids = grid.geometry.centroid\n",
    "\n",
    "print(f\"Grid: {len(grid)} cells | CRS EPSG: {grid.crs.to_epsg()} | id col: {GID_COL}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Load cleaned GHCND station-day table\n",
    "# -----------------------------\n",
    "cdo = pd.read_parquet(GHCND_PATH)\n",
    "\n",
    "required = [STATION_COL, DATE_COL, LAT_COL, LON_COL, PRCP_COL_IN]\n",
    "missing = [c for c in required if c not in cdo.columns]\n",
    "if missing:\n",
    "    raise KeyError(f\"Missing required columns in {GHCND_PATH.name}: {missing}. Have: {list(cdo.columns)}\")\n",
    "\n",
    "# Normalize date to UTC day\n",
    "cdo[DATE_COL] = pd.to_datetime(cdo[DATE_COL], utc=True, errors=\"coerce\").dt.normalize()\n",
    "\n",
    "# Filter date window\n",
    "start = pd.to_datetime(CONFIG[\"start_date\"], utc=True).normalize()\n",
    "end = pd.to_datetime(CONFIG[\"end_date\"], utc=True).normalize()\n",
    "cdo = cdo[(cdo[DATE_COL] >= start) & (cdo[DATE_COL] <= end)].copy()\n",
    "\n",
    "# Drop invalid coords / missing precip\n",
    "cdo = cdo[np.isfinite(cdo[LAT_COL]) & np.isfinite(cdo[LON_COL])].copy()\n",
    "cdo = cdo[np.isfinite(cdo[PRCP_COL_IN])].copy()\n",
    "\n",
    "print(\"Station-day rows:\", len(cdo), \"| stations:\", cdo[STATION_COL].nunique(), \"| dates:\", cdo[DATE_COL].nunique())\n",
    "\n",
    "# GeoDataFrame in OPS_EPSG\n",
    "pts = gpd.GeoDataFrame(\n",
    "    cdo,\n",
    "    geometry=gpd.points_from_xy(cdo[LON_COL], cdo[LAT_COL]),\n",
    "    crs=f\"EPSG:{WGS84_EPSG}\"\n",
    ")\n",
    "pts = ensure_epsg(pts, OPS_EPSG)\n",
    "\n",
    "# Pre-group by date for speed (avoid repeated boolean filters)\n",
    "pts_by_date = {d: df for d, df in pts.groupby(DATE_COL)}\n",
    "all_dates = pd.date_range(start=start, end=end, freq=\"D\")\n",
    "\n",
    "# -----------------------------\n",
    "# Daily loop with resume\n",
    "# -----------------------------\n",
    "written = 0\n",
    "skipped = 0\n",
    "\n",
    "for d in all_dates:\n",
    "    tag = d.strftime(\"%Y%m%d\")\n",
    "    shard_path = SHARDS_DIR / f\"final_prcp_{tag}.parquet\"\n",
    "    if shard_path.exists():\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "    day_pts = pts_by_date.get(d)\n",
    "    interp = interpolate_prcp_for_day(centroids, day_pts)\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        GID_COL: grid[GID_COL].values,\n",
    "        \"date\": np.full(len(grid), d),\n",
    "    })\n",
    "    out = pd.concat([out, interp], axis=1)\n",
    "\n",
    "    out.to_parquet(shard_path, index=False)\n",
    "    written += 1\n",
    "\n",
    "print(f\"Shards written: {written} | skipped (resume): {skipped} | total days: {len(all_dates)}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Compose FINAL parquet\n",
    "# -----------------------------\n",
    "shards = sorted(SHARDS_DIR.glob(\"final_prcp_*.parquet\"))\n",
    "if not shards:\n",
    "    raise FileNotFoundError(f\"No shards found in {SHARDS_DIR}\")\n",
    "\n",
    "df_final = pd.concat((pd.read_parquet(p) for p in shards), ignore_index=True)\n",
    "\n",
    "df_final[\"date\"] = pd.to_datetime(df_final[\"date\"], utc=True).dt.normalize()\n",
    "df_final = df_final.sort_values([GID_COL, \"date\"]).reset_index(drop=True)\n",
    "\n",
    "FINAL_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "df_final.to_parquet(FINAL_PATH, index=False)\n",
    "\n",
    "print(\"Saved FINAL:\", FINAL_PATH)\n",
    "print(\"Final rows:\", len(df_final), \"| cells:\", df_final[GID_COL].nunique(), \"| dates:\", df_final[\"date\"].nunique())\n",
    "print(df_final.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blueleaflabs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
