{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72d0c1d1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Imports ---\n",
        "import os, json, math, io, zipfile, time, re, shutil, glob, pathlib, sys, subprocess, shlex, tempfile, warnings\n",
        "from pathlib import Path\n",
        "from datetime import datetime as dt, timedelta, timezone, date, datetime\n",
        "from dateutil import parser as dateparser\n",
        "from dateutil.relativedelta import relativedelta\n",
        "from urllib.parse import urljoin, quote\n",
        "from functools import reduce\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from tqdm import tqdm\n",
        "import pytz\n",
        "import xarray as xr\n",
        "import rasterio\n",
        "from rasterio.mask import mask\n",
        "\n",
        "import shapely\n",
        "from shapely import ops\n",
        "from shapely.geometry import Point, Polygon, box, mapping\n",
        "from shapely.ops import unary_union, transform as shp_transform\n",
        "\n",
        "from pyproj import Transformer\n",
        "import geopandas as gpd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Rectangle\n",
        "import folium\n",
        "import ee  # Earth Engine\n",
        "\n",
        "from sklearn.neighbors import BallTree\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.spatial.distance import cdist\n",
        "from geopy.distance import geodesic\n",
        "\n",
        "import yaml\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "\n",
        "def skip_if_exists(path: str) -> bool:\n",
        "    return os.path.exists(path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6f568ab",
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Configuration loader (shared HeatShield/HydroPulse) ---\n",
        "from pathlib import Path\n",
        "import os\n",
        "import re\n",
        "import yaml\n",
        "\n",
        "def load_env_file(path: Path) -> dict:\n",
        "    env = {}\n",
        "    if not path.exists():\n",
        "        return env\n",
        "    for line in path.read_text().splitlines():\n",
        "        line = line.strip()\n",
        "        if not line or line.startswith(\"#\") or \"=\" not in line:\n",
        "            continue\n",
        "        key, val = line.split(\"=\", 1)\n",
        "        env[key.strip()] = val.strip().strip('\"').strip(\"'\")\n",
        "    return env\n",
        "\n",
        "def apply_env_overrides() -> None:\n",
        "    env = load_env_file(Path(\".env\"))\n",
        "    for k, v in env.items():\n",
        "        if v:\n",
        "            os.environ[k] = v\n",
        "\n",
        "def _fmt_yyyymmdd(s: str) -> str:\n",
        "    # expects YYYY-MM-DD\n",
        "    return re.sub(r\"-\", \"\", s.strip())\n",
        "\n",
        "def _render_template(tpl: str, *, start: str, end: str, res_m: int, epsg: int, region: str) -> str:\n",
        "    return tpl.format(\n",
        "        start=_fmt_yyyymmdd(start),\n",
        "        end=_fmt_yyyymmdd(end),\n",
        "        res_m=int(res_m),\n",
        "        epsg=int(epsg),\n",
        "        region=str(region),\n",
        "    )\n",
        "\n",
        "def _resolve_out_dir(project_dir: Path, out_dir_value: str | None) -> Path:\n",
        "    out_dir_value = out_dir_value or \"results\"\n",
        "    p = Path(out_dir_value)\n",
        "    return (p if p.is_absolute() else (project_dir / p)).resolve()\n",
        "\n",
        "def _resolve_pathlike_keys(cfg: dict, out_dir: Path) -> None:\n",
        "    \"\"\"\n",
        "    Resolve config values that look like relative file paths under out_dir.\n",
        "    Rule: if key ends with _FILENAME, _PATH, _ZIP, _TIF_NAME, _CSV_NAME, _PARQUET_NAME, _TXT_NAME\n",
        "    and the value is a relative path, make it absolute under out_dir.\n",
        "    \"\"\"\n",
        "    suffixes = (\n",
        "        \"_FILENAME\", \"_PATH\", \"_ZIP\",\n",
        "        \"_TIF_NAME\", \"_CSV_NAME\", \"_PARQUET_NAME\", \"_TXT_NAME\",\n",
        "        \"_NETCDF_NAME\", \"_ZARR_NAME\"\n",
        "    )\n",
        "    for k, v in list(cfg.items()):\n",
        "        if not isinstance(v, str):\n",
        "            continue\n",
        "        if not k.endswith(suffixes):\n",
        "            continue\n",
        "        p = Path(v)\n",
        "        if p.is_absolute():\n",
        "            cfg[k] = str(p)\n",
        "        else:\n",
        "            cfg[k] = str((out_dir / p).resolve())\n",
        "\n",
        "# Fail fast unless PROJECT_DIR is explicitly provided.\n",
        "apply_env_overrides()\n",
        "\n",
        "PROJECT_DIR = os.environ.get(\"PROJECT_DIR\")\n",
        "if not PROJECT_DIR:\n",
        "    raise FileNotFoundError(\n",
        "        \"PROJECT_DIR is not set. Add PROJECT_DIR to .env or environment variables.\"\n",
        "    )\n",
        "\n",
        "PROJECT_DIR = Path(PROJECT_DIR).expanduser().resolve()\n",
        "CONFIG_PATH = PROJECT_DIR / \"config\" / \"config.yaml\"\n",
        "if not CONFIG_PATH.exists():\n",
        "    raise FileNotFoundError(f\"Missing config file: {CONFIG_PATH}\")\n",
        "\n",
        "CONFIG = yaml.safe_load(CONFIG_PATH.read_text()) or {}\n",
        "\n",
        "# Set out_dir to an absolute path early.\n",
        "OUT_DIR = _resolve_out_dir(PROJECT_DIR, CONFIG.get(\"out_dir\", \"results\"))\n",
        "CONFIG[\"out_dir\"] = str(OUT_DIR)\n",
        "\n",
        "# Optional: Apply env overrides ONLY for keys that exist in config.yaml.\n",
        "# This prevents HeatShield-specific lists in shared code.\n",
        "for key in list(CONFIG.keys()):\n",
        "    env_val = os.environ.get(key)\n",
        "    if env_val:\n",
        "        CONFIG[key] = env_val\n",
        "\n",
        "# Also allow a small, explicit allowlist for common optional overrides across projects.\n",
        "for key in [\"PURPLEAIR_SENSOR_INDEX\"]:  # harmless if absent in HydroPulse config\n",
        "    env_val = os.environ.get(key)\n",
        "    if env_val:\n",
        "        CONFIG[key] = env_val\n",
        "\n",
        "# Render FINAL_DAILY_FILENAME from template if provided.\n",
        "# Keeps downstream code stable: always refer to CONFIG[\"FINAL_DAILY_FILENAME\"].\n",
        "if \"FINAL_DAILY_FILENAME_TEMPLATE\" in CONFIG:\n",
        "    region = CONFIG.get(\"region\", \"CA\")\n",
        "    start_date = CONFIG[\"start_date\"]\n",
        "    end_date = CONFIG[\"end_date\"]\n",
        "    res_m = CONFIG.get(\"grid_resolution_m\", 3000)\n",
        "    epsg = CONFIG.get(\"OPS_EPSG\", CONFIG.get(\"CA_ALBERS_EPSG\", 3310))\n",
        "    CONFIG[\"FINAL_DAILY_FILENAME\"] = _render_template(\n",
        "        CONFIG[\"FINAL_DAILY_FILENAME_TEMPLATE\"],\n",
        "        start=start_date,\n",
        "        end=end_date,\n",
        "        res_m=res_m,\n",
        "        epsg=epsg,\n",
        "        region=region,\n",
        "    )\n",
        "\n",
        "# Resolve pathlike keys under out_dir (only for keys that exist).\n",
        "_resolve_pathlike_keys(CONFIG, OUT_DIR)\n",
        "\n",
        "# Create common directories only if they are referenced in config.\n",
        "# (Avoid hardcoding \"manual\" for HydroPulse.)\n",
        "for k, v in CONFIG.items():\n",
        "    if k.endswith(\"_DIRNAME\") and isinstance(v, str):\n",
        "        os.makedirs(Path(CONFIG[\"out_dir\"]) / v, exist_ok=True)\n",
        "\n",
        "# EPSG constants (configurable)\n",
        "WGS84_EPSG = int(CONFIG.get(\"WGS84_EPSG\", 4326))\n",
        "CA_ALBERS_EPSG = int(CONFIG.get(\"CA_ALBERS_EPSG\", 3310))\n",
        "OPS_EPSG = int(CONFIG.get(\"OPS_EPSG\", CA_ALBERS_EPSG))\n",
        "\n",
        "# Set working directory\n",
        "os.chdir(PROJECT_DIR)\n",
        "\n",
        "print(f\"Config loaded from {CONFIG_PATH}\")\n",
        "print(f\"Output dir: {CONFIG['out_dir']}\")\n",
        "print(f\"Final daily filename: {CONFIG.get('FINAL_DAILY_FILENAME')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19be8061",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "def resolve_out_path(path_str: str) -> str:\n",
        "    \"\"\"\n",
        "    Resolve a path string relative to CONFIG['out_dir'] unless already absolute.\n",
        "    Returns an absolute string path.\n",
        "    \"\"\"\n",
        "    p = Path(path_str)\n",
        "    if p.is_absolute():\n",
        "        return str(p)\n",
        "    return str((Path(CONFIG[\"out_dir\"]) / p).resolve())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ea9e2b8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Ensure California boundary and build 3 km grid clipped to land ---\n",
        "\n",
        "\n",
        "# Config\n",
        "res_m = int(CONFIG.get(\"grid_resolution_m\", 3000))\n",
        "out_epsg = int(CONFIG.get(\"crs_epsg\", 4326))\n",
        "out_dir = CONFIG[\"out_dir\"]; os.makedirs(out_dir, exist_ok=True)\n",
        "inset_buffer_m = int(CONFIG.get(\"coast_inset_m\", 0))  # e.g. 5000\n",
        "boundary_path = CONFIG.get(\"ca_boundary_path\", None)\n",
        "\n",
        "# 1) Ensure boundary: download Census cartographic boundary if missing\n",
        "if not boundary_path or not os.path.exists(boundary_path):\n",
        "    states_zip = os.path.join(out_dir, \"cb_2023_us_state_20m.zip\")\n",
        "    if not os.path.exists(states_zip):\n",
        "        url = CONFIG[\"CENSUS_STATES_ZIP_URL\"]\n",
        "        r = requests.get(url, timeout=int(CONFIG.get(\"CENSUS_STATES_TIMEOUT\", 120))); r.raise_for_status()\n",
        "        with open(states_zip, \"wb\") as f: f.write(r.content)\n",
        "    # Read from zip directly and select California\n",
        "    states = gpd.read_file(f\"zip://{states_zip}\")\n",
        "    if states.empty:\n",
        "        raise ValueError(\"Census states file loaded empty.\")\n",
        "    ca = states[states[\"STATEFP\"].astype(str).str.zfill(2).eq(\"06\")][[\"geometry\"]]\n",
        "    if ca.empty:\n",
        "        raise ValueError(\"California polygon not found in Census states file.\")\n",
        "    boundary_path = os.path.join(out_dir, \"california_boundary.gpkg\")\n",
        "    ca.to_file(boundary_path, driver=\"GPKG\")\n",
        "    CONFIG[\"ca_boundary_path\"] = boundary_path  # persist for later cells\n",
        "\n",
        "# 2) Load boundary, dissolve, project, optional inward buffer\n",
        "b = gpd.read_file(boundary_path)\n",
        "if b.crs is None: raise ValueError(\"Boundary file has no CRS.\")\n",
        "b = b[[\"geometry\"]].copy()\n",
        "b = b.to_crs(CA_ALBERS_EPSG)\n",
        "b = gpd.GeoDataFrame(geometry=[b.unary_union], crs=f\"EPSG:{CA_ALBERS_EPSG}\")\n",
        "if inset_buffer_m > 0:\n",
        "    b.geometry = b.buffer(-inset_buffer_m)\n",
        "    b = gpd.GeoDataFrame(geometry=[b.unary_union], crs=f\"EPSG:{CA_ALBERS_EPSG}\")\n",
        "\n",
        "# 3) Build snapped rectilinear grid over boundary bounds in EPSG:3310\n",
        "minx, miny, maxx, maxy = b.total_bounds\n",
        "snap_down = lambda v, s: np.floor(v/s)*s\n",
        "snap_up   = lambda v, s: np.ceil(v/s)*s\n",
        "minx, miny = snap_down(minx, res_m), snap_down(miny, res_m)\n",
        "maxx, maxy = snap_up(maxx, res_m), snap_up(maxy, res_m)\n",
        "\n",
        "xs = np.arange(minx, maxx, res_m)\n",
        "ys = np.arange(miny, maxy, res_m)\n",
        "n_rect = len(xs)*len(ys)\n",
        "if n_rect > 3_500_000:\n",
        "    raise MemoryError(f\"Grid too large ({n_rect:,}). Increase res_m or tile the state.\")\n",
        "\n",
        "cells, col_i, row_j = [], [], []\n",
        "for j, y in enumerate(ys):\n",
        "    for i, x in enumerate(xs):\n",
        "        cells.append(box(x, y, x+res_m, y+res_m)); col_i.append(i); row_j.append(j)\n",
        "\n",
        "gdf_proj = gpd.GeoDataFrame({\"col_i\": np.int32(col_i), \"row_j\": np.int32(row_j)},\n",
        "                            geometry=cells, crs=f\"EPSG:{CA_ALBERS_EPSG}\")\n",
        "gdf_proj[\"cell_area_m2\"] = float(res_m)*float(res_m)\n",
        "gdf_proj[\"grid_id\"] = f\"CA3310_{res_m}_\" + gdf_proj[\"col_i\"].astype(str) + \"_\" + gdf_proj[\"row_j\"].astype(str)\n",
        "\n",
        "# 4) Strict land clip and land fraction\n",
        "gdf_proj = gpd.sjoin(gdf_proj, b, how=\"inner\", predicate=\"intersects\").drop(columns=[\"index_right\"])\n",
        "inter = gpd.overlay(gdf_proj[[\"grid_id\",\"geometry\"]], b, how=\"intersection\", keep_geom_type=True)\n",
        "inter[\"land_area_m2\"] = inter.geometry.area\n",
        "land = inter[[\"grid_id\",\"land_area_m2\"]].groupby(\"grid_id\", as_index=False).sum()\n",
        "gdf_proj = gdf_proj.merge(land, on=\"grid_id\", how=\"left\")\n",
        "gdf_proj[\"land_area_m2\"] = gdf_proj[\"land_area_m2\"].fillna(0.0)\n",
        "gdf_proj[\"land_frac\"] = (gdf_proj[\"land_area_m2\"] / gdf_proj[\"cell_area_m2\"]).clip(0,1)\n",
        "gdf_proj = gdf_proj[gdf_proj[\"land_frac\"] > 0].reset_index(drop=True)\n",
        "\n",
        "# 5) Reproject to requested output CRS and save\n",
        "grid_gdf = gdf_proj.to_crs(out_epsg)\n",
        "\n",
        "parquet_path = os.path.join(out_dir, f\"grid_{res_m}m_CA.parquet\")\n",
        "grid_gdf.to_parquet(parquet_path, index=False)\n",
        "\n",
        "geojson_path = os.path.join(out_dir, f\"grid_{res_m}m_CA_head10.geojson\")\n",
        "grid_gdf.head(10).to_file(geojson_path, driver=\"GeoJSON\")\n",
        "\n",
        "# Diagnostics\n",
        "cell_area_km2 = (res_m/1000.0)**2\n",
        "eff_land_km2 = float((grid_gdf.get(\"land_frac\",1.0) * cell_area_km2).sum())\n",
        "print(f\"Saved: {parquet_path}\")\n",
        "print(f\"Cells: {len(grid_gdf):,}\")\n",
        "print(f\"Effective land area ≈ {round(eff_land_km2):,} km²\")\n",
        "print(f\"Implied cell size ≈ {round((eff_land_km2/len(grid_gdf))**0.5,2)} km\")\n",
        "\n",
        "grid_gdf.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "026fd848",
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Persist config + save grid (3310 ops copy, 4326 preview) + write metadata ---\n",
        "\n",
        "# Inputs assumed from prior cell:\n",
        "# - grid_gdf            : current grid GeoDataFrame (any CRS)\n",
        "# - CONFIG              : dict with out_dir, grid_resolution_m, crs_epsg, ca_boundary_path\n",
        "# - CA_ALBERS_EPSG=3310 : defined earlier\n",
        "\n",
        "out_dir = CONFIG[\"out_dir\"]; os.makedirs(out_dir, exist_ok=True)\n",
        "res_m = int(CONFIG.get(\"grid_resolution_m\", 3000))\n",
        "out_epsg = int(CONFIG.get(\"crs_epsg\", 4326))\n",
        "boundary_path = CONFIG.get(\"ca_boundary_path\")\n",
        "\n",
        "# 1) Persist boundary path back to CONFIG \n",
        "if not boundary_path or not os.path.exists(boundary_path):\n",
        "    raise FileNotFoundError(\"CONFIG['ca_boundary_path'] missing or invalid. Rebuild boundary.\")\n",
        "CONFIG[\"ca_boundary_path\"] = boundary_path\n",
        "\n",
        "config_runtime_path = os.path.join(out_dir, \"config_runtime.json\")\n",
        "with open(config_runtime_path, \"w\") as f:\n",
        "    json.dump(CONFIG, f, indent=2)\n",
        "print(\"Saved:\", config_runtime_path)\n",
        "\n",
        "# 2) Ensure we have an EPSG:3310 version for spatial ops\n",
        "if grid_gdf.crs is None:\n",
        "    raise ValueError(\"grid_gdf has no CRS. Rebuild grid.\")\n",
        "grid_3310 = grid_gdf.to_crs(3310) if grid_gdf.crs.to_epsg() != 3310 else grid_gdf\n",
        "\n",
        "# 3) Save operational GeoParquet in 3310 + lightweight WGS84 preview\n",
        "parquet_3310 = os.path.join(out_dir, f\"grid_{res_m}m_CA_epsg3310.parquet\")\n",
        "grid_3310.to_parquet(parquet_3310, index=False)\n",
        "print(\"Saved:\", parquet_3310, \"| cells:\", len(grid_3310))\n",
        "\n",
        "# Optional small preview in 4326 for quick map checks\n",
        "preview_4326 = grid_3310.to_crs(4326).head(500)  # cap to avoid huge files\n",
        "geojson_preview = os.path.join(out_dir, f\"grid_{res_m}m_CA_head500_epsg4326.geojson\")\n",
        "preview_4326.to_file(geojson_preview, driver=\"GeoJSON\")\n",
        "print(\"Saved:\", geojson_preview)\n",
        "\n",
        "# 4) Compute and save metadata\n",
        "cell_area_km2 = (res_m/1000.0)**2\n",
        "effective_land_km2 = float((grid_3310.get(\"land_frac\", 1.0) * cell_area_km2).sum())\n",
        "implied_cell_km = float((effective_land_km2 / len(grid_3310))**0.5)\n",
        "minx, miny, maxx, maxy = grid_3310.total_bounds\n",
        "bbox_km = ((maxx-minx)/1000.0, (maxy-miny)/1000.0)\n",
        "\n",
        "meta = {\n",
        "    \"timestamp_utc\": dt.utcnow().isoformat(timespec=\"seconds\") + \"Z\",\n",
        "    \"grid_resolution_m\": res_m,\n",
        "    \"crs_ops_epsg\": 3310,\n",
        "    \"crs_export_default_epsg\": out_epsg,\n",
        "    \"cells\": int(len(grid_3310)),\n",
        "    \"effective_land_area_km2\": round(effective_land_km2, 2),\n",
        "    \"implied_cell_km\": round(implied_cell_km, 4),\n",
        "    \"bbox_km_width_height\": [round(bbox_km[0], 2), round(bbox_km[1], 2)],\n",
        "    \"has_land_frac\": bool(\"land_frac\" in grid_3310.columns),\n",
        "    \"boundary_path\": boundary_path,\n",
        "    \"parquet_3310_path\": parquet_3310,\n",
        "    \"geojson_preview_4326_path\": geojson_preview,\n",
        "}\n",
        "\n",
        "meta_path = os.path.join(out_dir, f\"grid_{res_m}m_CA_meta.json\")\n",
        "with open(meta_path, \"w\") as f:\n",
        "    json.dump(meta, f, indent=2)\n",
        "print(\"Saved:\", meta_path)\n",
        "meta\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29ff5b92",
      "metadata": {},
      "outputs": [],
      "source": [
        "# CDO data fetch and processing functions\n",
        "# repeat some variables for clarity\n",
        "OUT_DIR = CONFIG[\"out_dir\"]\n",
        "RAW_DIR = os.path.join(OUT_DIR, CONFIG[\"CDO_RAW_DIRNAME\"])\n",
        "CLEAN_DIR = os.path.join(OUT_DIR, CONFIG[\"CDO_CLEAN_DIRNAME\"])\n",
        "os.makedirs(RAW_DIR, exist_ok=True); os.makedirs(CLEAN_DIR, exist_ok=True)\n",
        "\n",
        "def month_windows(start_date, end_date):\n",
        "    s = dt.fromisoformat(start_date).date().replace(day=1)\n",
        "    e = dt.fromisoformat(end_date).date()\n",
        "    cur = s\n",
        "    while cur <= e:\n",
        "        nxt = (cur + relativedelta(months=1)) - relativedelta(days=1)\n",
        "        yield cur.isoformat(), min(nxt, e).isoformat()\n",
        "        cur = (cur + relativedelta(months=1)).replace(day=1)\n",
        "\n",
        "def parse_attributes(attr):\n",
        "    parts = (attr or \"\").split(\",\"); parts += [\"\"] * (4 - len(parts))\n",
        "    mflag, qflag, sflag, obs_hhmm = parts[:4]\n",
        "    return mflag or None, qflag or None, sflag or None, obs_hhmm or None\n",
        "\n",
        "def fetch_cdo_page(session, url, headers, params, max_retries=None, base_delay=None, timeout=None):\n",
        "    if max_retries is None:\n",
        "        max_retries = int(CONFIG.get(\"CDO_MAX_RETRIES\", 6))\n",
        "    if base_delay is None:\n",
        "        base_delay = float(CONFIG.get(\"CDO_BACKOFF_BASE\", 0.8))\n",
        "    if timeout is None:\n",
        "        timeout = int(CONFIG.get(\"CDO_TIMEOUT\", 180))\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            r = session.get(url, headers=headers, params=params, timeout=timeout)\n",
        "            if r.status_code in (429, 500, 502, 503, 504):\n",
        "                raise requests.HTTPError(f\"{r.status_code} retry\")\n",
        "            r.raise_for_status()\n",
        "            return r.json()\n",
        "        except Exception:\n",
        "            if attempt == max_retries - 1:\n",
        "                raise\n",
        "            time.sleep(base_delay * (2 ** attempt))\n",
        "\n",
        "\n",
        "def cdo_stream_monthly(datasetid, locationid, startdate, enddate, datatypes, token,\n",
        "                       units=\"standard\", page_limit=1000, force=False):\n",
        "    url = CONFIG[\"CDO_BASE_URL\"]\n",
        "    headers = {\"token\": token}\n",
        "    session = requests.Session()\n",
        "    written = []\n",
        "\n",
        "    for dtid in datatypes:\n",
        "        for ms, me in month_windows(startdate, enddate):\n",
        "            out_csv = os.path.join(RAW_DIR, f\"ghcnd_{dtid}_{ms[:7]}.csv\")\n",
        "            if skip_if_exists(out_csv) and not force:\n",
        "                # resume: skip existing month-datatype file\n",
        "                written.append(out_csv); continue\n",
        "\n",
        "            frames = []\n",
        "            offset = 1\n",
        "            while True:\n",
        "                params = {\n",
        "                    \"datasetid\": datasetid, \"locationid\": locationid,\n",
        "                    \"startdate\": ms, \"enddate\": me,\n",
        "                    \"datatypeid\": dtid, \"units\": units,\n",
        "                    \"limit\": page_limit, \"offset\": offset\n",
        "                }\n",
        "                js = fetch_cdo_page(session, url, headers, params)\n",
        "                rows = js.get(\"results\", [])\n",
        "                if not rows:\n",
        "                    break\n",
        "                frames.append(pd.json_normalize(rows))\n",
        "                if len(rows) < page_limit:\n",
        "                    break\n",
        "                offset += page_limit\n",
        "                time.sleep(0.15)  # gentle pacing\n",
        "\n",
        "            if frames:\n",
        "                df = pd.concat(frames, ignore_index=True)\n",
        "                # normalize\n",
        "                df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\").dt.date\n",
        "                parsed = df[\"attributes\"].apply(parse_attributes)\n",
        "                df[[\"mflag\",\"qflag\",\"sflag\",\"obs_hhmm\"]] = pd.DataFrame(parsed.tolist(), index=df.index)\n",
        "                # scale tenths\n",
        "                scale = {\"PRCP\": 0.1, \"TMAX\": 0.1, \"TMIN\": 0.1}\n",
        "                df[\"datatype\"] = df[\"datatype\"].astype(str)\n",
        "                df[\"value\"] = pd.to_numeric(df[\"value\"], errors=\"coerce\")\n",
        "                df[\"value_scaled\"] = df.apply(lambda r: r[\"value\"] * scale.get(r[\"datatype\"], 1.0), axis=1)\n",
        "                # write monthly raw\n",
        "                df[[\"date\",\"datatype\",\"station\",\"attributes\",\"mflag\",\"qflag\",\"sflag\",\"obs_hhmm\",\"value\",\"value_scaled\"]].to_csv(out_csv, index=False)\n",
        "                written.append(out_csv)\n",
        "            else:\n",
        "                # create an empty file with header to mark completion\n",
        "                with open(out_csv, \"w\", newline=\"\") as f:\n",
        "                    w = csv.writer(f); w.writerow([\"date\",\"datatype\",\"station\",\"attributes\",\"mflag\",\"qflag\",\"sflag\",\"obs_hhmm\",\"value\",\"value_scaled\"])\n",
        "                written.append(out_csv)\n",
        "    return written\n",
        "\n",
        "def build_clean_wide():\n",
        "    # read all monthly raw files and assemble cleaned wide once\n",
        "    files = sorted([os.path.join(RAW_DIR, f) for f in os.listdir(RAW_DIR) if f.endswith(\".csv\")])\n",
        "    if not files:\n",
        "        return None\n",
        "    df = pd.concat((pd.read_csv(f, dtype={\"datatype\":str,\"station\":str}) for f in files), ignore_index=True)\n",
        "    # convert types back\n",
        "    df[\"date\"] = pd.to_datetime(df[\"date\"]).dt.date\n",
        "    # keep good qflag\n",
        "    df = df[(df[\"qflag\"].isna()) | (df[\"qflag\"]==\"\")]\n",
        "    wide = (\n",
        "        df.pivot_table(index=[\"station\",\"date\"], columns=\"datatype\", values=\"value_scaled\", aggfunc=\"mean\")\n",
        "          .reset_index()\n",
        "          .rename(columns={\"date\":\"obs_date\",\"PRCP\":\"precipitation_mm\",\"TMAX\":\"temperature_max_c\",\"TMIN\":\"temperature_min_c\"})\n",
        "          .sort_values([\"obs_date\",\"station\"])\n",
        "    )\n",
        "    # attach obs time from PRCP\n",
        "    prcp_times = df[df[\"datatype\"]==\"PRCP\"][[\"station\",\"date\",\"obs_hhmm\"]].drop_duplicates().rename(columns={\"date\":\"obs_date\"})\n",
        "    wide = wide.merge(prcp_times, on=[\"station\",\"obs_date\"], how=\"left\")\n",
        "    raw_all = os.path.join(OUT_DIR, \"ghcnd_daily_raw_all.csv\")\n",
        "    wide_all = os.path.join(OUT_DIR, \"ghcnd_daily_wide.csv\")\n",
        "    df.to_csv(raw_all, index=False)\n",
        "    wide.to_csv(wide_all, index=False)\n",
        "    return raw_all, wide_all, len(df), len(wide), wide[\"station\"].nunique(), wide[\"obs_date\"].nunique()\n",
        "\n",
        "# ---- Run statewide with resume capability ----\n",
        "token = os.environ.get(\"CDO_TOKEN\") or CONFIG.get(\"CDO_TOKEN\", \"\")\n",
        "if token and token != \"YOUR_NCEI_CDO_TOKEN\":\n",
        "    written = cdo_stream_monthly(\n",
        "        datasetid=\"GHCND\",\n",
        "        locationid=\"FIPS:06\",                      # California statewide\n",
        "        startdate=CONFIG[\"start_date\"],\n",
        "        enddate=CONFIG[\"end_date\"],\n",
        "        datatypes=[\"TMAX\",\"TMIN\",\"PRCP\"],\n",
        "        token=token,\n",
        "        units=\"standard\",\n",
        "        page_limit=1000,\n",
        "        force=False                                 # set True to re-download\n",
        "    )\n",
        "    print(f\"Monthly files written: {len(written)} → {RAW_DIR}\")\n",
        "\n",
        "    res = build_clean_wide()\n",
        "    if res:\n",
        "        raw_all, wide_all, n_raw, n_wide, n_stn, n_dates = res\n",
        "        print(f\"Saved raw:  {raw_all}\")\n",
        "        print(f\"Saved wide: {wide_all}\")\n",
        "        print(f\"Counts → raw: {n_raw} | wide: {n_wide} | stations: {n_stn} | dates: {n_dates}\")\n",
        "else:\n",
        "    print(\"Skipping CDO (missing CDO token).\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54dc4706",
      "metadata": {},
      "outputs": [],
      "source": [
        "# === GHCND DAILY: raw (long) -> cleaned (wide with lat/lon in bbox) ===\n",
        "# Input  (from your earlier step):  results/ghcnd_daily_raw_all.csv  (long form)\n",
        "# Output (used by superset):        results/ghcnd_daily_cleaned.parquet  (wide per station-day with lat/lon)\n",
        "\n",
        "# Need to do this because we aren't getting proper \"joins\" in our superset setup.\n",
        "\n",
        "\n",
        "BASE = CONFIG[\"out_dir\"]\n",
        "RAW = resolve_out_path(CONFIG[\"GHCND_RAW_CSV_NAME\"])\n",
        "OUT_PARQ = resolve_out_path(CONFIG[\"GHCND_CLEAN_PARQUET_NAME\"])\n",
        "OUT_CSV = resolve_out_path(CONFIG[\"GHCND_CLEAN_CSV_NAME\"])\n",
        "\n",
        "assert os.path.exists(RAW), f\"Missing raw GHCND file: {RAW}\"\n",
        "\n",
        "# 1) Ensure we have a station catalog with lat/lon\n",
        "#    Prefer a local copy if you already saved one; otherwise download NOAA's reference once.\n",
        "CAT_DIR = os.path.join(BASE, CONFIG[\"MANUAL_DIRNAME\"]); os.makedirs(CAT_DIR, exist_ok=True)\n",
        "CAT_TXT = os.path.join(CAT_DIR, CONFIG[\"GHCND_STATIONS_TXT_NAME\"])\n",
        "\n",
        "if not os.path.exists(CAT_TXT):\n",
        "    url = CONFIG[\"GHCND_STATIONS_URL\"]\n",
        "    r = requests.get(url, timeout=int(CONFIG.get(\"CENSUS_STATES_TIMEOUT\", 120))); r.raise_for_status()\n",
        "    with open(CAT_TXT, \"wb\") as f: f.write(r.content)\n",
        "\n",
        "# Parse ghcnd-stations.txt (fixed-width)\n",
        "# Columns per docs: ID(1-11), LAT(13-20), LON(22-30), ELEV(32-37), STATE(39-40), NAME(42-71) ...\n",
        "def parse_stations(path):\n",
        "    recs = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "        for line in f:\n",
        "            if len(line) < 40: \n",
        "                continue\n",
        "            sid = line[0:11].strip()\n",
        "            try:\n",
        "                lat = float(line[12:20].strip())\n",
        "                lon = float(line[21:30].strip())\n",
        "            except ValueError:\n",
        "                continue\n",
        "            state = line[38:40].strip()\n",
        "            name  = line[41:71].strip()\n",
        "            recs.append((sid, lat, lon, state, name))\n",
        "    return pd.DataFrame(recs, columns=[\"station_core\",\"lat\",\"lon\",\"state\",\"name\"])\n",
        "\n",
        "stations = parse_stations(CAT_TXT)\n",
        "\n",
        "# 2) Load your raw long-form CDO file\n",
        "# Expected columns seen in your sample:\n",
        "# ['attributes','datatype','date','mflag','obs_hhmm','qflag','sflag','station','value','value_scaled']\n",
        "raw = pd.read_csv(RAW, low_memory=False)\n",
        "\n",
        "# Normalize station key: raw uses \"GHCND:USW00023232\" → core \"USW00023232\"\n",
        "raw[\"station_core\"] = raw[\"station\"].astype(str).str.replace(\"^GHCND:\", \"\", regex=True)\n",
        "\n",
        "# Pick a numeric value column: prefer value_scaled if present; else scale GHCND native units.\n",
        "# GHCND native: PRCP = tenths of mm, TMAX/TMIN = tenths of °C.\n",
        "have_scaled = \"value_scaled\" in raw.columns\n",
        "def scaled_val(row):\n",
        "    if have_scaled and pd.notna(row[\"value_scaled\"]):\n",
        "        return float(row[\"value_scaled\"])\n",
        "    v = pd.to_numeric(row[\"value\"], errors=\"coerce\")\n",
        "    if pd.isna(v): \n",
        "        return np.nan\n",
        "    if row[\"datatype\"] == \"PRCP\":\n",
        "        return v * 0.1             # → mm\n",
        "    if row[\"datatype\"] in (\"TMAX\",\"TMIN\"):\n",
        "        return v * 0.1             # → °C\n",
        "    return v\n",
        "\n",
        "raw[\"val_clean\"] = raw.apply(scaled_val, axis=1)\n",
        "\n",
        "# Filter to the analysis window if your raw contains more than needed\n",
        "if \"start_date\" in CONFIG and \"end_date\" in CONFIG:\n",
        "    sd = pd.to_datetime(CONFIG[\"start_date\"], utc=True, errors=\"coerce\")\n",
        "    ed = pd.to_datetime(CONFIG[\"end_date\"],   utc=True, errors=\"coerce\")\n",
        "    raw[\"date\"] = pd.to_datetime(raw[\"date\"], utc=True, errors=\"coerce\")\n",
        "    raw = raw[(raw[\"date\"]>=sd) & (raw[\"date\"]<=ed)]\n",
        "else:\n",
        "    raw[\"date\"] = pd.to_datetime(raw[\"date\"], utc=True, errors=\"coerce\")\n",
        "\n",
        "# 3) Keep only the datatypes we need and one value per (station,date,datatype)\n",
        "keep_types = {\"PRCP\":\"precipitation_mm\", \"TMAX\":\"temperature_max_c\", \"TMIN\":\"temperature_min_c\"}\n",
        "raw = raw[raw[\"datatype\"].isin(keep_types.keys())].copy()\n",
        "\n",
        "# If multiple rows per (station,date,datatype), average them\n",
        "agg = (raw.groupby([\"station_core\",\"date\",\"datatype\"], as_index=False)[\"val_clean\"]\n",
        "          .mean())\n",
        "\n",
        "# 4) Pivot to wide columns\n",
        "wide = (agg.pivot(index=[\"station_core\",\"date\"], columns=\"datatype\", values=\"val_clean\")\n",
        "           .reset_index())\n",
        "# Rename columns to our canonical names\n",
        "wide = wide.rename(columns={k:v for k,v in keep_types.items() if k in wide.columns})\n",
        "\n",
        "# 5) Attach lat/lon from station catalog and clip to CA bbox\n",
        "wide = wide.merge(stations[[\"station_core\",\"lat\",\"lon\"]], on=\"station_core\", how=\"left\")\n",
        "\n",
        "# Clip to CONFIG[\"bbox\"] (California in your setup)\n",
        "bbox = CONFIG[\"bbox\"]\n",
        "minx, miny, maxx, maxy = bbox[\"nwlng\"], bbox[\"selat\"], bbox[\"selng\"], bbox[\"nwlat\"]\n",
        "in_box = (wide[\"lon\"].between(minx, maxx)) & (wide[\"lat\"].between(miny, maxy))\n",
        "wide = wide[in_box].copy()\n",
        "\n",
        "# 6) Final tidy columns + sorts\n",
        "cols_order = [\"station_core\",\"date\",\"lat\",\"lon\",\n",
        "              \"precipitation_mm\",\"temperature_max_c\",\"temperature_min_c\"]\n",
        "for c in cols_order:\n",
        "    if c not in wide.columns: wide[c] = np.nan\n",
        "wide = wide[cols_order].sort_values([\"station_core\",\"date\"])\n",
        "\n",
        "# 7) Save for the superset\n",
        "wide.to_parquet(OUT_PARQ, index=False)\n",
        "wide.to_csv(OUT_CSV, index=False)\n",
        "print(f\"Saved cleaned CDO daily → {OUT_PARQ} (rows={len(wide)}, stations={wide['station_core'].nunique()})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "599931c0",
      "metadata": {},
      "outputs": [],
      "source": [
        "### We've changed our approach -- DO NOT RUN THIS ANY MORE ###\n",
        "\n",
        "# === PRISM | Cell A: Raw ingest via official Web Service (resumable + chunkable) ===\n",
        "#\n",
        "# Web service syntax (per PRISM doc):\n",
        "#   https://services.nacse.org/prism/data/get/<region>/<res>/<element>/<date><?format=[nc|asc|bil]>\n",
        "# One grid per request, returns a .zip.  [oai_citation:4‡Prism Group](https://prism.oregonstate.edu/documents/PRISM_downloads_web_service.pdf)\n",
        "#\n",
        "# PRISM download limits:\n",
        "# - If a file is downloaded twice in a 24-hour period, no more downloads of that file allowed in that period\n",
        "# - Excessive activity may result in IP blocking  [oai_citation:5‡Prism Group](https://prism.oregonstate.edu/documents/PRISM_downloads_web_service.pdf)\n",
        "#\n",
        "# This cell is designed for \"download once, then reuse\", and for running in small chunks.\n",
        "\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "import requests\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "# -----------------------------\n",
        "# Required config (uses your existing baseline keys)\n",
        "# -----------------------------\n",
        "BASELINE_START = pd.to_datetime(CONFIG[\"BASELINE_START_DATE\"])\n",
        "BASELINE_END   = pd.to_datetime(CONFIG[\"BASELINE_END_DATE\"])\n",
        "\n",
        "# PRISM web service parameters\n",
        "PRISM_SERVICE_BASE = str(CONFIG.get(\"PRISM_SERVICE_BASE_URL\", \"https://services.nacse.org/prism/data/get\")) \n",
        "PRISM_REGION = str(CONFIG.get(\"PRISM_REGION\", \"us\"))     # 'us' CONUS  [oai_citation:6‡Prism Group](https://prism.oregonstate.edu/documents/PRISM_downloads_web_service.pdf)\n",
        "PRISM_RES    = str(CONFIG.get(\"PRISM_RESOLUTION\", \"4km\"))# '4km' supported  [oai_citation:7‡Prism Group](https://prism.oregonstate.edu/documents/PRISM_downloads_web_service.pdf)\n",
        "PRISM_ELEMENTS = CONFIG.get(\"PRISM_ELEMENTS\", [\"ppt\", \"tmean\"])  # elements list in doc  [oai_citation:8‡Prism Group](https://prism.oregonstate.edu/documents/PRISM_downloads_web_service.pdf)\n",
        "\n",
        "# Output folder\n",
        "OUT_DIR = Path(CONFIG[\"out_dir\"])\n",
        "RAW_DIR = OUT_DIR / \"prism_raw_baseline_ws\"\n",
        "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# -----------------------------\n",
        "# Controls for \"small chunks over several days\"\n",
        "# -----------------------------\n",
        "# You can run year-by-year to reduce server load and make progress predictable.\n",
        "# Set these each session (or add to config later if you like).\n",
        "RUN_YEAR_START = int(CONFIG.get(\"PRISM_RUN_YEAR_START\", BASELINE_START.year))\n",
        "RUN_YEAR_END   = int(CONFIG.get(\"PRISM_RUN_YEAR_END\", RUN_YEAR_START))  # default: single year\n",
        "MAX_DOWNLOADS_THIS_RUN = int(CONFIG.get(\"PRISM_MAX_DOWNLOADS_PER_RUN\", 400))  # hard stop per run\n",
        "\n",
        "# Throttling/backoff\n",
        "BASE_SLEEP_S = float(CONFIG.get(\"PRISM_BASE_SLEEP_S\", 2.0))   # PRISM sample script uses sleep 2  [oai_citation:9‡Prism Group](https://prism.oregonstate.edu/documents/PRISM_downloads_web_service.pdf)\n",
        "JITTER_S     = float(CONFIG.get(\"PRISM_JITTER_S\", 0.75))\n",
        "TIMEOUT_S    = int(CONFIG.get(\"PRISM_TIMEOUT_S\", 120))\n",
        "\n",
        "MAX_RETRIES  = int(CONFIG.get(\"PRISM_MAX_RETRIES\", 6))\n",
        "BACKOFF_BASE = float(CONFIG.get(\"PRISM_BACKOFF_BASE\", 1.7))\n",
        "\n",
        "# Optional: request format (default returns COG package; doc mentions optional formats nc/asc/bil)  [oai_citation:10‡Prism Group](https://prism.oregonstate.edu/documents/PRISM_downloads_web_service.pdf)\n",
        "# Leave as None for default; or set to \"bil\" / \"nc\"\n",
        "PRISM_FORMAT = CONFIG.get(\"PRISM_FORMAT\", None)\n",
        "\n",
        "# Optional: releaseDate check (defaults OFF).\n",
        "# When OFF: \"download once\" behavior = if file exists, skip without checking.\n",
        "USE_RELEASEDATE_CHECK = bool(CONFIG.get(\"PRISM_USE_RELEASEDATE_CHECK\", False))\n",
        "\n",
        "# -----------------------------\n",
        "# URL helpers (per doc)\n",
        "# -----------------------------\n",
        "def prism_grid_url(element: str, yyyymmdd: str) -> str:\n",
        "    # https://services.nacse.org/prism/data/get/<region>/<res>/<element>/<date><?format=...>  [oai_citation:11‡Prism Group](https://prism.oregonstate.edu/documents/PRISM_downloads_web_service.pdf)\n",
        "    url = f\"{PRISM_SERVICE_BASE}/{PRISM_REGION}/{PRISM_RES}/{element}/{yyyymmdd}\"\n",
        "    if PRISM_FORMAT:\n",
        "        url += f\"?format={PRISM_FORMAT}\"\n",
        "    return url\n",
        "\n",
        "def prism_release_url(element: str, yyyymmdd: str) -> str:\n",
        "    # https://services.nacse.org/prism/data/get/releaseDate/<region>/<resolution>/<element>/<date>?json=true  [oai_citation:12‡Prism Group](https://prism.oregonstate.edu/documents/PRISM_downloads_web_service.pdf)\n",
        "    return f\"{PRISM_SERVICE_BASE}/releaseDate/{PRISM_REGION}/{PRISM_RES}/{element}/{yyyymmdd}?json=true\"\n",
        "\n",
        "def safe_session() -> requests.Session:\n",
        "    s = requests.Session()\n",
        "    s.headers.update({\n",
        "        \"User-Agent\": CONFIG.get(\"USER_AGENT_HEADERS\", {}).get(\"User-Agent\", \"BlueLeafLabs/HydroPulse\"),\n",
        "        \"Accept\": \"*/*\",\n",
        "    })\n",
        "    return s\n",
        "\n",
        "def parse_filename_from_cd(cd: str) -> str | None:\n",
        "    # Content-Disposition: attachment; filename=prism_ppt_us_4km_19910101.zip\n",
        "    if not cd:\n",
        "        return None\n",
        "    cd = cd.strip()\n",
        "    parts = cd.split(\";\")\n",
        "    for p in parts:\n",
        "        p = p.strip()\n",
        "        if p.lower().startswith(\"filename=\"):\n",
        "            fn = p.split(\"=\", 1)[1].strip().strip('\"')\n",
        "            return fn\n",
        "    return None\n",
        "\n",
        "# -----------------------------\n",
        "# Local pathing\n",
        "# -----------------------------\n",
        "def out_path_for(element: str, yyyymmdd: str, filename_hint: str | None = None) -> Path:\n",
        "    ed = RAW_DIR / element\n",
        "    ed.mkdir(parents=True, exist_ok=True)\n",
        "    if filename_hint:\n",
        "        return ed / filename_hint\n",
        "    # Fallback (stable and unique even if server changes naming slightly)\n",
        "    suffix = PRISM_FORMAT if PRISM_FORMAT else \"cog\"\n",
        "    return ed / f\"prism_{element}_{PRISM_REGION}_{PRISM_RES}_{yyyymmdd}_{suffix}.zip\"\n",
        "\n",
        "# -----------------------------\n",
        "# Optional releaseDate logic\n",
        "# -----------------------------\n",
        "def fetch_release_date(session: requests.Session, element: str, yyyymmdd: str) -> str | None:\n",
        "    # doc: releaseDate service provides release date; older than Apr 2014 may be unpopulated  [oai_citation:13‡Prism Group](https://prism.oregonstate.edu/documents/PRISM_downloads_web_service.pdf)\n",
        "    try:\n",
        "        r = session.get(prism_release_url(element, yyyymmdd), timeout=TIMEOUT_S)\n",
        "        if r.status_code != 200:\n",
        "            return None\n",
        "        js = r.json()\n",
        "        # The PDF describes fields; response structure may be list/dict depending on single vs range.\n",
        "        # We'll defensively extract any plausible release date string.\n",
        "        if isinstance(js, list) and js:\n",
        "            return js[0].get(\"releaseDate\") or js[0].get(\"ReleaseDate\") or js[0].get(\"release_date\")\n",
        "        if isinstance(js, dict):\n",
        "            return js.get(\"releaseDate\") or js.get(\"ReleaseDate\") or js.get(\"release_date\")\n",
        "        return None\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "# -----------------------------\n",
        "# Download with retries + backoff\n",
        "# -----------------------------\n",
        "def download_one(session: requests.Session, element: str, yyyymmdd: str) -> tuple[str, Path | None]:\n",
        "    url = prism_grid_url(element, yyyymmdd)\n",
        "\n",
        "    # First request HEAD-like via GET stream (server returns a zip)\n",
        "    for attempt in range(MAX_RETRIES):\n",
        "        try:\n",
        "            r = session.get(url, stream=True, timeout=TIMEOUT_S)\n",
        "            if r.status_code == 404:\n",
        "                return (\"unavailable\", None)\n",
        "            if r.status_code in (429, 500, 502, 503, 504):\n",
        "                raise RuntimeError(f\"transient {r.status_code}\")\n",
        "            r.raise_for_status()\n",
        "\n",
        "            fn = parse_filename_from_cd(r.headers.get(\"Content-Disposition\", \"\"))\n",
        "            out_path = out_path_for(element, yyyymmdd, fn)\n",
        "\n",
        "            if out_path.exists():\n",
        "                # Do not re-download. Close response promptly.\n",
        "                r.close()\n",
        "                return (\"exists\", out_path)\n",
        "\n",
        "            tmp = out_path.with_suffix(out_path.suffix + \".part\")\n",
        "            with open(tmp, \"wb\") as f:\n",
        "                for chunk in r.iter_content(chunk_size=1024 * 1024):\n",
        "                    if chunk:\n",
        "                        f.write(chunk)\n",
        "            os.replace(tmp, out_path)\n",
        "            return (\"downloaded\", out_path)\n",
        "\n",
        "        except Exception as e:\n",
        "            # backoff + jitter\n",
        "            sleep_s = (BACKOFF_BASE ** attempt) + random.random() * JITTER_S\n",
        "            print(f\"[WARN] {element} {yyyymmdd} attempt {attempt+1}/{MAX_RETRIES}: {e} -> sleep {sleep_s:.2f}s\")\n",
        "            time.sleep(sleep_s)\n",
        "\n",
        "    return (\"failed\", None)\n",
        "\n",
        "# -----------------------------\n",
        "# Build run date range (year-sliced)\n",
        "# -----------------------------\n",
        "run_start = max(BASELINE_START, pd.Timestamp(year=RUN_YEAR_START, month=1, day=1))\n",
        "run_end   = min(BASELINE_END,   pd.Timestamp(year=RUN_YEAR_END,   month=12, day=31))\n",
        "dates = pd.date_range(run_start, run_end, freq=\"D\")\n",
        "\n",
        "print(\"PRISM Cell A (web service) starting\")\n",
        "print(f\"Baseline window: {BASELINE_START.date()} → {BASELINE_END.date()}\")\n",
        "print(f\"Run slice      : {run_start.date()} → {run_end.date()} ({len(dates)} days)\")\n",
        "print(f\"Elements       : {PRISM_ELEMENTS}\")\n",
        "print(f\"Resolution     : {PRISM_RES} | Region: {PRISM_REGION}\")\n",
        "print(f\"Max downloads  : {MAX_DOWNLOADS_THIS_RUN}\")\n",
        "print(f\"Sleep (on dl)  : {BASE_SLEEP_S}s + jitter up to {JITTER_S}s\\n\")\n",
        "\n",
        "# -----------------------------\n",
        "# Main loop\n",
        "# -----------------------------\n",
        "session = safe_session()\n",
        "\n",
        "stats = {\"downloaded\": 0, \"exists\": 0, \"unavailable\": 0, \"failed\": 0, \"skipped_release\": 0}\n",
        "downloads_this_run = 0\n",
        "\n",
        "for element in PRISM_ELEMENTS:\n",
        "    print(f\"--- Element: {element} ---\")\n",
        "    for i, d in enumerate(dates, start=1):\n",
        "        yyyymmdd = d.strftime(\"%Y%m%d\")\n",
        "\n",
        "        # Enforce per-run cap (lets you run small chunks over multiple days)\n",
        "        if downloads_this_run >= MAX_DOWNLOADS_THIS_RUN:\n",
        "            print(f\"[STOP] Reached PRISM_MAX_DOWNLOADS_PER_RUN={MAX_DOWNLOADS_THIS_RUN}. Safe to rerun later.\")\n",
        "            break\n",
        "\n",
        "        # If file exists, skip immediately (resume behavior)\n",
        "        # We don’t know server filename until request, so check fallback name pattern too.\n",
        "        # We’ll do a cheap existence check by globbing element dir for this date.\n",
        "        el_dir = RAW_DIR / element\n",
        "        if el_dir.exists():\n",
        "            hits = list(el_dir.glob(f\"*{yyyymmdd}*.zip\"))\n",
        "            if hits:\n",
        "                stats[\"exists\"] += 1\n",
        "                continue\n",
        "\n",
        "        # Optional release-date check (OFF by default)\n",
        "        if USE_RELEASEDATE_CHECK:\n",
        "            _ = fetch_release_date(session, element, yyyymmdd)  # you can wire this into a manifest later\n",
        "\n",
        "        status, path = download_one(session, element, yyyymmdd)\n",
        "        stats[status] += 1\n",
        "\n",
        "        if status == \"downloaded\":\n",
        "            downloads_this_run += 1\n",
        "            # polite sleep only when we actually transfer bytes (PRISM sample script sleeps 2)  [oai_citation:14‡Prism Group](https://prism.oregonstate.edu/documents/PRISM_downloads_web_service.pdf)\n",
        "            time.sleep(BASE_SLEEP_S + random.random() * JITTER_S)\n",
        "\n",
        "        # Heartbeat every ~50 days\n",
        "        if i % 50 == 0:\n",
        "            print(f\"{element} day {i}/{len(dates)} | dl={stats['downloaded']} exist={stats['exists']} unavail={stats['unavailable']} failed={stats['failed']}\")\n",
        "\n",
        "    print(f\"Completed element: {element}\\n\")\n",
        "\n",
        "print(\"PRISM Cell A complete (this run slice)\")\n",
        "print(stats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4a4a349",
      "metadata": {},
      "outputs": [],
      "source": [
        "# === PRISM | Cell B: Build HydroPulse baseline from PRISM daily long-term normals (avg_30y) ===\n",
        "#\n",
        "# Inputs (local, no downloads):\n",
        "#   {out_dir}/manual/prism/prism_ppt_us_25m_YYYYMMDD_avg_30y.zip\n",
        "#   {out_dir}/manual/prism/prism_tmean_us_25m_YYYYMMDD_avg_30y.zip\n",
        "#\n",
        "# Output:\n",
        "#   {out_dir}/prism_baseline_normals/prism_normals_doy_grid_25m_to_3km.parquet\n",
        "#\n",
        "# Output schema:\n",
        "#   grid_id, doy, prism_ppt_norm_mm, prism_tmean_norm_c\n",
        "\n",
        "import re\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "\n",
        "import rasterio\n",
        "from rasterio.io import MemoryFile\n",
        "from pyproj import Transformer\n",
        "\n",
        "OUT_DIR = Path(CONFIG[\"out_dir\"])\n",
        "\n",
        "# Where you placed PRISM normals\n",
        "PRISM_DIR = Path(CONFIG.get(\"PRISM_MANUAL_DIR\", OUT_DIR / \"manual\" / \"prism\"))\n",
        "if not PRISM_DIR.exists():\n",
        "    raise FileNotFoundError(f\"PRISM_MANUAL_DIR not found: {PRISM_DIR}\")\n",
        "\n",
        "# HydroPulse grid (EPSG:3310)\n",
        "OPS_EPSG = int(CONFIG.get(\"OPS_EPSG\", 3310))\n",
        "GRID_PATH = Path(resolve_out_path(CONFIG[\"GRID_FILENAME\"]))\n",
        "\n",
        "# Output\n",
        "BASE_DIR = OUT_DIR / \"prism_baseline_normals\"\n",
        "BASE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "SHARDS_DIR = BASE_DIR / \"shards_doy\"\n",
        "SHARDS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "FINAL_PATH = BASE_DIR / \"prism_normals_doy_grid_25m_to_3km.parquet\"\n",
        "\n",
        "# --- Load grid centroids ---\n",
        "grid = gpd.read_parquet(GRID_PATH)\n",
        "if grid.crs is None or (grid.crs.to_epsg() or 0) != OPS_EPSG:\n",
        "    grid = grid.set_crs(f\"EPSG:{OPS_EPSG}\", allow_override=True) if grid.crs is None else grid.to_crs(OPS_EPSG)\n",
        "\n",
        "if \"grid_id\" not in grid.columns:\n",
        "    raise KeyError(\"Expected grid parquet to contain 'grid_id'.\")\n",
        "\n",
        "grid_id = grid[\"grid_id\"].astype(str).values\n",
        "centroids = grid.geometry.centroid\n",
        "xs = centroids.x.values\n",
        "ys = centroids.y.values\n",
        "n_cells = len(grid_id)\n",
        "\n",
        "print(f\"Grid: {n_cells} cells | EPSG:{OPS_EPSG}\")\n",
        "print(f\"PRISM normals dir: {PRISM_DIR}\")\n",
        "\n",
        "# --- PRISM filename parser (your observed convention) ---\n",
        "# Example: prism_ppt_us_25m_20200115_avg_30y.zip\n",
        "pat = re.compile(r\"^prism_(ppt|tmean)_us_25m_(\\d{8})_avg_30y\\.zip$\", re.IGNORECASE)\n",
        "\n",
        "ppt = {}\n",
        "tmean = {}\n",
        "\n",
        "for p in sorted(PRISM_DIR.glob(\"*.zip\")):\n",
        "    m = pat.match(p.name)\n",
        "    if not m:\n",
        "        continue\n",
        "    var = m.group(1).lower()\n",
        "    yyyymmdd = m.group(2)\n",
        "\n",
        "    # PRISM daily normals commonly use year=2020 as a convenient leap-year index;\n",
        "    # we convert YYYYMMDD -> DOY using that year token directly.\n",
        "    dt = pd.to_datetime(yyyymmdd, format=\"%Y%m%d\", utc=True)\n",
        "    doy = int(dt.dayofyear)\n",
        "\n",
        "    if var == \"ppt\":\n",
        "        ppt[doy] = p\n",
        "    elif var == \"tmean\":\n",
        "        tmean[doy] = p\n",
        "\n",
        "doys = sorted(set(ppt.keys()) & set(tmean.keys()))\n",
        "if not doys:\n",
        "    raise RuntimeError(\n",
        "        \"No matching PRISM ppt/tmean normals found.\\n\"\n",
        "        f\"Expected filenames like: prism_ppt_us_25m_20200115_avg_30y.zip in {PRISM_DIR}\"\n",
        "    )\n",
        "\n",
        "print(f\"Matched DOYs: {len(doys)} (e.g., {doys[:5]})\")\n",
        "\n",
        "# --- Read GeoTIFF inside PRISM zip via MemoryFile ---\n",
        "def open_tif_from_zip(zip_path: Path):\n",
        "    with zipfile.ZipFile(zip_path, \"r\") as z:\n",
        "        tif_names = [n for n in z.namelist() if n.lower().endswith(\".tif\")]\n",
        "        if not tif_names:\n",
        "            raise FileNotFoundError(f\"No .tif found inside {zip_path.name}\")\n",
        "        tif_name = tif_names[0]\n",
        "        tif_bytes = z.read(tif_name)\n",
        "\n",
        "    mem = MemoryFile(tif_bytes)\n",
        "    ds = mem.open()\n",
        "    return mem, ds\n",
        "\n",
        "def sample_zip_to_grid(zip_path: Path, xs3310: np.ndarray, ys3310: np.ndarray) -> np.ndarray:\n",
        "    mem, ds = open_tif_from_zip(zip_path)\n",
        "    try:\n",
        "        tf = Transformer.from_crs(f\"EPSG:{OPS_EPSG}\", ds.crs, always_xy=True)\n",
        "        sx, sy = tf.transform(xs3310, ys3310)\n",
        "\n",
        "        vals = np.array([v[0] for v in ds.sample(zip(sx, sy))], dtype=np.float64)\n",
        "        nodata = ds.nodata\n",
        "        if nodata is not None:\n",
        "            vals[vals == nodata] = np.nan\n",
        "        vals[~np.isfinite(vals)] = np.nan\n",
        "        return vals\n",
        "    finally:\n",
        "        ds.close()\n",
        "        mem.close()\n",
        "\n",
        "# --- Build shards per DOY (resumable) ---\n",
        "written = 0\n",
        "skipped = 0\n",
        "\n",
        "for doy in doys:\n",
        "    shard_path = SHARDS_DIR / f\"prism_normals_doy_{doy:03d}.parquet\"\n",
        "    if shard_path.exists():\n",
        "        skipped += 1\n",
        "        continue\n",
        "\n",
        "    ppt_vals = sample_zip_to_grid(ppt[doy], xs, ys).astype(np.float32)      # mm\n",
        "    tm_vals  = sample_zip_to_grid(tmean[doy], xs, ys).astype(np.float32)    # °C\n",
        "\n",
        "    out = pd.DataFrame({\n",
        "        \"grid_id\": grid_id,\n",
        "        \"doy\": np.full(n_cells, doy, dtype=np.int16),\n",
        "        \"prism_ppt_norm_mm\": ppt_vals,\n",
        "        \"prism_tmean_norm_c\": tm_vals,\n",
        "    })\n",
        "    out.to_parquet(shard_path, index=False)\n",
        "    written += 1\n",
        "\n",
        "    if written % 25 == 0:\n",
        "        print(f\"Shards written: {written} | latest DOY={doy:03d}\")\n",
        "\n",
        "print(f\"Shard pass complete | written={written} | skipped={skipped}\")\n",
        "\n",
        "# --- Combine to one baseline parquet (fast enough at 366 shards) ---\n",
        "shards = sorted(SHARDS_DIR.glob(\"prism_normals_doy_*.parquet\"))\n",
        "df_all = pd.concat((pd.read_parquet(p) for p in shards), ignore_index=True)\n",
        "df_all.to_parquet(FINAL_PATH, index=False)\n",
        "\n",
        "print(f\"Saved baseline: {FINAL_PATH}\")\n",
        "print(f\"Rows: {len(df_all)} | doys: {df_all['doy'].nunique()} | cells: {df_all['grid_id'].nunique()}\")\n",
        "print(df_all.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b9a2bba",
      "metadata": {},
      "outputs": [],
      "source": [
        "# === SMAP (SPL4SMGP) via Harmony: CA-only subset + quiet + strong resume ===\n",
        "\n",
        "from pathlib import Path\n",
        "import os\n",
        "import datetime as dt\n",
        "\n",
        "import earthaccess\n",
        "from harmony import BBox, Client, Collection, Request, CapabilitiesRequest\n",
        "\n",
        "# ---------- 0) env reload (reuse your existing helpers if available) ----------\n",
        "try:\n",
        "    apply_env_overrides()\n",
        "except NameError:\n",
        "    pass\n",
        "\n",
        "if not os.environ.get(\"EARTHDATA_TOKEN\") and not (\n",
        "    os.environ.get(\"EARTHDATA_USERNAME\") and os.environ.get(\"EARTHDATA_PASSWORD\")\n",
        "):\n",
        "    raise RuntimeError(\"Missing Earthdata credentials. Set EARTHDATA_TOKEN (recommended) in .env.\")\n",
        "\n",
        "earthaccess.login(strategy=\"environment\")  # should be quiet if already logged in\n",
        "\n",
        "# ---------- 1) config ----------\n",
        "bbox = CONFIG[\"bbox\"]\n",
        "W = float(bbox[\"nwlng\"])\n",
        "S = float(bbox[\"selat\"])\n",
        "E = float(bbox[\"selng\"])\n",
        "N = float(bbox[\"nwlat\"])\n",
        "\n",
        "START = dt.date.fromisoformat(CONFIG[\"start_date\"])\n",
        "END   = dt.date.fromisoformat(CONFIG[\"end_date\"])\n",
        "\n",
        "SHORT_NAME = CONFIG.get(\"SMAP_L4_SHORT_NAME\", \"SPL4SMGP\")\n",
        "DESIRED_VARS = CONFIG.get(\"SMAP_L4_VARIABLES\", [\"sm_surface\", \"sm_rootzone\"])\n",
        "\n",
        "OUT_DIR = Path(CONFIG[\"out_dir\"])\n",
        "SMAP_DIR = OUT_DIR / \"manual\" / \"smap\" / SHORT_NAME\n",
        "SMAP_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "SENTINEL_DIR = SMAP_DIR / \"_done\"\n",
        "SENTINEL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def month_start(d: dt.date) -> dt.date:\n",
        "    return dt.date(d.year, d.month, 1)\n",
        "\n",
        "def next_month(d: dt.date) -> dt.date:\n",
        "    return dt.date(d.year + (d.month == 12), 1 if d.month == 12 else d.month + 1, 1)\n",
        "\n",
        "def month_range(start: dt.date, end: dt.date):\n",
        "    cur = month_start(start)\n",
        "    while cur <= end:\n",
        "        nxt = next_month(cur)\n",
        "        yield cur, min(end + dt.timedelta(days=1), nxt)  # end is effectively exclusive\n",
        "        cur = nxt\n",
        "\n",
        "def has_any_files(folder: Path) -> bool:\n",
        "    if not folder.exists():\n",
        "        return False\n",
        "    # Harmony may write nested outputs; look recursively for non-empty files\n",
        "    for p in folder.rglob(\"*\"):\n",
        "        if p.is_file() and p.stat().st_size > 0:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "print(\"SMAP/Harmony (quiet) setup\")\n",
        "print(\"  short_name:\", SHORT_NAME)\n",
        "print(\"  bbox (W,S,E,N):\", (W, S, E, N))\n",
        "print(\"  time:\", START, \"→\", END)\n",
        "print(\"  root:\", SMAP_DIR)\n",
        "\n",
        "# ---------- 2) capabilities + variable sanitization ----------\n",
        "harmony_client = Client()\n",
        "\n",
        "cap_req = CapabilitiesRequest(short_name=SHORT_NAME)\n",
        "cap = harmony_client.submit(cap_req)\n",
        "\n",
        "import json\n",
        "if isinstance(cap, str):\n",
        "    cap = json.loads(cap)\n",
        "\n",
        "concept_id = cap.get(\"conceptId\") or cap.get(\"concept_id\") or cap.get(\"conceptID\")\n",
        "if not concept_id:\n",
        "    raise RuntimeError(f\"Could not determine conceptId for {SHORT_NAME} from Harmony capabilities.\")\n",
        "\n",
        "available_var_names = set()\n",
        "for v in (cap.get(\"variables\") or []):\n",
        "    if isinstance(v, dict) and \"name\" in v:\n",
        "        available_var_names.add(v[\"name\"])\n",
        "\n",
        "if available_var_names:\n",
        "    chosen = []\n",
        "    for name in DESIRED_VARS:\n",
        "        if name in available_var_names:\n",
        "            chosen.append(name)\n",
        "        else:\n",
        "            hits = [vn for vn in available_var_names if vn == name or vn.endswith(name) or name in vn]\n",
        "            if hits:\n",
        "                chosen.append(hits[0])\n",
        "    DESIRED_VARS = sorted(set(chosen))\n",
        "\n",
        "# ---------- 3) submit/download month-by-month (quiet + strong resume) ----------\n",
        "jobs_submitted = 0\n",
        "months_skipped = 0\n",
        "files_downloaded = 0\n",
        "\n",
        "for m0, m1 in month_range(START, END):\n",
        "    tag = f\"{m0:%Y%m}\"\n",
        "    sentinel = SENTINEL_DIR / f\"{tag}.done\"\n",
        "    month_dir = SMAP_DIR / tag\n",
        "\n",
        "    # Strong resume: skip if sentinel exists OR month folder already has files\n",
        "    if sentinel.exists() or has_any_files(month_dir):\n",
        "        months_skipped += 1\n",
        "        if not sentinel.exists():\n",
        "            # create sentinel so future runs are clean\n",
        "            sentinel.write_text(\"done=1\\nnote=folder already had files\\n\")\n",
        "        print(f\"[SKIP] {tag}\")\n",
        "        continue\n",
        "\n",
        "    req = Request(\n",
        "        collection=Collection(id=concept_id),\n",
        "        spatial=BBox(W, S, E, N),\n",
        "        temporal={\"start\": dt.datetime(m0.year, m0.month, m0.day),\n",
        "                  \"stop\":  dt.datetime(m1.year, m1.month, m1.day)},\n",
        "        variables=DESIRED_VARS if DESIRED_VARS else None,\n",
        "    )\n",
        "\n",
        "    if not req.is_valid():\n",
        "        raise RuntimeError(f\"Invalid Harmony request for month {tag}: check bbox/time/vars\")\n",
        "\n",
        "    print(f\"[RUN] {tag} submitting…\")\n",
        "    job_id = harmony_client.submit(req)\n",
        "    jobs_submitted += 1\n",
        "\n",
        "    # Quiet wait (no progress spam)\n",
        "    harmony_client.wait_for_processing(job_id, show_progress=False)\n",
        "\n",
        "    month_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    futures = harmony_client.download_all(job_id, directory=str(month_dir), overwrite=False)\n",
        "    out_files = [f.result() for f in futures]\n",
        "    out_files = [p for p in out_files if p]\n",
        "\n",
        "    files_downloaded += len(out_files)\n",
        "    sentinel.write_text(f\"job_id={job_id}\\nfiles={len(out_files)}\\n\")\n",
        "    print(f\"[DONE] {tag} files={len(out_files)}\")\n",
        "\n",
        "print(\"SMAP/Harmony complete\")\n",
        "print(\"  jobs submitted:\", jobs_submitted)\n",
        "print(\"  months skipped:\", months_skipped)\n",
        "print(\"  files downloaded this run:\", files_downloaded)\n",
        "print(\"  root dir:\", SMAP_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4ee2569",
      "metadata": {},
      "outputs": [],
      "source": [
        "# === SMAP Cell A (fixed): SPL4SMGP Harmony subset -> daily canonical parquet (resume-safe, quiet) ===\n",
        "\n",
        "from pathlib import Path\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xarray as xr\n",
        "\n",
        "OUT_DIR = Path(CONFIG[\"out_dir\"])\n",
        "SMAP_ROOT = OUT_DIR / Path(CONFIG.get(\"SMAP_L4_DIRNAME\", \"manual/smap/SPL4SMGP\"))\n",
        "if not SMAP_ROOT.exists():\n",
        "    raise FileNotFoundError(f\"Missing SMAP root dir: {SMAP_ROOT}\")\n",
        "\n",
        "DAILY_DIR = OUT_DIR / Path(CONFIG.get(\"SMAP_DAILY_DIRNAME\", \"derived/smap_daily\"))\n",
        "DAILY_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "GROUP = (CONFIG.get(\"SMAP_NETCDF_GROUP\", \"Geophysical_Data\") or \"\").strip()\n",
        "VAR_SURF = CONFIG.get(\"SMAP_VAR_SURFACE\", \"sm_surface\")\n",
        "VAR_ROOT = CONFIG.get(\"SMAP_VAR_ROOTZONE\", \"sm_rootzone\")\n",
        "\n",
        "TEMPLATE = CONFIG.get(\"SMAP_DAILY_TEMPLATE\", \"smap_daily_{date}.parquet\")\n",
        "AGG = (CONFIG.get(\"SMAP_DAILY_AGG\", \"median\") or \"median\").lower()\n",
        "LOG_EVERY = int(CONFIG.get(\"SMAP_LOG_EVERY_N_DAYS\", 20))\n",
        "\n",
        "if AGG not in {\"median\", \"mean\"}:\n",
        "    raise ValueError(\"SMAP_DAILY_AGG must be 'median' or 'mean'\")\n",
        "\n",
        "# Filenames like: SMAP_L4_SM_gph_20240731T223000_Vv8010_001_subsetted.nc4\n",
        "TS_RE = re.compile(r\"_(\\d{8})T(\\d{6})_\")\n",
        "\n",
        "def parse_dt_from_name(p: Path) -> pd.Timestamp:\n",
        "    m = TS_RE.search(p.name)\n",
        "    if not m:\n",
        "        raise ValueError(f\"Cannot parse timestamp from filename: {p.name}\")\n",
        "    ymd, hms = m.group(1), m.group(2)\n",
        "    return pd.Timestamp(f\"{ymd}{hms}\", tz=\"UTC\")\n",
        "\n",
        "def out_path_for_date(d_utc: pd.Timestamp) -> Path:\n",
        "    return DAILY_DIR / TEMPLATE.format(date=d_utc.date().isoformat())\n",
        "\n",
        "def open_smap(fp: Path) -> xr.Dataset:\n",
        "    # Use netcdf4 engine; open the science group\n",
        "    return xr.open_dataset(fp, engine=\"netcdf4\", group=GROUP, decode_times=False, mask_and_scale=True)\n",
        "\n",
        "def agg_stack(stack: np.ndarray) -> np.ndarray:\n",
        "    # stack shape: (T, H, W), with NaN\n",
        "    if AGG == \"median\":\n",
        "        with np.errstate(all=\"ignore\"):\n",
        "            return np.nanmedian(stack, axis=0)\n",
        "    return np.nanmean(stack, axis=0)\n",
        "\n",
        "# Collect likely data files (Harmony outputs often .nc4)\n",
        "OK_EXT = {\".nc4\", \".nc\", \".cdf\", \".h5\", \".hdf5\"}\n",
        "all_files = sorted(\n",
        "    p for p in SMAP_ROOT.rglob(\"*\")\n",
        "    if p.is_file()\n",
        "    and \"_done\" not in p.parts\n",
        "    and not p.name.endswith(\".done\")\n",
        "    and p.stat().st_size > 0\n",
        "    and (p.suffix.lower() in OK_EXT)\n",
        ")\n",
        "\n",
        "if not all_files:\n",
        "    raise FileNotFoundError(f\"No SMAP data files found under {SMAP_ROOT}\")\n",
        "\n",
        "# Build index of files with timestamps\n",
        "rows = []\n",
        "for fp in all_files:\n",
        "    try:\n",
        "        ts = parse_dt_from_name(fp)\n",
        "    except Exception:\n",
        "        continue\n",
        "    rows.append((fp, ts, ts.normalize()))\n",
        "\n",
        "idx = pd.DataFrame(rows, columns=[\"path\", \"ts_utc\", \"date_utc\"])\n",
        "if idx.empty:\n",
        "    raise RuntimeError(\"Found SMAP files but none matched the expected timestamp pattern.\")\n",
        "\n",
        "# Filter to your configured analysis window\n",
        "start = pd.Timestamp(CONFIG[\"start_date\"], tz=\"UTC\")\n",
        "end = pd.Timestamp(CONFIG[\"end_date\"], tz=\"UTC\") + pd.Timedelta(days=1) - pd.Timedelta(seconds=1)\n",
        "idx = idx[(idx[\"ts_utc\"] >= start) & (idx[\"ts_utc\"] <= end)].copy()\n",
        "idx.sort_values([\"date_utc\", \"ts_utc\"], inplace=True)\n",
        "\n",
        "dates = idx[\"date_utc\"].drop_duplicates().tolist()\n",
        "if not dates:\n",
        "    raise RuntimeError(\"No SMAP files within CONFIG start/end window.\")\n",
        "\n",
        "print(\"SMAP Cell A starting\")\n",
        "print(\"  SMAP root      :\", SMAP_ROOT)\n",
        "print(\"  Files in window:\", len(idx))\n",
        "print(\"  Days in window :\", len(dates))\n",
        "print(\"  Group/vars     :\", GROUP, VAR_SURF, VAR_ROOT)\n",
        "print(\"  Output dir     :\", DAILY_DIR)\n",
        "\n",
        "processed = 0\n",
        "skipped = 0\n",
        "\n",
        "for i, d in enumerate(dates, start=1):\n",
        "    outp = out_path_for_date(d)\n",
        "    if outp.exists() and outp.stat().st_size > 0:\n",
        "        skipped += 1\n",
        "        continue\n",
        "\n",
        "    day_files = idx.loc[idx[\"date_utc\"] == d, \"path\"].tolist()\n",
        "    if not day_files:\n",
        "        continue\n",
        "\n",
        "    surf_list, root_list = [], []\n",
        "\n",
        "    for fp in day_files:\n",
        "        ds = open_smap(fp)\n",
        "\n",
        "        # pull arrays\n",
        "        surf = ds[VAR_SURF].astype(\"float32\").values\n",
        "        root = ds[VAR_ROOT].astype(\"float32\").values\n",
        "\n",
        "        # replace fill (-9999) + enforce plausible range [0,1]\n",
        "        surf = np.where((surf >= 0.0) & (surf <= 1.0), surf, np.nan)\n",
        "        root = np.where((root >= 0.0) & (root <= 1.0), root, np.nan)\n",
        "\n",
        "        surf_list.append(surf)\n",
        "        root_list.append(root)\n",
        "\n",
        "        ds.close()\n",
        "\n",
        "    surf_stack = np.stack(surf_list, axis=0)\n",
        "    root_stack = np.stack(root_list, axis=0)\n",
        "\n",
        "    surf_day = agg_stack(surf_stack)\n",
        "    root_day = agg_stack(root_stack)\n",
        "\n",
        "    n_obs = np.sum(np.isfinite(surf_stack), axis=0).astype(\"int16\")\n",
        "\n",
        "    H, W = surf_day.shape\n",
        "    yy, xx = np.indices((H, W))\n",
        "\n",
        "    df = pd.DataFrame({\n",
        "        \"date_utc\": np.repeat(d, H * W),\n",
        "        \"y\": yy.ravel().astype(\"int32\"),\n",
        "        \"x\": xx.ravel().astype(\"int32\"),\n",
        "        \"sm_surface\": surf_day.ravel().astype(\"float32\"),\n",
        "        \"sm_rootzone\": root_day.ravel().astype(\"float32\"),\n",
        "        \"n_obs\": n_obs.ravel().astype(\"int16\"),\n",
        "    })\n",
        "\n",
        "    # Drop pixels with no data at all\n",
        "    df = df[~(df[\"sm_surface\"].isna() & df[\"sm_rootzone\"].isna())]\n",
        "\n",
        "    tmp = outp.with_suffix(\".parquet.tmp\")\n",
        "    df.to_parquet(tmp, index=False)\n",
        "    os.replace(tmp, outp)\n",
        "\n",
        "    processed += 1\n",
        "    if (processed % LOG_EVERY) == 0 or i == len(dates):\n",
        "        print(f\"  day {i}/{len(dates)} | wrote={processed} skipped={skipped} | last={d.date()} | rows={len(df):,}\")\n",
        "\n",
        "print(\"SMAP Cell A complete\")\n",
        "print(\"  wrote  :\", processed)\n",
        "print(\"  skipped:\", skipped)\n",
        "print(\"  daily dir:\", DAILY_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f7dd8be",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Some SMAP validation/QA on daily parquet outputs\n",
        "\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "OUT_DIR = Path(CONFIG[\"out_dir\"])\n",
        "DAILY_DIR = OUT_DIR / Path(CONFIG.get(\"SMAP_DAILY_DIRNAME\", \"derived/smap_daily\"))\n",
        "\n",
        "files = sorted(DAILY_DIR.glob(\"smap_daily_*.parquet\"))\n",
        "assert files, f\"No SMAP daily parquet files found in {DAILY_DIR}\"\n",
        "\n",
        "# Sample a few days + last day\n",
        "sample_files = [files[0], files[len(files)//2], files[-1]]\n",
        "rows = []\n",
        "\n",
        "for fp in sample_files:\n",
        "    df = pd.read_parquet(fp)\n",
        "\n",
        "    rows.append({\n",
        "        \"file\": fp.name,\n",
        "        \"rows\": len(df),\n",
        "        \"unique_pixels\": df[[\"y\",\"x\"]].drop_duplicates().shape[0],\n",
        "        \"sm_surface_min\": float(np.nanmin(df[\"sm_surface\"].values)) if df[\"sm_surface\"].notna().any() else np.nan,\n",
        "        \"sm_surface_p01\": float(np.nanpercentile(df[\"sm_surface\"].dropna(), 1)) if df[\"sm_surface\"].notna().any() else np.nan,\n",
        "        \"sm_surface_p50\": float(np.nanpercentile(df[\"sm_surface\"].dropna(), 50)) if df[\"sm_surface\"].notna().any() else np.nan,\n",
        "        \"sm_surface_p99\": float(np.nanpercentile(df[\"sm_surface\"].dropna(), 99)) if df[\"sm_surface\"].notna().any() else np.nan,\n",
        "        \"sm_surface_max\": float(np.nanmax(df[\"sm_surface\"].values)) if df[\"sm_surface\"].notna().any() else np.nan,\n",
        "        \"sm_rootzone_p50\": float(np.nanpercentile(df[\"sm_rootzone\"].dropna(), 50)) if df[\"sm_rootzone\"].notna().any() else np.nan,\n",
        "        \"n_obs_p50\": float(np.nanpercentile(df[\"n_obs\"].values, 50)),\n",
        "        \"n_obs_min\": int(df[\"n_obs\"].min()),\n",
        "        \"n_obs_max\": int(df[\"n_obs\"].max()),\n",
        "        \"nan_surface_frac\": float(df[\"sm_surface\"].isna().mean()),\n",
        "        \"nan_rootzone_frac\": float(df[\"sm_rootzone\"].isna().mean()),\n",
        "    })\n",
        "\n",
        "qa = pd.DataFrame(rows)\n",
        "qa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "841a9137",
      "metadata": {},
      "outputs": [],
      "source": [
        "# === SMAP Cell B: daily SMAP pixels -> HydroPulse 3km grid_id daily table (resume-safe) ===\n",
        "\n",
        "from pathlib import Path\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import Point\n",
        "import xarray as xr\n",
        "\n",
        "OUT_DIR = Path(CONFIG[\"out_dir\"])\n",
        "\n",
        "# Inputs\n",
        "GRID_PATH = Path(resolve_out_path(CONFIG.get(\"GRID_FILENAME\", \"grid_3000m_CA_epsg3310.parquet\")))\n",
        "SMAP_ROOT = OUT_DIR / Path(CONFIG.get(\"SMAP_L4_DIRNAME\", \"manual/smap/SPL4SMGP\"))\n",
        "SMAP_DAILY_DIR = OUT_DIR / Path(CONFIG.get(\"SMAP_DAILY_DIRNAME\", \"derived/smap_daily\"))\n",
        "\n",
        "if not GRID_PATH.exists():\n",
        "    raise FileNotFoundError(f\"Missing grid parquet: {GRID_PATH}\")\n",
        "if not SMAP_ROOT.exists():\n",
        "    raise FileNotFoundError(f\"Missing SMAP root: {SMAP_ROOT}\")\n",
        "if not SMAP_DAILY_DIR.exists():\n",
        "    raise FileNotFoundError(f\"Missing SMAP daily dir: {SMAP_DAILY_DIR}\")\n",
        "\n",
        "# Outputs\n",
        "GRIDMAP_PATH = OUT_DIR / CONFIG.get(\"SMAP_GRIDMAP_FILENAME\", \"smap_pixel_to_grid_3310.parquet\")\n",
        "SHARDS_DIR = OUT_DIR / Path(CONFIG.get(\"SMAP_GRID_DAILY_SHARDS_DIRNAME\", \"derived/smap_grid_shards\"))\n",
        "SHARDS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "final_name_tmpl = CONFIG.get(\"SMAP_GRID_FINAL_FILENAME\", \"smap_daily_grid_CA_3000m_epsg3310_{start}_{end}.parquet\")\n",
        "FINAL_PATH = OUT_DIR / final_name_tmpl.format(\n",
        "    start=CONFIG[\"start_date\"].replace(\"-\", \"\"),\n",
        "    end=CONFIG[\"end_date\"].replace(\"-\", \"\")\n",
        ")\n",
        "\n",
        "AGG = (CONFIG.get(\"SMAP_GRID_AGG\", \"mean\") or \"mean\").lower()\n",
        "MIN_PIX = int(CONFIG.get(\"SMAP_GRID_MIN_PIXELS\", 1))\n",
        "LOG_EVERY = int(CONFIG.get(\"SMAP_LOG_EVERY_N_DAYS\", 20))\n",
        "NEAR_KM = float(CONFIG.get(\"SMAP_NEAREST_FALLBACK_KM\", 6.0))\n",
        "NEAR_M = NEAR_KM * 1000.0\n",
        "\n",
        "if AGG not in {\"mean\", \"median\"}:\n",
        "    raise ValueError(\"SMAP_GRID_AGG must be 'mean' or 'median'\")\n",
        "\n",
        "# -------------------------\n",
        "# 1) Load HydroPulse grid\n",
        "# -------------------------\n",
        "grid = gpd.read_parquet(GRID_PATH)\n",
        "if \"grid_id\" not in grid.columns:\n",
        "    raise KeyError(\"Grid parquet must contain 'grid_id' column.\")\n",
        "if \"geometry\" not in grid.columns:\n",
        "    raise KeyError(\"Grid parquet must contain 'geometry' column.\")\n",
        "\n",
        "# Ensure CRS is EPSG:3310 (your OPS_EPSG)\n",
        "OPS_EPSG = int(CONFIG.get(\"OPS_EPSG\", 3310))\n",
        "if grid.crs is None:\n",
        "    grid = grid.set_crs(epsg=OPS_EPSG)\n",
        "else:\n",
        "    grid = grid.to_crs(epsg=OPS_EPSG)\n",
        "\n",
        "grid = grid[[\"grid_id\", \"geometry\"]].copy()\n",
        "\n",
        "# -------------------------\n",
        "# 2) Build (y,x)->grid_id map (one-time), using cell_lat/cell_lon from a sample .nc4\n",
        "# -------------------------\n",
        "def find_sample_nc4() -> Path:\n",
        "    ok_ext = {\".nc4\", \".nc\", \".cdf\"}\n",
        "    cands = sorted(\n",
        "        p for p in SMAP_ROOT.rglob(\"*\")\n",
        "        if p.is_file()\n",
        "        and \"_done\" not in p.parts\n",
        "        and p.stat().st_size > 0\n",
        "        and p.suffix.lower() in ok_ext\n",
        "    )\n",
        "    if not cands:\n",
        "        raise FileNotFoundError(f\"No .nc/.nc4 files found under {SMAP_ROOT}\")\n",
        "    # pick largest, tends to be real science granule\n",
        "    return max(cands, key=lambda p: p.stat().st_size)\n",
        "\n",
        "def build_gridmap() -> pd.DataFrame:\n",
        "    sample = find_sample_nc4()\n",
        "\n",
        "    # Root group contains cell_lat/cell_lon per your earlier debug\n",
        "    ds_root = xr.open_dataset(sample, engine=\"netcdf4\", decode_times=False, mask_and_scale=True)\n",
        "\n",
        "    # Expect 2D arrays named cell_lat / cell_lon\n",
        "    if \"cell_lat\" not in ds_root.variables or \"cell_lon\" not in ds_root.variables:\n",
        "        raise KeyError(f\"Expected cell_lat/cell_lon in root group. Vars: {list(ds_root.variables.keys())[:50]}\")\n",
        "\n",
        "    lat = ds_root[\"cell_lat\"].values\n",
        "    lon = ds_root[\"cell_lon\"].values\n",
        "\n",
        "    # y/x coordinate arrays exist too; but we use integer indices y,x matching your daily parquet\n",
        "    H, W = lat.shape\n",
        "    yy, xx = np.indices((H, W))\n",
        "\n",
        "    # Flatten\n",
        "    df = pd.DataFrame({\n",
        "        \"y\": yy.ravel().astype(\"int32\"),\n",
        "        \"x\": xx.ravel().astype(\"int32\"),\n",
        "        \"lat\": lat.ravel().astype(\"float64\"),\n",
        "        \"lon\": lon.ravel().astype(\"float64\"),\n",
        "    })\n",
        "    ds_root.close()\n",
        "\n",
        "    # Drop any missing or insane coords\n",
        "    df = df.replace([np.inf, -np.inf], np.nan).dropna(subset=[\"lat\", \"lon\"])\n",
        "    df = df[(df[\"lat\"] >= -90) & (df[\"lat\"] <= 90) & (df[\"lon\"] >= -180) & (df[\"lon\"] <= 180)]\n",
        "\n",
        "    # Make points in WGS84 then project to 3310\n",
        "    gdf = gpd.GeoDataFrame(\n",
        "        df[[\"y\", \"x\"]].copy(),\n",
        "        geometry=gpd.points_from_xy(df[\"lon\"], df[\"lat\"]),\n",
        "        crs=\"EPSG:4326\"\n",
        "    ).to_crs(epsg=OPS_EPSG)\n",
        "\n",
        "    # Spatial join to grid polygons\n",
        "    joined = gpd.sjoin(gdf, grid, how=\"left\", predicate=\"within\")[[\"y\", \"x\", \"grid_id\", \"geometry\"]].copy()\n",
        "\n",
        "    missing = joined[\"grid_id\"].isna().sum()\n",
        "    if missing > 0:\n",
        "        # Fallback: nearest join within radius (meters)\n",
        "        # Keep only missing points for nearest join\n",
        "        miss = joined[joined[\"grid_id\"].isna()].drop(columns=[\"grid_id\"]).copy()\n",
        "        # sjoin_nearest is available in geopandas >=0.10; use max_distance to avoid nonsense matches\n",
        "        nearest = gpd.sjoin_nearest(miss, grid, how=\"left\", max_distance=NEAR_M, distance_col=\"dist_m\")\n",
        "        joined.loc[joined[\"grid_id\"].isna(), \"grid_id\"] = nearest[\"grid_id\"].values\n",
        "\n",
        "    joined = joined.drop(columns=[\"geometry\"])\n",
        "    joined = joined.dropna(subset=[\"grid_id\"]).copy()\n",
        "\n",
        "    # Enforce uniqueness\n",
        "    joined = joined.drop_duplicates(subset=[\"y\", \"x\"])\n",
        "    return joined\n",
        "\n",
        "if GRIDMAP_PATH.exists() and GRIDMAP_PATH.stat().st_size > 0:\n",
        "    gridmap = pd.read_parquet(GRIDMAP_PATH)\n",
        "else:\n",
        "    gridmap = build_gridmap()\n",
        "    tmp = GRIDMAP_PATH.with_suffix(\".tmp.parquet\")\n",
        "    gridmap.to_parquet(tmp, index=False)\n",
        "    os.replace(tmp, GRIDMAP_PATH)\n",
        "\n",
        "if not {\"y\",\"x\",\"grid_id\"}.issubset(set(gridmap.columns)):\n",
        "    raise RuntimeError(f\"Bad gridmap schema: {gridmap.columns}\")\n",
        "\n",
        "print(\"SMAP Cell B starting\")\n",
        "print(\"  Grid:\", GRID_PATH)\n",
        "print(\"  SMAP daily dir:\", SMAP_DAILY_DIR)\n",
        "print(\"  Gridmap:\", GRIDMAP_PATH, f\"(rows={len(gridmap):,})\")\n",
        "print(\"  Shards dir:\", SHARDS_DIR)\n",
        "print(\"  FINAL:\", FINAL_PATH)\n",
        "\n",
        "# -------------------------\n",
        "# 3) Process day shards (resume-safe)\n",
        "# -------------------------\n",
        "daily_files = sorted(SMAP_DAILY_DIR.glob(\"smap_daily_*.parquet\"))\n",
        "if not daily_files:\n",
        "    raise FileNotFoundError(f\"No smap_daily parquet files found in {SMAP_DAILY_DIR}\")\n",
        "\n",
        "written = 0\n",
        "skipped = 0\n",
        "\n",
        "def agg_group(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    # df has grid_id and moisture fields\n",
        "    if AGG == \"mean\":\n",
        "        out = df.groupby(\"grid_id\", as_index=False).agg(\n",
        "            sm_surface=(\"sm_surface\", \"mean\"),\n",
        "            sm_rootzone=(\"sm_rootzone\", \"mean\"),\n",
        "            n_pixels=(\"grid_id\", \"size\"),\n",
        "            n_obs_mean=(\"n_obs\", \"mean\"),\n",
        "        )\n",
        "    else:\n",
        "        out = df.groupby(\"grid_id\", as_index=False).agg(\n",
        "            sm_surface=(\"sm_surface\", \"median\"),\n",
        "            sm_rootzone=(\"sm_rootzone\", \"median\"),\n",
        "            n_pixels=(\"grid_id\", \"size\"),\n",
        "            n_obs_mean=(\"n_obs\", \"mean\"),\n",
        "        )\n",
        "    return out\n",
        "\n",
        "for i, fp in enumerate(daily_files, start=1):\n",
        "    date_str = fp.stem.split(\"_\")[-1]  # YYYY-MM-DD\n",
        "    outp = SHARDS_DIR / f\"smap_grid_{date_str}.parquet\"\n",
        "\n",
        "    if outp.exists() and outp.stat().st_size > 0:\n",
        "        skipped += 1\n",
        "        continue\n",
        "\n",
        "    df = pd.read_parquet(fp)  # columns: date_utc,y,x,sm_surface,sm_rootzone,n_obs\n",
        "    # Join mapping\n",
        "    df = df.merge(gridmap, on=[\"y\",\"x\"], how=\"inner\")\n",
        "\n",
        "    # Aggregate to grid\n",
        "    g = agg_group(df)\n",
        "\n",
        "    # Apply min pixels filter\n",
        "    g = g[g[\"n_pixels\"] >= MIN_PIX].copy()\n",
        "    g.insert(1, \"date_utc\", pd.Timestamp(date_str, tz=\"UTC\"))\n",
        "\n",
        "    tmp = outp.with_suffix(\".tmp.parquet\")\n",
        "    g.to_parquet(tmp, index=False)\n",
        "    os.replace(tmp, outp)\n",
        "\n",
        "    written += 1\n",
        "    if (written % LOG_EVERY) == 0 or i == len(daily_files):\n",
        "        print(f\"  day {i}/{len(daily_files)} | wrote={written} skipped={skipped} | last={date_str} | rows={len(g):,}\")\n",
        "\n",
        "print(\"SMAP Cell B shards complete\")\n",
        "print(\"  wrote  :\", written)\n",
        "print(\"  skipped:\", skipped)\n",
        "\n",
        "# -------------------------\n",
        "# 4) Optional: stitch shards into one final parquet (resume-safe)\n",
        "# -------------------------\n",
        "if FINAL_PATH.exists() and FINAL_PATH.stat().st_size > 0:\n",
        "    print(\"[SKIP] Final exists:\", FINAL_PATH)\n",
        "else:\n",
        "    shard_files = sorted(SHARDS_DIR.glob(\"smap_grid_*.parquet\"))\n",
        "    if not shard_files:\n",
        "        raise RuntimeError(\"No SMAP grid shards found to stitch.\")\n",
        "    parts = [pd.read_parquet(p) for p in shard_files]\n",
        "    final = pd.concat(parts, ignore_index=True)\n",
        "    tmp = FINAL_PATH.with_suffix(\".tmp.parquet\")\n",
        "    final.to_parquet(tmp, index=False)\n",
        "    os.replace(tmp, FINAL_PATH)\n",
        "    print(\"Saved FINAL:\", FINAL_PATH, \"rows=\", len(final))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91087cb0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================\n",
        "# SNOTEL — Cell A\n",
        "# Parse, normalize, and sanity-check\n",
        "# First, we download data manually from https://wcc.sc.egov.usda.gov/reportGenerator/\n",
        "# Parameters are: click on \"Advanced search\", select Network - SNOTEL, all stations in CA - this is about 500 stations,\n",
        "# date range 1991-01-01 to 2026-01-12 (most recent available as of writing),\n",
        "# and select a few variables\n",
        "# The buildable report URL looks like this: \n",
        "# https://wcc.sc.egov.usda.gov/reportGenerator/view_csv/customMultiTimeSeriesGroupByStationReport/daily/start_of_period/county=%2522Alameda%2522,%2522Alpine%2522,%2522Amador%2522,%2522Butte%2522,%2522Calaveras%2522,%2522Colusa%2522,%2522Contra%2520Costa%2522,%2522Del%2520Norte%2522,%2522El%2520Dorado%2522,%2522Fresno%2522,%2522Glenn%2522,%2522Humboldt%2522,%2522Imperial%2522,%2522Inyo%2522,%2522Kern%2522,%2522Kings%2522,%2522Lake%2522,%2522Lassen%2522,%2522Los%2520Angeles%2522,%2522Madera%2522,%2522Marin%2522,%2522Mariposa%2522,%2522Mendocino%2522,%2522Merced%2522,%2522Modoc%2522,%2522Mono%2522,%2522Monterey%2522,%2522Napa%2522,%2522Nevada%2522,%2522Orange%2522,%2522Placer%2522,%2522Plumas%2522,%2522Riverside%2522,%2522Sacramento%2522,%2522San%2520Benito%2522,%2522San%2520Bernardino%2522,%2522San%2520Diego%2522,%2522San%2520Francisco%2522,%2522San%2520Joaquin%2522,%2522San%2520Luis%2520Obispo%2522,%2522San%2520Mateo%2522,%2522Santa%2520Barbara%2522,%2522Santa%2520Clara%2522,%2522Santa%2520Cruz%2522,%2522Shasta%2522,%2522Sierra%2522,%2522Siskiyou%2522,%2522Solano%2522,%2522Sonoma%2522,%2522Stanislaus%2522,%2522Sutter%2522,%2522Tehama%2522,%2522Trinity%2522,%2522Tulare%2522,%2522Tuolumne%2522,%2522Ventura%2522,%2522Yolo%2522,%2522Yuba%2522,%2522UNKNOWN%2522%2520AND%2520network=%2522SNTL%2522%2520AND%2520outServiceDate=%25222100-01-01%2522%257Cname/1991-01-01,2026-12-31/stationId,name,WTEQ::value,SNWD::value,PREC::value,PRCP:-2:value,PRCP::value,TAVG::value,TMAX::value,TMIN::value?fitToScreen=false\n",
        "# For making more changes, one can go to the main URL, experiment with parameters, and then copy the resulting CSV URL.\n",
        "# Manually save the file as \"manual/snotel/snotel-19910101-20260112.txt\"\n",
        "# and then of course, run the below code to parse and normalize it.\n",
        "# ============================\n",
        "\n",
        "# ============================\n",
        "# SNOTEL — Cell A (FINAL, resolve_out_path)\n",
        "# - Reads one wide NRCS report export (1991 → present)\n",
        "# - Melts to long tidy format (station_id × date × variable)\n",
        "# - Prefers non \"-2in\" variant when duplicates exist\n",
        "# - Writes parquet directly into results/ (out_dir)\n",
        "# ============================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from pathlib import Path\n",
        "\n",
        "# ---------- config-driven paths ----------\n",
        "OUT_DIR = Path(CONFIG[\"out_dir\"])\n",
        "SNOTEL_MANUAL_DIR = Path(resolve_out_path(CONFIG[\"SNOTEL_MANUAL_DIR\"]))         # e.g. \"manual/snotel\"\n",
        "RAW_FILE = SNOTEL_MANUAL_DIR / Path(CONFIG[\"SNOTEL_RAW_FILENAME\"]).name\n",
        "OUT_PARQUET = OUT_DIR / CONFIG[\"SNOTEL_DAILY_LONG_PARQUET_NAME\"]               # e.g. \"snotel_daily_long.parquet\"\n",
        "\n",
        "print(\"SNOTEL Cell A starting\")\n",
        "print(\"  out_dir   :\", OUT_DIR)\n",
        "print(\"  raw file  :\", RAW_FILE)\n",
        "print(\"  out parquet:\", OUT_PARQUET)\n",
        "\n",
        "if not RAW_FILE.exists():\n",
        "    raise FileNotFoundError(f\"Missing SNOTEL raw file: {RAW_FILE}\")\n",
        "\n",
        "# ---------- read raw (robust: skip WCC metadata preamble) ----------\n",
        "def find_header_row(path: Path, max_lines: int = 500) -> int:\n",
        "    \"\"\"\n",
        "    Find the first line index (0-based) that looks like the actual CSV header.\n",
        "    WCC report exports often include a preamble before the header.\n",
        "    \"\"\"\n",
        "    with open(path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
        "        for i, line in enumerate(f):\n",
        "            if i >= max_lines:\n",
        "                break\n",
        "            s = line.strip().lower()\n",
        "            # header usually starts with 'date' and contains at least one '(' stationId ')'\n",
        "            if s.startswith(\"date\") and \"(\" in s and \")\" in s and \",\" in s:\n",
        "                return i\n",
        "            # sometimes it's literally just 'date,' without station ids on the same line (rare)\n",
        "            if s.startswith(\"date,\"):\n",
        "                return i\n",
        "    raise ValueError(f\"Could not find header row in first {max_lines} lines of {path}\")\n",
        "\n",
        "header_row = find_header_row(RAW_FILE)\n",
        "print(f\"SNOTEL: detected CSV header at line {header_row+1} (1-based)\")\n",
        "\n",
        "df_raw = pd.read_csv(\n",
        "    RAW_FILE,\n",
        "    header=header_row,\n",
        "    low_memory=False,\n",
        "    encoding=\"utf-8\",\n",
        "    encoding_errors=\"replace\",\n",
        ")\n",
        "# First column is date in this NRCS export\n",
        "df_raw.rename(columns={df_raw.columns[0]: \"date\"}, inplace=True)\n",
        "df_raw[\"date\"] = pd.to_datetime(df_raw[\"date\"], errors=\"coerce\", utc=True)\n",
        "if df_raw[\"date\"].isna().any():\n",
        "    bad = df_raw[df_raw[\"date\"].isna()].head(5)\n",
        "    raise ValueError(f\"Found NaT dates after parsing date column; sample:\\n{bad}\")\n",
        "\n",
        "# ---------- helper to parse column names ----------\n",
        "def parse_col(col: str):\n",
        "    \"\"\"\n",
        "    Expected column pattern:\n",
        "      '<station name> (<stationId>) <variable label...>'\n",
        "    Returns (station_name, station_id, variable_key, is_minus2_variant) or None\n",
        "    \"\"\"\n",
        "    m = re.match(r\"(.+?)\\s+\\((\\d+)\\)\\s+(.+)\", col)\n",
        "    if not m:\n",
        "        return None\n",
        "\n",
        "    station_name = m.group(1).strip()\n",
        "    station_id = int(m.group(2))\n",
        "    rest = m.group(3)\n",
        "\n",
        "    # Canonical variables\n",
        "    if \"Snow Water Equivalent\" in rest:\n",
        "        var = \"swe_in\"\n",
        "    elif \"Snow Depth\" in rest:\n",
        "        var = \"snow_depth_in\"\n",
        "    elif \"Precipitation Increment\" in rest:\n",
        "        var = \"precip_increment_in\"\n",
        "    elif \"Precipitation Accumulation\" in rest:\n",
        "        var = \"precip_accum_in\"\n",
        "    elif \"Air Temperature Average\" in rest:\n",
        "        var = \"tavg_f\"\n",
        "    elif \"Air Temperature Maximum\" in rest:\n",
        "        var = \"tmax_f\"\n",
        "    elif \"Air Temperature Minimum\" in rest:\n",
        "        var = \"tmin_f\"\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "    is_minus2 = \"-2in\" in rest\n",
        "    return station_name, station_id, var, is_minus2\n",
        "\n",
        "# ---------- melt wide -> long ----------\n",
        "records = []\n",
        "for col in df_raw.columns[1:]:\n",
        "    parsed = parse_col(col)\n",
        "    if parsed is None:\n",
        "        continue\n",
        "\n",
        "    station_name, station_id, var, is_minus2 = parsed\n",
        "\n",
        "    s = pd.to_numeric(df_raw[col], errors=\"coerce\").astype(\"float32\")\n",
        "\n",
        "    # Basic validity cleaning (keep conservative; do more later if needed)\n",
        "    if var.startswith(\"t\"):\n",
        "        # Fahrenheit: discard extreme negatives (sentinels)\n",
        "        s = s.where(s > -50)\n",
        "    else:\n",
        "        # SWE/Depth/Precip should not be negative\n",
        "        s = s.where(s >= 0)\n",
        "\n",
        "    records.append(pd.DataFrame({\n",
        "        \"date\": df_raw[\"date\"],\n",
        "        \"station_id\": station_id,\n",
        "        \"station_name\": station_name,\n",
        "        \"variable\": var,\n",
        "        \"value\": s,\n",
        "        \"minus2_variant\": is_minus2\n",
        "    }))\n",
        "\n",
        "if not records:\n",
        "    raise RuntimeError(\n",
        "        \"Parsed zero SNOTEL data columns. The column naming pattern likely changed. \"\n",
        "        \"Inspect df_raw.columns[:50] to update parse_col().\"\n",
        "    )\n",
        "\n",
        "df_long = pd.concat(records, ignore_index=True)\n",
        "\n",
        "# ---------- resolve duplicate variants (prefer non '-2in') ----------\n",
        "df_long.sort_values(\n",
        "    by=[\"date\", \"station_id\", \"variable\", \"minus2_variant\"],  # False first -> non -2in preferred\n",
        "    inplace=True\n",
        ")\n",
        "\n",
        "df_long = (\n",
        "    df_long\n",
        "    .drop_duplicates(subset=[\"date\", \"station_id\", \"variable\"], keep=\"first\")\n",
        "    .drop(columns=[\"minus2_variant\"])\n",
        ")\n",
        "\n",
        "df_long[\"source\"] = \"SNOTEL\"\n",
        "\n",
        "# ---------- sanity summary ----------\n",
        "print(\"SNOTEL Cell A summary\")\n",
        "print(\"  rows      :\", len(df_long))\n",
        "print(\"  stations  :\", df_long[\"station_id\"].nunique())\n",
        "print(\"  variables :\", sorted(df_long[\"variable\"].unique()))\n",
        "print(\"  date min  :\", df_long[\"date\"].min())\n",
        "print(\"  date max  :\", df_long[\"date\"].max())\n",
        "\n",
        "# ---------- write (atomic) ----------\n",
        "tmp_path = OUT_PARQUET.with_suffix(\".tmp.parquet\")\n",
        "df_long.to_parquet(tmp_path, index=False)\n",
        "tmp_path.replace(OUT_PARQUET)\n",
        "\n",
        "print(\"Saved:\", OUT_PARQUET)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e24b138b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================\n",
        "# SNOTEL — Station metadata (AWDB)\n",
        "# Builds station_id -> lat/lon lookup for gridding\n",
        "# ============================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from pathlib import Path\n",
        "from urllib.parse import quote\n",
        "\n",
        "OUT_DIR = Path(CONFIG[\"out_dir\"])\n",
        "SNOTEL_LONG_PATH = OUT_DIR / CONFIG.get(\"SNOTEL_DAILY_LONG_PARQUET_NAME\", \"snotel_daily_long.parquet\")\n",
        "META_OUT = OUT_DIR / CONFIG.get(\"SNOTEL_STATION_META_PARQUET_NAME\", \"snotel_station_metadata.parquet\")\n",
        "\n",
        "AWDB_URL_TMPL = CONFIG.get(\n",
        "    \"AWDB_META_URL_TEMPLATE\",\n",
        "    \"https://wcc.sc.egov.usda.gov/awdbRestApi/services/v1/stations?stationTriplets={triplets}&elements=\"\n",
        ")\n",
        "\n",
        "if not SNOTEL_LONG_PATH.exists():\n",
        "    raise FileNotFoundError(f\"Missing SNOTEL long parquet: {SNOTEL_LONG_PATH}\")\n",
        "\n",
        "df_long = pd.read_parquet(SNOTEL_LONG_PATH, columns=[\"station_id\", \"station_name\"])\n",
        "station_ids = sorted(df_long[\"station_id\"].dropna().astype(int).unique().tolist())\n",
        "\n",
        "print(\"SNOTEL station metadata starting\")\n",
        "print(\"  stations:\", len(station_ids))\n",
        "print(\"  out     :\", META_OUT)\n",
        "\n",
        "# If cached, load and only fetch missing\n",
        "if META_OUT.exists():\n",
        "    meta = pd.read_parquet(META_OUT)\n",
        "    have = set(meta[\"station_id\"].astype(int).unique().tolist())\n",
        "    missing = [sid for sid in station_ids if sid not in have]\n",
        "    print(f\"  cached  : {len(have)} stations | missing: {len(missing)}\")\n",
        "else:\n",
        "    meta = pd.DataFrame()\n",
        "    missing = station_ids\n",
        "\n",
        "def fetch_awdb(triplets: list[str]) -> pd.DataFrame:\n",
        "    # AWDB expects comma-separated triplets like \"1234:CA:SNTL,5678:CA:SNTL\"\n",
        "    trip_str = \",\".join(triplets)\n",
        "    url = AWDB_URL_TMPL.format(triplets=quote(trip_str, safe=\",:\"))\n",
        "    r = requests.get(url, timeout=120)\n",
        "    r.raise_for_status()\n",
        "    js = r.json()\n",
        "    # AWDB returns {'stations': [...]} or direct list depending on endpoint version\n",
        "    rows = js.get(\"stations\", js) if isinstance(js, dict) else js\n",
        "    return pd.json_normalize(rows)\n",
        "\n",
        "rows = []\n",
        "batch_size = 80  # conservative; avoids long URLs\n",
        "for i in range(0, len(missing), batch_size):\n",
        "    batch = missing[i:i+batch_size]\n",
        "    triplets = [f\"{sid}:CA:SNTL\" for sid in batch]  # CA SNOTEL triplet format\n",
        "    try:\n",
        "        got = fetch_awdb(triplets)\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"AWDB metadata fetch failed for batch {i//batch_size}: {e}\")\n",
        "    rows.append(got)\n",
        "    if (i//batch_size + 1) % 5 == 0 or (i + batch_size) >= len(missing):\n",
        "        print(f\"  fetched batch {i//batch_size + 1} | total rows so far: {sum(len(x) for x in rows):,}\")\n",
        "\n",
        "new = pd.concat(rows, ignore_index=True) if rows else pd.DataFrame()\n",
        "\n",
        "# Normalize / keep only what we need; AWDB fields can vary slightly.\n",
        "# We key on 'stationTriplet' and/or 'stationId' when present.\n",
        "if not new.empty:\n",
        "    # stationId sometimes present; otherwise parse from stationTriplet\n",
        "    if \"stationId\" in new.columns:\n",
        "        new[\"station_id\"] = pd.to_numeric(new[\"stationId\"], errors=\"coerce\")\n",
        "    elif \"stationTriplet\" in new.columns:\n",
        "        new[\"station_id\"] = pd.to_numeric(new[\"stationTriplet\"].str.split(\":\").str[0], errors=\"coerce\")\n",
        "    else:\n",
        "        raise KeyError(f\"AWDB response missing stationId/stationTriplet; columns={list(new.columns)[:50]}\")\n",
        "\n",
        "    # lat/lon fields\n",
        "    lat_col = \"latitude\" if \"latitude\" in new.columns else (\"lat\" if \"lat\" in new.columns else None)\n",
        "    lon_col = \"longitude\" if \"longitude\" in new.columns else (\"lon\" if \"lon\" in new.columns else None)\n",
        "    if lat_col is None or lon_col is None:\n",
        "        raise KeyError(f\"AWDB response missing lat/lon columns; columns={list(new.columns)[:50]}\")\n",
        "\n",
        "    out = pd.DataFrame({\n",
        "        \"station_id\": new[\"station_id\"].astype(\"Int64\"),\n",
        "        \"lat\": pd.to_numeric(new[lat_col], errors=\"coerce\"),\n",
        "        \"lon\": pd.to_numeric(new[lon_col], errors=\"coerce\"),\n",
        "        \"elev_m\": pd.to_numeric(new.get(\"elevation\", np.nan), errors=\"coerce\"),\n",
        "        \"state\": new.get(\"state\", \"CA\"),\n",
        "        \"station_name_meta\": new.get(\"name\", pd.NA),\n",
        "        \"station_triplet\": new.get(\"stationTriplet\", pd.NA),\n",
        "    }).dropna(subset=[\"station_id\"])\n",
        "else:\n",
        "    out = pd.DataFrame(columns=[\"station_id\",\"lat\",\"lon\",\"elev_m\",\"state\",\"station_name_meta\",\"station_triplet\"])\n",
        "\n",
        "# Merge with existing cache\n",
        "if meta.empty:\n",
        "    meta2 = out\n",
        "else:\n",
        "    meta2 = pd.concat([meta, out], ignore_index=True)\n",
        "    meta2 = meta2.drop_duplicates(subset=[\"station_id\"], keep=\"last\")\n",
        "\n",
        "# Basic sanity\n",
        "meta2[\"station_id\"] = meta2[\"station_id\"].astype(int)\n",
        "ok = meta2.dropna(subset=[\"lat\",\"lon\"])\n",
        "print(\"  meta rows:\", len(meta2), \"| with lat/lon:\", len(ok))\n",
        "\n",
        "tmp = META_OUT.with_suffix(\".tmp.parquet\")\n",
        "meta2.to_parquet(tmp, index=False)\n",
        "tmp.replace(META_OUT)\n",
        "print(\"Saved:\", META_OUT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a1e8553",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================\n",
        "# SNOTEL — Cell B\n",
        "# Grid stations to 3km grid; write daily shards; stitch final parquet (analysis window)\n",
        "# ============================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import geopandas as gpd\n",
        "\n",
        "# ---------- paths ----------\n",
        "OUT_DIR = Path(CONFIG[\"out_dir\"])\n",
        "\n",
        "GRID_PATH = Path(resolve_out_path(CONFIG.get(\"GRID_FILENAME\", \"grid_3000m_CA_epsg3310.parquet\")))\n",
        "SNOTEL_LONG_PATH = OUT_DIR / CONFIG.get(\"SNOTEL_DAILY_LONG_PARQUET_NAME\", \"snotel_daily_long.parquet\")\n",
        "\n",
        "GRIDMAP_PATH = OUT_DIR / CONFIG.get(\"SNOTEL_GRIDMAP_PARQUET_NAME\", \"snotel_station_to_grid_3310.parquet\")\n",
        "SHARDS_DIR = OUT_DIR / CONFIG.get(\"SNOTEL_GRID_SHARDS_DIRNAME\", \"derived/snotel_grid_shards\")\n",
        "SHARDS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "start = pd.to_datetime(CONFIG[\"start_date\"]).tz_localize(\"UTC\")\n",
        "end   = pd.to_datetime(CONFIG[\"end_date\"]).tz_localize(\"UTC\")\n",
        "\n",
        "final_name_tmpl = CONFIG.get(\n",
        "    \"SNOTEL_DAILY_GRID_PARQUET_NAME\",\n",
        "    \"snotel_daily_grid_CA_3000m_epsg3310_{start}_{end}.parquet\"\n",
        ")\n",
        "FINAL_PATH = OUT_DIR / final_name_tmpl.format(\n",
        "    start=start.strftime(\"%Y%m%d\"),\n",
        "    end=end.strftime(\"%Y%m%d\")\n",
        ")\n",
        "\n",
        "print(\"SNOTEL Cell B starting\")\n",
        "print(\"  Grid     :\", GRID_PATH)\n",
        "print(\"  SNOTEL A :\", SNOTEL_LONG_PATH)\n",
        "print(\"  Gridmap  :\", GRIDMAP_PATH)\n",
        "print(\"  Shards   :\", SHARDS_DIR)\n",
        "print(\"  FINAL    :\", FINAL_PATH)\n",
        "\n",
        "if not GRID_PATH.exists():\n",
        "    raise FileNotFoundError(f\"Missing grid parquet: {GRID_PATH}\")\n",
        "if not SNOTEL_LONG_PATH.exists():\n",
        "    raise FileNotFoundError(f\"Missing SNOTEL long parquet (Cell A output): {SNOTEL_LONG_PATH}\")\n",
        "\n",
        "# ---------- load grid ----------\n",
        "ggrid = gpd.read_parquet(GRID_PATH)\n",
        "if \"grid_id\" not in ggrid.columns:\n",
        "    raise KeyError(\"Grid file must contain grid_id\")\n",
        "if ggrid.crs is None:\n",
        "    raise ValueError(\"Grid GeoDataFrame missing CRS\")\n",
        "# ensure EPSG 3310 ops\n",
        "ggrid = ggrid.to_crs(epsg=int(CONFIG.get(\"OPS_EPSG\", 3310)))\n",
        "\n",
        "# precompute centroids for nearest mapping\n",
        "ggrid_cent = ggrid.copy()\n",
        "ggrid_cent[\"geometry\"] = ggrid_cent.geometry.centroid\n",
        "ggrid_cent = ggrid_cent[[\"grid_id\", \"geometry\"]]\n",
        "\n",
        "# ---------- load SNOTEL long ----------\n",
        "df = pd.read_parquet(SNOTEL_LONG_PATH)\n",
        "\n",
        "# Slice to analysis window early (massive speedup)\n",
        "df = df[(df[\"date\"] >= start) & (df[\"date\"] <= end)].copy()\n",
        "if df.empty:\n",
        "    raise ValueError(\"SNOTEL long table has no rows in analysis window. Check date parsing/timezones.\")\n",
        "\n",
        "META_PATH = OUT_DIR / CONFIG.get(\"SNOTEL_STATION_META_PARQUET_NAME\", \"snotel_station_metadata.parquet\")\n",
        "if not META_PATH.exists():\n",
        "    raise FileNotFoundError(f\"Missing station metadata parquet: {META_PATH}. Run the SNOTEL metadata cell first.\")\n",
        "\n",
        "meta = pd.read_parquet(META_PATH)\n",
        "meta[\"station_id\"] = meta[\"station_id\"].astype(int)\n",
        "\n",
        "# Build station table from metadata + names from long table\n",
        "stations = (\n",
        "    df[[\"station_id\", \"station_name\"]]\n",
        "    .drop_duplicates(subset=[\"station_id\"])\n",
        "    .merge(meta[[\"station_id\",\"lat\",\"lon\"]], on=\"station_id\", how=\"left\")\n",
        ")\n",
        "\n",
        "missing_ll = stations[\"lat\"].isna().sum()\n",
        "if missing_ll:\n",
        "    print(f\"[WARN] {missing_ll} stations missing lat/lon; they will be dropped for gridding.\")\n",
        "stations = stations.dropna(subset=[\"lat\",\"lon\"])\n",
        "\n",
        "\n",
        "# ---------- station -> grid mapping (cached) ----------\n",
        "if GRIDMAP_PATH.exists():\n",
        "    gridmap = pd.read_parquet(GRIDMAP_PATH)\n",
        "    print(f\"Loaded existing gridmap ({len(gridmap):,} stations)\")\n",
        "else:\n",
        "    gstations = gpd.GeoDataFrame(\n",
        "        stations,\n",
        "        geometry=gpd.points_from_xy(stations[\"lon\"], stations[\"lat\"]),\n",
        "        crs=\"EPSG:4326\"\n",
        "    ).to_crs(ggrid_cent.crs)\n",
        "\n",
        "    # nearest grid centroid\n",
        "    joined = gpd.sjoin_nearest(\n",
        "        gstations,\n",
        "        ggrid_cent,\n",
        "        how=\"left\",\n",
        "        distance_col=\"dist_m\"\n",
        "    )\n",
        "\n",
        "    gridmap = joined[[\"station_id\", \"grid_id\", \"dist_m\"]].copy()\n",
        "    gridmap.to_parquet(GRIDMAP_PATH, index=False)\n",
        "    print(f\"Saved gridmap ({len(gridmap):,} stations) -> {GRIDMAP_PATH}\")\n",
        "\n",
        "# attach grid_id to all records\n",
        "df = df.merge(gridmap[[\"station_id\", \"grid_id\"]], on=\"station_id\", how=\"inner\")\n",
        "if df.empty:\n",
        "    raise ValueError(\"After station->grid join, SNOTEL has zero rows. Check station_id alignment.\")\n",
        "\n",
        "# ---------- pick variables of interest (you can expand later) ----------\n",
        "# Keep everything for now; but when aggregating, we compute grid-level stats per variable.\n",
        "keep_vars = [\n",
        "    \"precip_increment_in\",  # primary precip\n",
        "    \"swe_in\",\n",
        "    \"snow_depth_in\",\n",
        "    \"tavg_f\",\n",
        "    \"tmax_f\",\n",
        "    \"tmin_f\",\n",
        "    \"precip_accum_in\",      # kept but not primary\n",
        "]\n",
        "df = df[df[\"variable\"].isin(keep_vars)].copy()\n",
        "\n",
        "# ---------- aggregate per grid_id x day x variable ----------\n",
        "# We'll compute mean across stations in same grid cell, plus count.\n",
        "df[\"date_day\"] = df[\"date\"].dt.floor(\"D\")\n",
        "\n",
        "all_days = pd.date_range(start=start.floor(\"D\"), end=end.floor(\"D\"), freq=\"D\", tz=\"UTC\")\n",
        "\n",
        "wrote = skipped = 0\n",
        "for i, day in enumerate(all_days, start=1):\n",
        "    day_tag = day.strftime(\"%Y-%m-%d\")\n",
        "    shard_path = SHARDS_DIR / f\"snotel_grid_{day_tag}.parquet\"\n",
        "\n",
        "    if shard_path.exists():\n",
        "        skipped += 1\n",
        "        continue\n",
        "\n",
        "    dday = df[df[\"date_day\"] == day]\n",
        "    if dday.empty:\n",
        "        # still write an empty shard to mark completion (resume-safe)\n",
        "        empty = pd.DataFrame(columns=[\"grid_id\", \"date\", \"variable\", \"value_mean\", \"n_stations\"])\n",
        "        empty.to_parquet(shard_path, index=False)\n",
        "        wrote += 1\n",
        "        continue\n",
        "\n",
        "    agg = (\n",
        "        dday.groupby([\"grid_id\", \"date_day\", \"variable\"], as_index=False)\n",
        "        .agg(\n",
        "            value_mean=(\"value\", \"mean\"),\n",
        "            n_stations=(\"value\", \"count\")\n",
        "        )\n",
        "        .rename(columns={\"date_day\": \"date\"})\n",
        "    )\n",
        "\n",
        "    # Optional: pivot to wide per day for easier merges later\n",
        "    wide = agg.pivot_table(\n",
        "        index=[\"grid_id\", \"date\"],\n",
        "        columns=\"variable\",\n",
        "        values=\"value_mean\"\n",
        "    ).reset_index()\n",
        "\n",
        "    # attach station counts in wide form too\n",
        "    counts = agg.pivot_table(\n",
        "        index=[\"grid_id\", \"date\"],\n",
        "        columns=\"variable\",\n",
        "        values=\"n_stations\"\n",
        "    ).add_prefix(\"n_\").reset_index()\n",
        "\n",
        "    out = wide.merge(counts, on=[\"grid_id\", \"date\"], how=\"left\")\n",
        "\n",
        "    out.to_parquet(shard_path, index=False)\n",
        "    wrote += 1\n",
        "\n",
        "    if i % 20 == 0 or i == len(all_days):\n",
        "        print(f\"  day {i}/{len(all_days)} | wrote={wrote} skipped={skipped} | last={day_tag} | rows={len(out):,}\")\n",
        "\n",
        "print(\"SNOTEL Cell B shards complete\")\n",
        "print(\"  wrote  :\", wrote)\n",
        "print(\"  skipped:\", skipped)\n",
        "\n",
        "# ---------- stitch final ----------\n",
        "shards = sorted(SHARDS_DIR.glob(\"snotel_grid_*.parquet\"))\n",
        "parts = [pd.read_parquet(p) for p in shards]\n",
        "final = pd.concat(parts, ignore_index=True)\n",
        "\n",
        "# Some days may be empty shards; drop them\n",
        "final = final.dropna(subset=[\"grid_id\", \"date\"], how=\"any\")\n",
        "\n",
        "final.to_parquet(FINAL_PATH, index=False)\n",
        "print(\"Saved FINAL:\", FINAL_PATH, \"rows=\", len(final))\n",
        "print(final.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "b088b5b9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ERA5 master cell starting\n",
            "  out_dir : /Users/Shared/blueleaflabs/hydropulse/results\n",
            "  RAW_DIR : /Users/Shared/blueleaflabs/hydropulse/results/era5_raw\n",
            "  CLEAN_DIR (future use): /Users/Shared/blueleaflabs/hydropulse/results/era5_clean\n",
            "  AREA   : [42.0095, -124.482, 32.5343, -114.1315] (N,W,S,E)\n",
            "  Daily dataset  : derived-era5-land-daily-statistics\n",
            "  Monthly dataset: reanalysis-era5-land-monthly-means\n",
            "  Daily vars: ['2m_temperature', 'snow_depth_water_equivalent', 'volumetric_soil_water_layer_1']\n",
            "  Analysis window : 2024-06-01 → 2024-10-31\n",
            "  Baseline window : 1991-01-01 → 2025-12-31\n",
            "\n",
            "ERA5 daily downloads:\n",
            "[DL] era5l_daily_CA_202406_mean.nc  (dataset=derived-era5-land-daily-statistics)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-01-12 21:22:43,595 INFO Request ID is b051bcd7-774d-48a9-b7ae-0cbf189afccc\n",
            "2026-01-12 21:22:43,808 INFO status has been updated to accepted\n",
            "2026-01-12 21:22:53,032 INFO status has been updated to running\n",
            "2026-01-12 21:31:11,022 INFO status has been updated to successful\n",
            "                                                                                         \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[OK] era5l_daily_CA_202406_mean.nc (1.3 MB)\n",
            "[DL] era5l_daily_CA_202407_mean.nc  (dataset=derived-era5-land-daily-statistics)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-01-12 21:31:15,793 INFO Request ID is bbbf1732-28ba-472d-b58e-c95ef1053c1c\n",
            "2026-01-12 21:31:16,038 INFO status has been updated to accepted\n",
            "2026-01-12 21:31:30,700 INFO status has been updated to running\n",
            "2026-01-12 21:33:12,511 INFO status has been updated to accepted\n",
            "2026-01-12 21:34:11,352 INFO status has been updated to running\n",
            "2026-01-12 21:39:42,795 INFO status has been updated to successful\n",
            "                                                                                         \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[OK] era5l_daily_CA_202407_mean.nc (1.3 MB)\n",
            "[DL] era5l_daily_CA_202408_mean.nc  (dataset=derived-era5-land-daily-statistics)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-01-12 21:39:47,470 INFO Request ID is 31f80d73-d5bb-454a-bbbc-3da0b461ca00\n",
            "2026-01-12 21:39:47,726 INFO status has been updated to accepted\n",
            "2026-01-12 21:40:10,045 INFO status has been updated to running\n",
            "2026-01-12 21:46:11,283 INFO status has been updated to successful\n",
            "                                                                                         \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[OK] era5l_daily_CA_202408_mean.nc (1.3 MB)\n",
            "[DL] era5l_daily_CA_202409_mean.nc  (dataset=derived-era5-land-daily-statistics)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-01-12 21:46:18,889 INFO Request ID is 1babde93-769b-463c-aacd-43016c6ff766\n",
            "2026-01-12 21:46:19,195 INFO status has been updated to accepted\n",
            "2026-01-12 21:47:10,969 INFO status has been updated to running\n",
            "2026-01-12 21:52:44,452 INFO status has been updated to successful\n",
            "                                                                                         \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[OK] era5l_daily_CA_202409_mean.nc (1.3 MB)\n",
            "[DL] era5l_daily_CA_202410_mean.nc  (dataset=derived-era5-land-daily-statistics)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-01-12 21:52:48,399 INFO Request ID is a1c8e289-73e1-4321-a919-adf95836e43c\n",
            "2026-01-12 21:52:48,593 INFO status has been updated to accepted\n",
            "2026-01-12 21:53:22,361 INFO status has been updated to running\n",
            "2026-01-12 21:59:13,496 INFO status has been updated to successful\n",
            "                                                                                         \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[OK] era5l_daily_CA_202410_mean.nc (1.4 MB)\n",
            "Daily summary: {'downloaded': 5, 'skipped': 0}\n",
            "\n",
            "ERA5 monthly baseline downloads:\n",
            "[DL] era5l_monthly_CA_1991.nc  (dataset=reanalysis-era5-land-monthly-means)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-01-12 21:59:19,274 INFO Request ID is b6ff0ac1-869c-4d5d-8eb0-540dba5fa8d0\n",
            "2026-01-12 21:59:19,520 INFO status has been updated to accepted\n",
            "2026-01-12 21:59:36,293 INFO status has been updated to running\n",
            "2026-01-12 21:59:44,155 INFO status has been updated to accepted\n",
            "2026-01-12 21:59:55,751 INFO status has been updated to successful\n",
            "                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[WARN] attempt 1/6 failed: Download finished but output looks too small: /Users/Shared/blueleaflabs/hydropulse/results/era5_raw/era5l_monthly_CA_1991.nc\n",
            "       sleeping 25s, then retrying...\n",
            "[DL] era5l_monthly_CA_1991.nc  (dataset=reanalysis-era5-land-monthly-means)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-01-12 22:00:24,843 INFO Request ID is 3250a60c-3a41-47c3-ac4c-12e532db3215\n",
            "2026-01-12 22:00:26,914 INFO status has been updated to accepted\n",
            "2026-01-12 22:00:36,906 INFO status has been updated to successful\n",
            "                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[WARN] attempt 2/6 failed: Download finished but output looks too small: /Users/Shared/blueleaflabs/hydropulse/results/era5_raw/era5l_monthly_CA_1991.nc\n",
            "       sleeping 50s, then retrying...\n",
            "[DL] era5l_monthly_CA_1991.nc  (dataset=reanalysis-era5-land-monthly-means)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-01-12 22:01:31,808 INFO Request ID is ba899ca7-de1d-4eb5-b891-b8bc5007dfd0\n",
            "2026-01-12 22:01:32,199 INFO status has been updated to accepted\n",
            "2026-01-12 22:01:46,562 INFO status has been updated to successful\n",
            "                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[WARN] attempt 3/6 failed: Download finished but output looks too small: /Users/Shared/blueleaflabs/hydropulse/results/era5_raw/era5l_monthly_CA_1991.nc\n",
            "       sleeping 75s, then retrying...\n",
            "[DL] era5l_monthly_CA_1991.nc  (dataset=reanalysis-era5-land-monthly-means)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-01-12 22:03:06,929 INFO Request ID is a4b35a59-8283-467b-aa79-87b4b7518dd5\n",
            "2026-01-12 22:03:07,184 INFO status has been updated to accepted\n",
            "2026-01-12 22:03:30,281 INFO status has been updated to successful\n",
            "                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[WARN] attempt 4/6 failed: Download finished but output looks too small: /Users/Shared/blueleaflabs/hydropulse/results/era5_raw/era5l_monthly_CA_1991.nc\n",
            "       sleeping 100s, then retrying...\n",
            "[DL] era5l_monthly_CA_1991.nc  (dataset=reanalysis-era5-land-monthly-means)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-01-12 22:05:16,326 INFO Request ID is 5d657024-59f0-458f-ac01-4122fd2c378f\n",
            "2026-01-12 22:05:16,867 INFO status has been updated to accepted\n",
            "2026-01-12 22:05:22,403 INFO status has been updated to running\n",
            "2026-01-12 22:05:26,002 INFO status has been updated to successful\n",
            "                                                                                       \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[WARN] attempt 5/6 failed: Download finished but output looks too small: /Users/Shared/blueleaflabs/hydropulse/results/era5_raw/era5l_monthly_CA_1991.nc\n",
            "       sleeping 125s, then retrying...\n",
            "[DL] era5l_monthly_CA_1991.nc  (dataset=reanalysis-era5-land-monthly-means)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-01-12 22:07:35,622 INFO Request ID is 3630b934-6b11-4521-affc-c3903f4da419\n",
            "2026-01-12 22:07:35,819 INFO status has been updated to accepted\n",
            "2026-01-12 22:07:50,261 INFO status has been updated to successful\n",
            "                                                                                       \r"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Download finished but output looks too small: /Users/Shared/blueleaflabs/hydropulse/results/era5_raw/era5l_monthly_CA_1991.nc",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[74]\u001b[39m\u001b[32m, line 179\u001b[39m\n\u001b[32m    177\u001b[39m stats = {\u001b[33m\"\u001b[39m\u001b[33mdownloaded\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m0\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mskipped\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m0\u001b[39m}\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m year \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(baseline_start.year, baseline_end.year + \u001b[32m1\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m     res = \u001b[43mdownload_monthly_baseline\u001b[49m\u001b[43m(\u001b[49m\u001b[43myear\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m     stats[res] += \u001b[32m1\u001b[39m\n\u001b[32m    181\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mMonthly baseline summary:\u001b[39m\u001b[33m\"\u001b[39m, stats)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[74]\u001b[39m\u001b[32m, line 158\u001b[39m, in \u001b[36mdownload_monthly_baseline\u001b[39m\u001b[34m(year)\u001b[39m\n\u001b[32m    149\u001b[39m request = {\n\u001b[32m    150\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mvariable\u001b[39m\u001b[33m\"\u001b[39m: VARS_DAILY,     \u001b[38;5;66;03m# same variable list to keep schema aligned\u001b[39;00m\n\u001b[32m    151\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33myear\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear\u001b[38;5;132;01m:\u001b[39;00m\u001b[33md\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    155\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mformat\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mnetcdf\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    156\u001b[39m }\n\u001b[32m    157\u001b[39m out = RAW_DIR / \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mera5l_monthly_CA_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00myear\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.nc\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcds_download\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMONTHLY_DATASET\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[74]\u001b[39m\u001b[32m, line 107\u001b[39m, in \u001b[36mcds_download\u001b[39m\u001b[34m(dataset, request, out_path)\u001b[39m\n\u001b[32m    105\u001b[39m client.retrieve(dataset, request, \u001b[38;5;28mstr\u001b[39m(out_path))\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ok_file(out_path):\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDownload finished but output looks too small: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    108\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[OK] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_path.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_path.stat().st_size/\u001b[32m1e6\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m MB)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mdownloaded\u001b[39m\u001b[33m\"\u001b[39m\n",
            "\u001b[31mRuntimeError\u001b[39m: Download finished but output looks too small: /Users/Shared/blueleaflabs/hydropulse/results/era5_raw/era5l_monthly_CA_1991.nc"
          ]
        }
      ],
      "source": [
        "# ============================\n",
        "# ERA5 MASTER CELL (CDS API)\n",
        "# - Downloads CA-subset ERA5-Land data into results/{era5_raw,era5_clean}\n",
        "# - Supports:\n",
        "#   A) derived-era5-land-daily-statistics (daily mean)\n",
        "#   B) reanalysis-era5-land-monthly-means (baseline climatology)\n",
        "# - Resume-safe, low-noise, config-driven\n",
        "\n",
        "# Got the base data from here: https://cds.climate.copernicus.eu/datasets/derived-era5-land-daily-statistics?tab=download\n",
        "# Config file contains the variables\n",
        "# Created an account, and got an API key from https://cds.climate.copernicus.eu/api-how-to\n",
        "# Saved it in ~/.cdsapirc as:\n",
        "#   url: https://cds.climate.copernicus.eu/api/v2\n",
        "#   key: <uid>:<api_key>   \n",
        "# Config file contains the directories and download parameters\n",
        "\n",
        "\n",
        "# ============================\n",
        "\n",
        "from pathlib import Path\n",
        "import calendar\n",
        "import time\n",
        "import cdsapi\n",
        "import pandas as pd\n",
        "\n",
        "# ---------- toggles (edit these per run) ----------\n",
        "DO_DAILY_ANALYSIS_WINDOW = True          # 2024-06-01 .. 2024-10-31 (or CONFIG window)\n",
        "DO_MONTHLY_BASELINE = True               # baseline monthly means (1991-2020 by default)\n",
        "\n",
        "# If you want to limit daily downloads to only some months, set e.g. [6,7,8,9,10]\n",
        "DAILY_MONTHS_OVERRIDE = None  # e.g. [6,7,8,9,10] or None to auto from CONFIG start/end\n",
        "\n",
        "# ---------- helpers ----------\n",
        "def bbox_to_area(bbox: dict) -> list[float]:\n",
        "    # CDS expects [North, West, South, East]\n",
        "    return [\n",
        "        float(bbox[\"nwlat\"]),\n",
        "        float(bbox[\"nwlng\"]),\n",
        "        float(bbox[\"selat\"]),\n",
        "        float(bbox[\"selng\"]),\n",
        "    ]\n",
        "\n",
        "def ensure_dir(p: Path) -> Path:\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "    return p\n",
        "\n",
        "def ok_file(path: Path, min_bytes: int = 1_000_000) -> bool:\n",
        "    return path.exists() and path.is_file() and path.stat().st_size >= min_bytes\n",
        "\n",
        "def retry_sleep(base: float, attempt: int) -> float:\n",
        "    return base * attempt\n",
        "\n",
        "# ---------- config ----------\n",
        "OUT_DIR = Path(CONFIG[\"out_dir\"])\n",
        "RAW_DIR = ensure_dir(OUT_DIR / CONFIG.get(\"ERA5_RAW_DIRNAME\", \"era5_raw\"))\n",
        "CLEAN_DIR = ensure_dir(OUT_DIR / CONFIG.get(\"ERA5_CLEAN_DIRNAME\", \"era5_clean\"))  # placeholder for later\n",
        "\n",
        "BBOX = CONFIG[\"bbox\"]\n",
        "AREA = bbox_to_area(BBOX)\n",
        "\n",
        "DAILY_DATASET = CONFIG.get(\"ERA5_DAILY_DATASET\", \"derived-era5-land-daily-statistics\")\n",
        "MONTHLY_DATASET = CONFIG.get(\"ERA5_MONTHLY_DATASET\", \"reanalysis-era5-land-monthly-means\")\n",
        "\n",
        "VARS_DAILY = CONFIG.get(\"ERA5_VARIABLES_DAILY\", [\n",
        "    \"2m_temperature\",\n",
        "    \"snow_depth_water_equivalent\",\n",
        "    \"volumetric_soil_water_layer_1\",\n",
        "])\n",
        "\n",
        "MAX_RETRIES = int(CONFIG.get(\"ERA5_MAX_RETRIES\", 6))\n",
        "BACKOFF_BASE = float(CONFIG.get(\"ERA5_BACKOFF_BASE_S\", 25))\n",
        "\n",
        "# Analysis window from CONFIG\n",
        "analysis_start = pd.to_datetime(CONFIG[\"start_date\"]).date()\n",
        "analysis_end   = pd.to_datetime(CONFIG[\"end_date\"]).date()\n",
        "\n",
        "# Baseline window from CONFIG\n",
        "baseline_start = pd.to_datetime(CONFIG.get(\"BASELINE_START_DATE\", \"1991-01-01\")).date()\n",
        "baseline_end   = pd.to_datetime(CONFIG.get(\"BASELINE_END_DATE\", \"2020-12-31\")).date()\n",
        "\n",
        "print(\"ERA5 master cell starting\")\n",
        "print(\"  out_dir :\", OUT_DIR)\n",
        "print(\"  RAW_DIR :\", RAW_DIR)\n",
        "print(\"  CLEAN_DIR (future use):\", CLEAN_DIR)\n",
        "print(\"  AREA   :\", AREA, \"(N,W,S,E)\")\n",
        "print(\"  Daily dataset  :\", DAILY_DATASET)\n",
        "print(\"  Monthly dataset:\", MONTHLY_DATASET)\n",
        "print(\"  Daily vars:\", VARS_DAILY)\n",
        "print(\"  Analysis window :\", analysis_start, \"→\", analysis_end)\n",
        "print(\"  Baseline window :\", baseline_start, \"→\", baseline_end)\n",
        "\n",
        "# ---------- CDS client ----------\n",
        "client = cdsapi.Client()\n",
        "\n",
        "# ---------- download functions ----------\n",
        "def cds_download(dataset: str, request: dict, out_path: Path):\n",
        "    # resume-safe: skip if file exists\n",
        "    if ok_file(out_path):\n",
        "        print(f\"[SKIP] {out_path.name} exists ({out_path.stat().st_size/1e6:.1f} MB)\")\n",
        "        return \"skipped\"\n",
        "\n",
        "    for attempt in range(1, MAX_RETRIES + 1):\n",
        "        try:\n",
        "            print(f\"[DL] {out_path.name}  (dataset={dataset})\")\n",
        "            client.retrieve(dataset, request, str(out_path))\n",
        "            if not ok_file(out_path):\n",
        "                raise RuntimeError(f\"Download finished but output looks too small: {out_path}\")\n",
        "            print(f\"[OK] {out_path.name} ({out_path.stat().st_size/1e6:.1f} MB)\")\n",
        "            return \"downloaded\"\n",
        "        except Exception as e:\n",
        "            if attempt == MAX_RETRIES:\n",
        "                raise\n",
        "            sleep_s = retry_sleep(BACKOFF_BASE, attempt)\n",
        "            print(f\"[WARN] attempt {attempt}/{MAX_RETRIES} failed: {e}\")\n",
        "            print(f\"       sleeping {sleep_s:.0f}s, then retrying...\")\n",
        "            time.sleep(sleep_s)\n",
        "\n",
        "def download_daily_month(year: int, month: int):\n",
        "    ndays = calendar.monthrange(year, month)[1]\n",
        "    request = {\n",
        "        \"variable\": VARS_DAILY,\n",
        "        \"year\": f\"{year:d}\",\n",
        "        \"month\": f\"{month:02d}\",\n",
        "        \"day\": [f\"{d:02d}\" for d in range(1, ndays + 1)],\n",
        "        \"daily_statistic\": \"daily_mean\",\n",
        "        \"time_zone\": \"utc+00:00\",\n",
        "        \"frequency\": \"1_hourly\",\n",
        "        \"area\": AREA,\n",
        "        \"format\": \"netcdf\",\n",
        "    }\n",
        "    out = RAW_DIR / f\"era5l_daily_CA_{year}{month:02d}_mean.nc\"\n",
        "    return cds_download(DAILY_DATASET, request, out)\n",
        "\n",
        "def month_range_from_dates(d1, d2):\n",
        "    # inclusive month list between two dates\n",
        "    months = []\n",
        "    y, m = d1.year, d1.month\n",
        "    while (y, m) <= (d2.year, d2.month):\n",
        "        months.append((y, m))\n",
        "        if m == 12:\n",
        "            y += 1\n",
        "            m = 1\n",
        "        else:\n",
        "            m += 1\n",
        "    return months\n",
        "\n",
        "def download_monthly_baseline(year: int):\n",
        "    # Monthly means dataset supports selecting all months at once for a year\n",
        "    request = {\n",
        "        \"variable\": VARS_DAILY,     # same variable list to keep schema aligned\n",
        "        \"year\": f\"{year:d}\",\n",
        "        \"month\": [f\"{m:02d}\" for m in range(1, 13)],\n",
        "        \"time\": \"00:00\",\n",
        "        \"area\": AREA,\n",
        "        \"format\": \"netcdf\",\n",
        "    }\n",
        "    out = RAW_DIR / f\"era5l_monthly_CA_{year}.nc\"\n",
        "    return cds_download(MONTHLY_DATASET, request, out)\n",
        "\n",
        "# ---------- run daily downloads ----------\n",
        "if DO_DAILY_ANALYSIS_WINDOW:\n",
        "    if DAILY_MONTHS_OVERRIDE is not None:\n",
        "        months = [(analysis_start.year, m) for m in DAILY_MONTHS_OVERRIDE]\n",
        "    else:\n",
        "        months = month_range_from_dates(analysis_start, analysis_end)\n",
        "\n",
        "    print(\"\\nERA5 daily downloads:\")\n",
        "    stats = {\"downloaded\": 0, \"skipped\": 0}\n",
        "    for (y, m) in months:\n",
        "        res = download_daily_month(y, m)\n",
        "        stats[res] += 1\n",
        "    print(\"Daily summary:\", stats)\n",
        "\n",
        "# ---------- run monthly baseline downloads ----------\n",
        "if DO_MONTHLY_BASELINE:\n",
        "    print(\"\\nERA5 monthly baseline downloads:\")\n",
        "    stats = {\"downloaded\": 0, \"skipped\": 0}\n",
        "    for year in range(baseline_start.year, baseline_end.year + 1):\n",
        "        res = download_monthly_baseline(year)\n",
        "        stats[res] += 1\n",
        "    print(\"Monthly baseline summary:\", stats)\n",
        "\n",
        "print(\"\\nERA5 master cell complete.\")\n",
        "print(\"Raw files are in:\", RAW_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "id": "43209c96",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "USGS download starting (yearly chunks)\n",
            "  RAW_DIR  : /Users/Shared/blueleaflabs/hydropulse/results/usgs_raw\n",
            "  CLEAN_DIR: /Users/Shared/blueleaflabs/hydropulse/results/usgs_clean (reserved)\n",
            "  state/site/param: CA ST 00060\n",
            "  date range: 1991-01-01 → 2025-12-31\n",
            "  years: 1991 → 2025\n",
            "[DL] 1991 -> usgs_dv_ca_st_00060_1991.rdb\n",
            "[OK] usgs_dv_ca_st_00060_1991.rdb (9.97 MB)\n",
            "[DL] 1992 -> usgs_dv_ca_st_00060_1992.rdb\n",
            "[OK] usgs_dv_ca_st_00060_1992.rdb (9.79 MB)\n",
            "[DL] 1993 -> usgs_dv_ca_st_00060_1993.rdb\n",
            "[OK] usgs_dv_ca_st_00060_1993.rdb (9.54 MB)\n",
            "[DL] 1994 -> usgs_dv_ca_st_00060_1994.rdb\n",
            "[OK] usgs_dv_ca_st_00060_1994.rdb (9.68 MB)\n",
            "[DL] 1995 -> usgs_dv_ca_st_00060_1995.rdb\n",
            "[OK] usgs_dv_ca_st_00060_1995.rdb (9.45 MB)\n",
            "[DL] 1996 -> usgs_dv_ca_st_00060_1996.rdb\n",
            "[OK] usgs_dv_ca_st_00060_1996.rdb (9.59 MB)\n",
            "[DL] 1997 -> usgs_dv_ca_st_00060_1997.rdb\n",
            "[OK] usgs_dv_ca_st_00060_1997.rdb (9.53 MB)\n",
            "[DL] 1998 -> usgs_dv_ca_st_00060_1998.rdb\n",
            "[OK] usgs_dv_ca_st_00060_1998.rdb (9.66 MB)\n",
            "[DL] 1999 -> usgs_dv_ca_st_00060_1999.rdb\n",
            "[OK] usgs_dv_ca_st_00060_1999.rdb (10.03 MB)\n",
            "[DL] 2000 -> usgs_dv_ca_st_00060_2000.rdb\n",
            "[OK] usgs_dv_ca_st_00060_2000.rdb (10.21 MB)\n",
            "[DL] 2001 -> usgs_dv_ca_st_00060_2001.rdb\n",
            "[OK] usgs_dv_ca_st_00060_2001.rdb (10.37 MB)\n",
            "[DL] 2002 -> usgs_dv_ca_st_00060_2002.rdb\n",
            "[OK] usgs_dv_ca_st_00060_2002.rdb (10.35 MB)\n",
            "[DL] 2003 -> usgs_dv_ca_st_00060_2003.rdb\n",
            "[OK] usgs_dv_ca_st_00060_2003.rdb (10.27 MB)\n",
            "[DL] 2004 -> usgs_dv_ca_st_00060_2004.rdb\n",
            "[OK] usgs_dv_ca_st_00060_2004.rdb (10.31 MB)\n",
            "[DL] 2005 -> usgs_dv_ca_st_00060_2005.rdb\n",
            "[OK] usgs_dv_ca_st_00060_2005.rdb (10.05 MB)\n",
            "[DL] 2006 -> usgs_dv_ca_st_00060_2006.rdb\n",
            "[OK] usgs_dv_ca_st_00060_2006.rdb (10.13 MB)\n",
            "[DL] 2007 -> usgs_dv_ca_st_00060_2007.rdb\n",
            "[OK] usgs_dv_ca_st_00060_2007.rdb (10.25 MB)\n",
            "[DL] 2008 -> usgs_dv_ca_st_00060_2008.rdb\n",
            "[OK] usgs_dv_ca_st_00060_2008.rdb (10.29 MB)\n",
            "[DL] 2009 -> usgs_dv_ca_st_00060_2009.rdb\n",
            "[OK] usgs_dv_ca_st_00060_2009.rdb (10.28 MB)\n",
            "[DL] 2010 -> usgs_dv_ca_st_00060_2010.rdb\n",
            "[OK] usgs_dv_ca_st_00060_2010.rdb (10.43 MB)\n",
            "[DL] 2011 -> usgs_dv_ca_st_00060_2011.rdb\n",
            "[OK] usgs_dv_ca_st_00060_2011.rdb (10.21 MB)\n",
            "[DL] 2012 -> usgs_dv_ca_st_00060_2012.rdb\n",
            "[OK] usgs_dv_ca_st_00060_2012.rdb (10.38 MB)\n",
            "[DL] 2013 -> usgs_dv_ca_st_00060_2013.rdb\n",
            "[OK] usgs_dv_ca_st_00060_2013.rdb (10.30 MB)\n",
            "[DL] 2014 -> usgs_dv_ca_st_00060_2014.rdb\n",
            "[OK] usgs_dv_ca_st_00060_2014.rdb (10.29 MB)\n",
            "[DL] 2015 -> usgs_dv_ca_st_00060_2015.rdb\n",
            "[OK] usgs_dv_ca_st_00060_2015.rdb (10.27 MB)\n",
            "[DL] 2016 -> usgs_dv_ca_st_00060_2016.rdb\n",
            "[OK] usgs_dv_ca_st_00060_2016.rdb (10.07 MB)\n",
            "[DL] 2017 -> usgs_dv_ca_st_00060_2017.rdb\n",
            "[OK] usgs_dv_ca_st_00060_2017.rdb (9.85 MB)\n",
            "[DL] 2018 -> usgs_dv_ca_st_00060_2018.rdb\n",
            "[OK] usgs_dv_ca_st_00060_2018.rdb (9.95 MB)\n",
            "[DL] 2019 -> usgs_dv_ca_st_00060_2019.rdb\n",
            "[OK] usgs_dv_ca_st_00060_2019.rdb (9.85 MB)\n",
            "[DL] 2020 -> usgs_dv_ca_st_00060_2020.rdb\n",
            "[OK] usgs_dv_ca_st_00060_2020.rdb (9.92 MB)\n",
            "[DL] 2021 -> usgs_dv_ca_st_00060_2021.rdb\n",
            "[OK] usgs_dv_ca_st_00060_2021.rdb (9.83 MB)\n",
            "[DL] 2022 -> usgs_dv_ca_st_00060_2022.rdb\n",
            "[OK] usgs_dv_ca_st_00060_2022.rdb (9.74 MB)\n",
            "[DL] 2023 -> usgs_dv_ca_st_00060_2023.rdb\n",
            "[OK] usgs_dv_ca_st_00060_2023.rdb (9.57 MB)\n",
            "[DL] 2024 -> usgs_dv_ca_st_00060_2024.rdb\n",
            "[OK] usgs_dv_ca_st_00060_2024.rdb (8.74 MB)\n",
            "[DL] 2025 -> usgs_dv_ca_st_00060_2025.rdb\n",
            "[OK] usgs_dv_ca_st_00060_2025.rdb (5.92 MB)\n",
            "\n",
            "Downloaded/verified chunks: 35\n",
            "[COMBINE] Writing combined file: usgs_dv_ca_st_00060_1991-2025.rdb\n",
            "[OK] Combined: usgs_dv_ca_st_00060_1991-2025.rdb (343.43 MB)\n"
          ]
        }
      ],
      "source": [
        "# Next dataset is the USGS-StreamFlow dataset\n",
        "# ============================\n",
        "# Base endpoint (Daily Values / DV):\n",
        "# \t•\thttps://waterservices.usgs.gov/nwis/dv/\n",
        "\n",
        "# Key parameters you’ll use (these are the ones that matter):\n",
        "# \t•\tformat=rdb (tab-delimited, easy to parse)\n",
        "# \t•\tstateCd=ca (California-only site filter)\n",
        "# \t•\tparameterCd=00060 (discharge, cubic feet per second)\n",
        "# \t•\tsiteType=ST (streams)\n",
        "# \t•\tstartDT=1991-01-01 (baseline start)\n",
        "# \t•\tendDT=2020-12-31 (baseline end) or endDT=2024-10-31 (analysis end)\n",
        "# \t•\tsiteStatus=active (optional)\n",
        "# \t•\tstatCd=00003 (daily mean) (supported by DV service outputs)  ￼\n",
        "# ============================\n",
        "\n",
        "# URLs look like this: https://waterservices.usgs.gov/nwis/dv/?format=rdb&stateCd=CA&siteType=ST&parameterCd=00060&startDT=1991-01-01&endDT=2025-12-31\n",
        "# This is a large download! If this fails, then try manually\n",
        "# downloading from the above URL and saving as \"manual/usgs-streamflow/streamflow-19910101-20251231.txt\"\n",
        "# and then run the parsing/normalization cell below.\n",
        "# ============================\n",
        "\n",
        "\n",
        "# ============================\n",
        "# USGS NWIS Daily Values downloader (chunked, resumable)\n",
        "# Writes to:\n",
        "#   results/{USGS_RAW_DIRNAME}/  (yearly .rdb chunks + optional combined raw)\n",
        "#   results/{USGS_CLEAN_DIRNAME}/ (reserved for later)\n",
        "# ============================\n",
        "\n",
        "from pathlib import Path\n",
        "import time\n",
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "# ---- config ----\n",
        "OUT_DIR = Path(CONFIG[\"out_dir\"])\n",
        "\n",
        "RAW_DIR = OUT_DIR / CONFIG.get(\"USGS_RAW_DIRNAME\", \"usgs_raw\")\n",
        "CLEAN_DIR = OUT_DIR / CONFIG.get(\"USGS_CLEAN_DIRNAME\", \"usgs_clean\")\n",
        "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
        "CLEAN_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "STATE_CD = CONFIG.get(\"USGS_STATE_CD\", \"CA\")\n",
        "SITE_TYPE = CONFIG.get(\"USGS_SITE_TYPE\", \"ST\")\n",
        "PARAM = CONFIG.get(\"USGS_PARAM_DISCHARGE\", \"00060\")\n",
        "\n",
        "START = pd.to_datetime(CONFIG.get(\"USGS_START_DATE\", \"1991-01-01\")).date()\n",
        "END   = pd.to_datetime(CONFIG.get(\"USGS_END_DATE\", \"2025-12-31\")).date()\n",
        "\n",
        "SLEEP_S = float(CONFIG.get(\"USGS_SLEEP_S\", 2.5))\n",
        "MAX_RETRIES = int(CONFIG.get(\"USGS_MAX_RETRIES\", 6))\n",
        "BACKOFF_BASE = float(CONFIG.get(\"USGS_BACKOFF_BASE_S\", 10))\n",
        "TIMEOUT_S = int(CONFIG.get(\"USGS_TIMEOUT_S\", 180))\n",
        "\n",
        "BASE_URL = \"https://waterservices.usgs.gov/nwis/dv/\"\n",
        "\n",
        "def ok_file(path: Path, min_bytes: int = 50_000) -> bool:\n",
        "    return path.exists() and path.is_file() and path.stat().st_size >= min_bytes\n",
        "\n",
        "def fetch_year(year: int) -> Path:\n",
        "    startDT = f\"{year:04d}-01-01\"\n",
        "    endDT   = f\"{year:04d}-12-31\"\n",
        "    out = RAW_DIR / f\"usgs_dv_{STATE_CD.lower()}_{SITE_TYPE.lower()}_{PARAM}_{year:04d}.rdb\"\n",
        "\n",
        "    if ok_file(out):\n",
        "        print(f\"[SKIP] {out.name} exists ({out.stat().st_size/1e6:.2f} MB)\")\n",
        "        return out\n",
        "\n",
        "    params = {\n",
        "        \"format\": \"rdb\",\n",
        "        \"stateCd\": STATE_CD,\n",
        "        \"siteType\": SITE_TYPE,\n",
        "        \"parameterCd\": PARAM,\n",
        "        \"startDT\": startDT,\n",
        "        \"endDT\": endDT,\n",
        "    }\n",
        "\n",
        "    headers = {\n",
        "        \"User-Agent\": CONFIG.get(\"USER_AGENT_HEADERS\", {}).get(\"User-Agent\", \"BlueLeafLabs/HydroPulse\"),\n",
        "        \"Accept-Encoding\": \"gzip, deflate\",\n",
        "    }\n",
        "\n",
        "    for attempt in range(1, MAX_RETRIES + 1):\n",
        "        try:\n",
        "            print(f\"[DL] {year} -> {out.name}\")\n",
        "            r = requests.get(BASE_URL, params=params, headers=headers, timeout=TIMEOUT_S)\n",
        "            r.raise_for_status()\n",
        "\n",
        "            tmp = out.with_suffix(\".tmp\")\n",
        "            tmp.write_bytes(r.content)\n",
        "\n",
        "            if not ok_file(tmp):\n",
        "                raise RuntimeError(f\"Downloaded file too small for {year}: {tmp.stat().st_size} bytes\")\n",
        "\n",
        "            tmp.replace(out)\n",
        "            print(f\"[OK] {out.name} ({out.stat().st_size/1e6:.2f} MB)\")\n",
        "            time.sleep(SLEEP_S)\n",
        "            return out\n",
        "\n",
        "        except Exception as e:\n",
        "            if attempt == MAX_RETRIES:\n",
        "                raise\n",
        "            sleep = BACKOFF_BASE * attempt\n",
        "            print(f\"[WARN] {year} attempt {attempt}/{MAX_RETRIES} failed: {e}\")\n",
        "            print(f\"       sleeping {sleep:.0f}s then retrying...\")\n",
        "            time.sleep(sleep)\n",
        "\n",
        "# ---- run yearly downloads ----\n",
        "years = list(range(START.year, END.year + 1))\n",
        "\n",
        "print(\"USGS download starting (yearly chunks)\")\n",
        "print(\"  RAW_DIR  :\", RAW_DIR)\n",
        "print(\"  CLEAN_DIR:\", CLEAN_DIR, \"(reserved)\")\n",
        "print(\"  state/site/param:\", STATE_CD, SITE_TYPE, PARAM)\n",
        "print(\"  date range:\", START, \"→\", END)\n",
        "print(\"  years:\", years[0], \"→\", years[-1])\n",
        "\n",
        "paths = []\n",
        "for y in years:\n",
        "    paths.append(fetch_year(y))\n",
        "\n",
        "print(\"\\nDownloaded/verified chunks:\", len(paths))\n",
        "\n",
        "# ---- optional: combined raw file under RAW_DIR ----\n",
        "combined = RAW_DIR / f\"usgs_dv_{STATE_CD.lower()}_{SITE_TYPE.lower()}_{PARAM}_{START.year:04d}-{END.year:04d}.rdb\"\n",
        "if not ok_file(combined, min_bytes=200_000):\n",
        "    print(\"[COMBINE] Writing combined file:\", combined.name)\n",
        "    with combined.open(\"wb\") as w:\n",
        "        first = True\n",
        "        for p in paths:\n",
        "            data = p.read_bytes().splitlines(keepends=True)\n",
        "            if first:\n",
        "                w.writelines(data)\n",
        "                first = False\n",
        "            else:\n",
        "                # drop leading comment lines from each subsequent yearly shard\n",
        "                i = 0\n",
        "                while i < len(data) and data[i].lstrip().startswith(b\"#\"):\n",
        "                    i += 1\n",
        "                w.writelines(data[i:])\n",
        "    print(\"[OK] Combined:\", combined.name, f\"({combined.stat().st_size/1e6:.2f} MB)\")\n",
        "else:\n",
        "    print(\"[SKIP] Combined file already exists:\", combined.name)\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c11d662",
      "metadata": {},
      "outputs": [],
      "source": [
        "# MODIS DATA DOWNLOAD\n",
        "# ============================\n",
        "# Manual data again\n",
        "# https://appeears.earthdatacloud.nasa.gov/task/area\n",
        "# parameters: name: hydropulse_ca_ndvi\n",
        "# GeoJSON file: results/california_boundary.geojson\n",
        "# date range: 2000-01-01 to 2020-12-31\n",
        "# products: MOD13Q1.061 – MODIS/Terra Vegetation Indices 16-Day L3 Global 250m\n",
        "# From this product, select only: NDVI, VI_Quality, EVI\n",
        "# Output format: GeoTIFF\n",
        "# Projection: Native Projection \n",
        "# ============================\n",
        "\n",
        "# Then repeat the same thing for 2024-06-01 to 2024-10-31\n",
        "# Emails were sent when the jobs were received\n",
        "# Now to wait for the files to be ready for download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28ac19ff",
      "metadata": {},
      "outputs": [],
      "source": [
        "# === HydroPulse | Final Daily Grid Builder (v0: GHCND PRCP only) ===\n",
        "# Produces: CONFIG[\"FINAL_DAILY_FILENAME\"] as a canonical daily grid table:\n",
        "#   grid_id × date → prcp_mm + QC\n",
        "#\n",
        "# Inputs:\n",
        "#   - Grid parquet (EPSG:3310): CONFIG[\"GRID_FILENAME\"]\n",
        "#   - GHCND cleaned station-day parquet: resolve_out_path(CONFIG[\"GHCND_CLEAN_PARQUET_NAME\"])\n",
        "#\n",
        "# Notes for later HeatShield refactor:\n",
        "#   - This cell establishes the common \"final builder\" contract: {grid_id, date, variables..., QC...}\n",
        "#   - Keep the interface stable; only swap/extend source adapters per repo.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "from pathlib import Path\n",
        "from sklearn.neighbors import BallTree\n",
        "\n",
        "# -----------------------------\n",
        "# Paths\n",
        "# -----------------------------\n",
        "OUT_DIR = Path(CONFIG[\"out_dir\"])\n",
        "GRID_PATH = Path(resolve_out_path(CONFIG[\"GRID_FILENAME\"]))\n",
        "GHCND_PATH = Path(resolve_out_path(CONFIG[\"GHCND_CLEAN_PARQUET_NAME\"]))\n",
        "FINAL_PATH = Path(resolve_out_path(CONFIG[\"FINAL_DAILY_FILENAME\"]))\n",
        "\n",
        "# Resume via daily shards\n",
        "SHARDS_DIR = OUT_DIR / \"derived\" / \"final_daily_shards_prcp\"\n",
        "SHARDS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"GRID_PATH:\", GRID_PATH)\n",
        "print(\"GHCND_PATH:\", GHCND_PATH)\n",
        "print(\"SHARDS_DIR:\", SHARDS_DIR)\n",
        "print(\"FINAL_PATH:\", FINAL_PATH)\n",
        "\n",
        "# -----------------------------\n",
        "# Column contract (confirmed)\n",
        "# -----------------------------\n",
        "STATION_COL = \"station_core\"\n",
        "DATE_COL    = \"date\"\n",
        "LAT_COL     = \"lat\"\n",
        "LON_COL     = \"lon\"\n",
        "PRCP_COL_IN = \"precipitation_mm\"\n",
        "\n",
        "# Output column naming for the final table\n",
        "PRCP_COL_OUT = \"prcp_mm\"\n",
        "\n",
        "# -----------------------------\n",
        "# Tunables (can later move to config)\n",
        "# -----------------------------\n",
        "K = int(CONFIG.get(\"PRCP_IDW_K\", 8))                        # k nearest stations\n",
        "HARD_CAP_KM = float(CONFIG.get(\"PRCP_HARD_CAP_KM\", 100.0))  # ignore stations beyond this radius\n",
        "POWER = float(CONFIG.get(\"PRCP_IDW_POWER\", 2.0))            # IDW power\n",
        "\n",
        "OPS_EPSG = int(CONFIG.get(\"OPS_EPSG\", 3310))\n",
        "WGS84_EPSG = int(CONFIG.get(\"WGS84_EPSG\", 4326))\n",
        "\n",
        "# -----------------------------\n",
        "# Helpers (keep these stable across repos)\n",
        "# -----------------------------\n",
        "def ensure_epsg(gdf: gpd.GeoDataFrame, epsg: int) -> gpd.GeoDataFrame:\n",
        "    if gdf.crs is None:\n",
        "        gdf = gdf.set_crs(f\"EPSG:{WGS84_EPSG}\")\n",
        "    if (gdf.crs.to_epsg() or 0) != epsg:\n",
        "        gdf = gdf.to_crs(epsg)\n",
        "    return gdf\n",
        "\n",
        "def ensure_grid_id(grid: gpd.GeoDataFrame) -> tuple[gpd.GeoDataFrame, str]:\n",
        "    for c in [\"grid_id\", \"cell_id\", \"id\"]:\n",
        "        if c in grid.columns:\n",
        "            return grid, c\n",
        "    grid = grid.copy()\n",
        "    grid[\"grid_id\"] = np.arange(len(grid), dtype=np.int32)\n",
        "    return grid, \"grid_id\"\n",
        "\n",
        "def build_balltree_from_points(geom: gpd.GeoSeries) -> BallTree:\n",
        "    xy = np.column_stack([geom.x.values, geom.y.values])\n",
        "    return BallTree(xy, metric=\"euclidean\")\n",
        "\n",
        "def idw(dist_m: np.ndarray, vals: np.ndarray, power: float) -> float:\n",
        "    if np.any(dist_m == 0):\n",
        "        return float(vals[np.argmin(dist_m)])\n",
        "    w = 1.0 / np.power(dist_m, power)\n",
        "    return float(np.sum(w * vals) / np.sum(w))\n",
        "\n",
        "def interpolate_prcp_for_day(grid_centroids: gpd.GeoSeries, stations_day: gpd.GeoDataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Returns a DF aligned to grid_centroids order with columns:\n",
        "      prcp_mm, n_used, maxdist_km, method\n",
        "    \"\"\"\n",
        "    n_cells = len(grid_centroids)\n",
        "    out = pd.DataFrame({\n",
        "        PRCP_COL_OUT: np.full(n_cells, np.nan, dtype=float),\n",
        "        \"n_used\": np.zeros(n_cells, dtype=np.int16),\n",
        "        \"maxdist_km\": np.full(n_cells, np.nan, dtype=float),\n",
        "        \"method\": np.full(n_cells, None, dtype=object),\n",
        "    })\n",
        "\n",
        "    if stations_day is None or len(stations_day) == 0:\n",
        "        return out\n",
        "\n",
        "    # Build tree on station points\n",
        "    tree = build_balltree_from_points(stations_day.geometry)\n",
        "    vals = stations_day[PRCP_COL_IN].to_numpy(dtype=float)\n",
        "\n",
        "    qxy = np.column_stack([grid_centroids.x.values, grid_centroids.y.values])\n",
        "    k_eff = min(K, len(stations_day))\n",
        "    dist_m, idx = tree.query(qxy, k=k_eff)\n",
        "\n",
        "    hard_cap_m = HARD_CAP_KM * 1000.0\n",
        "\n",
        "    for i in range(n_cells):\n",
        "        d = dist_m[i]\n",
        "        j = idx[i]\n",
        "\n",
        "        # Apply hard cap\n",
        "        mask = d <= hard_cap_m\n",
        "        if not np.any(mask):\n",
        "            continue\n",
        "\n",
        "        d_use = d[mask]\n",
        "        v_use = vals[j[mask]]\n",
        "\n",
        "        # Drop NaNs (defensive)\n",
        "        good = np.isfinite(v_use)\n",
        "        d_use = d_use[good]\n",
        "        v_use = v_use[good]\n",
        "        if len(v_use) == 0:\n",
        "            continue\n",
        "\n",
        "        if len(v_use) == 1:\n",
        "            out.at[i, PRCP_COL_OUT] = float(v_use[0])\n",
        "            out.at[i, \"method\"] = \"nearest\"\n",
        "        else:\n",
        "            out.at[i, PRCP_COL_OUT] = idw(d_use, v_use, power=POWER)\n",
        "            out.at[i, \"method\"] = f\"idw_k{len(v_use)}\"\n",
        "\n",
        "        out.at[i, \"n_used\"] = int(len(v_use))\n",
        "        out.at[i, \"maxdist_km\"] = float(np.max(d_use) / 1000.0)\n",
        "\n",
        "    return out\n",
        "\n",
        "# -----------------------------\n",
        "# Load grid (EPSG:3310) and compute centroids\n",
        "# -----------------------------\n",
        "grid = gpd.read_parquet(GRID_PATH)\n",
        "grid = ensure_epsg(grid, OPS_EPSG)\n",
        "grid, GID_COL = ensure_grid_id(grid)\n",
        "centroids = grid.geometry.centroid\n",
        "\n",
        "print(f\"Grid: {len(grid)} cells | CRS EPSG: {grid.crs.to_epsg()} | id col: {GID_COL}\")\n",
        "\n",
        "# -----------------------------\n",
        "# Load cleaned GHCND station-day table\n",
        "# -----------------------------\n",
        "cdo = pd.read_parquet(GHCND_PATH)\n",
        "\n",
        "required = [STATION_COL, DATE_COL, LAT_COL, LON_COL, PRCP_COL_IN]\n",
        "missing = [c for c in required if c not in cdo.columns]\n",
        "if missing:\n",
        "    raise KeyError(f\"Missing required columns in {GHCND_PATH.name}: {missing}. Have: {list(cdo.columns)}\")\n",
        "\n",
        "# Normalize date to UTC day\n",
        "cdo[DATE_COL] = pd.to_datetime(cdo[DATE_COL], utc=True, errors=\"coerce\").dt.normalize()\n",
        "\n",
        "# Filter date window\n",
        "start = pd.to_datetime(CONFIG[\"start_date\"], utc=True).normalize()\n",
        "end = pd.to_datetime(CONFIG[\"end_date\"], utc=True).normalize()\n",
        "cdo = cdo[(cdo[DATE_COL] >= start) & (cdo[DATE_COL] <= end)].copy()\n",
        "\n",
        "# Drop invalid coords / missing precip\n",
        "cdo = cdo[np.isfinite(cdo[LAT_COL]) & np.isfinite(cdo[LON_COL])].copy()\n",
        "cdo = cdo[np.isfinite(cdo[PRCP_COL_IN])].copy()\n",
        "\n",
        "print(\"Station-day rows:\", len(cdo), \"| stations:\", cdo[STATION_COL].nunique(), \"| dates:\", cdo[DATE_COL].nunique())\n",
        "\n",
        "# GeoDataFrame in OPS_EPSG\n",
        "pts = gpd.GeoDataFrame(\n",
        "    cdo,\n",
        "    geometry=gpd.points_from_xy(cdo[LON_COL], cdo[LAT_COL]),\n",
        "    crs=f\"EPSG:{WGS84_EPSG}\"\n",
        ")\n",
        "pts = ensure_epsg(pts, OPS_EPSG)\n",
        "\n",
        "# Pre-group by date for speed (avoid repeated boolean filters)\n",
        "pts_by_date = {d: df for d, df in pts.groupby(DATE_COL)}\n",
        "all_dates = pd.date_range(start=start, end=end, freq=\"D\")\n",
        "\n",
        "# -----------------------------\n",
        "# Daily loop with resume\n",
        "# -----------------------------\n",
        "written = 0\n",
        "skipped = 0\n",
        "\n",
        "for d in all_dates:\n",
        "    tag = d.strftime(\"%Y%m%d\")\n",
        "    shard_path = SHARDS_DIR / f\"final_prcp_{tag}.parquet\"\n",
        "    if shard_path.exists():\n",
        "        skipped += 1\n",
        "        continue\n",
        "\n",
        "    day_pts = pts_by_date.get(d)\n",
        "    interp = interpolate_prcp_for_day(centroids, day_pts)\n",
        "\n",
        "    out = pd.DataFrame({\n",
        "        GID_COL: grid[GID_COL].values,\n",
        "        \"date\": np.full(len(grid), d),\n",
        "    })\n",
        "    out = pd.concat([out, interp], axis=1)\n",
        "\n",
        "    out.to_parquet(shard_path, index=False)\n",
        "    written += 1\n",
        "\n",
        "print(f\"Shards written: {written} | skipped (resume): {skipped} | total days: {len(all_dates)}\")\n",
        "\n",
        "# -----------------------------\n",
        "# Compose FINAL parquet\n",
        "# -----------------------------\n",
        "shards = sorted(SHARDS_DIR.glob(\"final_prcp_*.parquet\"))\n",
        "if not shards:\n",
        "    raise FileNotFoundError(f\"No shards found in {SHARDS_DIR}\")\n",
        "\n",
        "df_final = pd.concat((pd.read_parquet(p) for p in shards), ignore_index=True)\n",
        "\n",
        "df_final[\"date\"] = pd.to_datetime(df_final[\"date\"], utc=True).dt.normalize()\n",
        "df_final = df_final.sort_values([GID_COL, \"date\"]).reset_index(drop=True)\n",
        "\n",
        "FINAL_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
        "df_final.to_parquet(FINAL_PATH, index=False)\n",
        "\n",
        "print(\"Saved FINAL:\", FINAL_PATH)\n",
        "print(\"Final rows:\", len(df_final), \"| cells:\", df_final[GID_COL].nunique(), \"| dates:\", df_final[\"date\"].nunique())\n",
        "print(df_final.head())"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "blueleaflabs",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
