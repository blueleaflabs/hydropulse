{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72d0c1d1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Imports ---\n",
        "import os, json, math, io, zipfile, time, re, shutil, glob, pathlib, sys, subprocess, shlex, tempfile, warnings\n",
        "from pathlib import Path\n",
        "from datetime import datetime as dt, timedelta, timezone, date, datetime\n",
        "from dateutil import parser as dateparser\n",
        "from dateutil.relativedelta import relativedelta\n",
        "from urllib.parse import urljoin, quote\n",
        "from functools import reduce\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from tqdm import tqdm\n",
        "import pytz\n",
        "import xarray as xr\n",
        "import rasterio\n",
        "from rasterio.mask import mask\n",
        "\n",
        "import shapely\n",
        "from shapely import ops\n",
        "from shapely.geometry import Point, Polygon, box, mapping\n",
        "from shapely.ops import unary_union, transform as shp_transform\n",
        "\n",
        "from pyproj import Transformer\n",
        "import geopandas as gpd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Rectangle\n",
        "import folium\n",
        "import ee  # Earth Engine\n",
        "\n",
        "from sklearn.neighbors import BallTree\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.spatial.distance import cdist\n",
        "from geopy.distance import geodesic\n",
        "\n",
        "import yaml\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "\n",
        "def skip_if_exists(path: str) -> bool:\n",
        "    return os.path.exists(path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6f568ab",
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Configuration loader (shared HeatShield/HydroPulse) ---\n",
        "from pathlib import Path\n",
        "import os\n",
        "import re\n",
        "import yaml\n",
        "\n",
        "def load_env_file(path: Path) -> dict:\n",
        "    env = {}\n",
        "    if not path.exists():\n",
        "        return env\n",
        "    for line in path.read_text().splitlines():\n",
        "        line = line.strip()\n",
        "        if not line or line.startswith(\"#\") or \"=\" not in line:\n",
        "            continue\n",
        "        key, val = line.split(\"=\", 1)\n",
        "        env[key.strip()] = val.strip().strip('\"').strip(\"'\")\n",
        "    return env\n",
        "\n",
        "def apply_env_overrides() -> None:\n",
        "    env = load_env_file(Path(\".env\"))\n",
        "    for k, v in env.items():\n",
        "        if v:\n",
        "            os.environ[k] = v\n",
        "\n",
        "def _fmt_yyyymmdd(s: str) -> str:\n",
        "    # expects YYYY-MM-DD\n",
        "    return re.sub(r\"-\", \"\", s.strip())\n",
        "\n",
        "def _render_template(tpl: str, *, start: str, end: str, res_m: int, epsg: int, region: str) -> str:\n",
        "    return tpl.format(\n",
        "        start=_fmt_yyyymmdd(start),\n",
        "        end=_fmt_yyyymmdd(end),\n",
        "        res_m=int(res_m),\n",
        "        epsg=int(epsg),\n",
        "        region=str(region),\n",
        "    )\n",
        "\n",
        "def _resolve_out_dir(project_dir: Path, out_dir_value: str | None) -> Path:\n",
        "    out_dir_value = out_dir_value or \"results\"\n",
        "    p = Path(out_dir_value)\n",
        "    return (p if p.is_absolute() else (project_dir / p)).resolve()\n",
        "\n",
        "def _resolve_pathlike_keys(cfg: dict, out_dir: Path) -> None:\n",
        "    \"\"\"\n",
        "    Resolve config values that look like relative file paths under out_dir.\n",
        "    Rule: if key ends with _FILENAME, _PATH, _ZIP, _TIF_NAME, _CSV_NAME, _PARQUET_NAME, _TXT_NAME\n",
        "    and the value is a relative path, make it absolute under out_dir.\n",
        "    \"\"\"\n",
        "    suffixes = (\n",
        "        \"_FILENAME\", \"_PATH\", \"_ZIP\",\n",
        "        \"_TIF_NAME\", \"_CSV_NAME\", \"_PARQUET_NAME\", \"_TXT_NAME\",\n",
        "        \"_NETCDF_NAME\", \"_ZARR_NAME\"\n",
        "    )\n",
        "    for k, v in list(cfg.items()):\n",
        "        if not isinstance(v, str):\n",
        "            continue\n",
        "        if not k.endswith(suffixes):\n",
        "            continue\n",
        "        p = Path(v)\n",
        "        if p.is_absolute():\n",
        "            cfg[k] = str(p)\n",
        "        else:\n",
        "            cfg[k] = str((out_dir / p).resolve())\n",
        "\n",
        "# Fail fast unless PROJECT_DIR is explicitly provided.\n",
        "apply_env_overrides()\n",
        "\n",
        "PROJECT_DIR = os.environ.get(\"PROJECT_DIR\")\n",
        "if not PROJECT_DIR:\n",
        "    raise FileNotFoundError(\n",
        "        \"PROJECT_DIR is not set. Add PROJECT_DIR to .env or environment variables.\"\n",
        "    )\n",
        "\n",
        "PROJECT_DIR = Path(PROJECT_DIR).expanduser().resolve()\n",
        "CONFIG_PATH = PROJECT_DIR / \"config\" / \"config.yaml\"\n",
        "if not CONFIG_PATH.exists():\n",
        "    raise FileNotFoundError(f\"Missing config file: {CONFIG_PATH}\")\n",
        "\n",
        "CONFIG = yaml.safe_load(CONFIG_PATH.read_text()) or {}\n",
        "\n",
        "# Set out_dir to an absolute path early.\n",
        "OUT_DIR = _resolve_out_dir(PROJECT_DIR, CONFIG.get(\"out_dir\", \"results\"))\n",
        "CONFIG[\"out_dir\"] = str(OUT_DIR)\n",
        "\n",
        "# Optional: Apply env overrides ONLY for keys that exist in config.yaml.\n",
        "# This prevents HeatShield-specific lists in shared code.\n",
        "for key in list(CONFIG.keys()):\n",
        "    env_val = os.environ.get(key)\n",
        "    if env_val:\n",
        "        CONFIG[key] = env_val\n",
        "\n",
        "# Also allow a small, explicit allowlist for common optional overrides across projects.\n",
        "for key in [\"PURPLEAIR_SENSOR_INDEX\"]:  # harmless if absent in HydroPulse config\n",
        "    env_val = os.environ.get(key)\n",
        "    if env_val:\n",
        "        CONFIG[key] = env_val\n",
        "\n",
        "# Render FINAL_DAILY_FILENAME from template if provided.\n",
        "# Keeps downstream code stable: always refer to CONFIG[\"FINAL_DAILY_FILENAME\"].\n",
        "if \"FINAL_DAILY_FILENAME_TEMPLATE\" in CONFIG:\n",
        "    region = CONFIG.get(\"region\", \"CA\")\n",
        "    start_date = CONFIG[\"start_date\"]\n",
        "    end_date = CONFIG[\"end_date\"]\n",
        "    res_m = CONFIG.get(\"grid_resolution_m\", 3000)\n",
        "    epsg = CONFIG.get(\"OPS_EPSG\", CONFIG.get(\"CA_ALBERS_EPSG\", 3310))\n",
        "    CONFIG[\"FINAL_DAILY_FILENAME\"] = _render_template(\n",
        "        CONFIG[\"FINAL_DAILY_FILENAME_TEMPLATE\"],\n",
        "        start=start_date,\n",
        "        end=end_date,\n",
        "        res_m=res_m,\n",
        "        epsg=epsg,\n",
        "        region=region,\n",
        "    )\n",
        "\n",
        "# Resolve pathlike keys under out_dir (only for keys that exist).\n",
        "_resolve_pathlike_keys(CONFIG, OUT_DIR)\n",
        "\n",
        "# Create common directories only if they are referenced in config.\n",
        "# (Avoid hardcoding \"manual\" for HydroPulse.)\n",
        "for k, v in CONFIG.items():\n",
        "    if k.endswith(\"_DIRNAME\") and isinstance(v, str):\n",
        "        os.makedirs(Path(CONFIG[\"out_dir\"]) / v, exist_ok=True)\n",
        "\n",
        "# EPSG constants (configurable)\n",
        "WGS84_EPSG = int(CONFIG.get(\"WGS84_EPSG\", 4326))\n",
        "CA_ALBERS_EPSG = int(CONFIG.get(\"CA_ALBERS_EPSG\", 3310))\n",
        "OPS_EPSG = int(CONFIG.get(\"OPS_EPSG\", CA_ALBERS_EPSG))\n",
        "\n",
        "# Set working directory\n",
        "os.chdir(PROJECT_DIR)\n",
        "\n",
        "print(f\"Config loaded from {CONFIG_PATH}\")\n",
        "print(f\"Output dir: {CONFIG['out_dir']}\")\n",
        "print(f\"Final daily filename: {CONFIG.get('FINAL_DAILY_FILENAME')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19be8061",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "def resolve_out_path(path_str: str) -> str:\n",
        "    \"\"\"\n",
        "    Resolve a path string relative to CONFIG['out_dir'] unless already absolute.\n",
        "    Returns an absolute string path.\n",
        "    \"\"\"\n",
        "    p = Path(path_str)\n",
        "    if p.is_absolute():\n",
        "        return str(p)\n",
        "    return str((Path(CONFIG[\"out_dir\"]) / p).resolve())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ea9e2b8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Ensure California boundary and build 3 km grid clipped to land ---\n",
        "\n",
        "\n",
        "# Config\n",
        "res_m = int(CONFIG.get(\"grid_resolution_m\", 3000))\n",
        "out_epsg = int(CONFIG.get(\"crs_epsg\", 4326))\n",
        "out_dir = CONFIG[\"out_dir\"]; os.makedirs(out_dir, exist_ok=True)\n",
        "inset_buffer_m = int(CONFIG.get(\"coast_inset_m\", 0))  # e.g. 5000\n",
        "boundary_path = CONFIG.get(\"ca_boundary_path\", None)\n",
        "\n",
        "# 1) Ensure boundary: download Census cartographic boundary if missing\n",
        "if not boundary_path or not os.path.exists(boundary_path):\n",
        "    states_zip = os.path.join(out_dir, \"cb_2023_us_state_20m.zip\")\n",
        "    if not os.path.exists(states_zip):\n",
        "        url = CONFIG[\"CENSUS_STATES_ZIP_URL\"]\n",
        "        r = requests.get(url, timeout=int(CONFIG.get(\"CENSUS_STATES_TIMEOUT\", 120))); r.raise_for_status()\n",
        "        with open(states_zip, \"wb\") as f: f.write(r.content)\n",
        "    # Read from zip directly and select California\n",
        "    states = gpd.read_file(f\"zip://{states_zip}\")\n",
        "    if states.empty:\n",
        "        raise ValueError(\"Census states file loaded empty.\")\n",
        "    ca = states[states[\"STATEFP\"].astype(str).str.zfill(2).eq(\"06\")][[\"geometry\"]]\n",
        "    if ca.empty:\n",
        "        raise ValueError(\"California polygon not found in Census states file.\")\n",
        "    boundary_path = os.path.join(out_dir, \"california_boundary.gpkg\")\n",
        "    ca.to_file(boundary_path, driver=\"GPKG\")\n",
        "    CONFIG[\"ca_boundary_path\"] = boundary_path  # persist for later cells\n",
        "\n",
        "# 2) Load boundary, dissolve, project, optional inward buffer\n",
        "b = gpd.read_file(boundary_path)\n",
        "if b.crs is None: raise ValueError(\"Boundary file has no CRS.\")\n",
        "b = b[[\"geometry\"]].copy()\n",
        "b = b.to_crs(CA_ALBERS_EPSG)\n",
        "b = gpd.GeoDataFrame(geometry=[b.unary_union], crs=f\"EPSG:{CA_ALBERS_EPSG}\")\n",
        "if inset_buffer_m > 0:\n",
        "    b.geometry = b.buffer(-inset_buffer_m)\n",
        "    b = gpd.GeoDataFrame(geometry=[b.unary_union], crs=f\"EPSG:{CA_ALBERS_EPSG}\")\n",
        "\n",
        "# 3) Build snapped rectilinear grid over boundary bounds in EPSG:3310\n",
        "minx, miny, maxx, maxy = b.total_bounds\n",
        "snap_down = lambda v, s: np.floor(v/s)*s\n",
        "snap_up   = lambda v, s: np.ceil(v/s)*s\n",
        "minx, miny = snap_down(minx, res_m), snap_down(miny, res_m)\n",
        "maxx, maxy = snap_up(maxx, res_m), snap_up(maxy, res_m)\n",
        "\n",
        "xs = np.arange(minx, maxx, res_m)\n",
        "ys = np.arange(miny, maxy, res_m)\n",
        "n_rect = len(xs)*len(ys)\n",
        "if n_rect > 3_500_000:\n",
        "    raise MemoryError(f\"Grid too large ({n_rect:,}). Increase res_m or tile the state.\")\n",
        "\n",
        "cells, col_i, row_j = [], [], []\n",
        "for j, y in enumerate(ys):\n",
        "    for i, x in enumerate(xs):\n",
        "        cells.append(box(x, y, x+res_m, y+res_m)); col_i.append(i); row_j.append(j)\n",
        "\n",
        "gdf_proj = gpd.GeoDataFrame({\"col_i\": np.int32(col_i), \"row_j\": np.int32(row_j)},\n",
        "                            geometry=cells, crs=f\"EPSG:{CA_ALBERS_EPSG}\")\n",
        "gdf_proj[\"cell_area_m2\"] = float(res_m)*float(res_m)\n",
        "gdf_proj[\"grid_id\"] = f\"CA3310_{res_m}_\" + gdf_proj[\"col_i\"].astype(str) + \"_\" + gdf_proj[\"row_j\"].astype(str)\n",
        "\n",
        "# 4) Strict land clip and land fraction\n",
        "gdf_proj = gpd.sjoin(gdf_proj, b, how=\"inner\", predicate=\"intersects\").drop(columns=[\"index_right\"])\n",
        "inter = gpd.overlay(gdf_proj[[\"grid_id\",\"geometry\"]], b, how=\"intersection\", keep_geom_type=True)\n",
        "inter[\"land_area_m2\"] = inter.geometry.area\n",
        "land = inter[[\"grid_id\",\"land_area_m2\"]].groupby(\"grid_id\", as_index=False).sum()\n",
        "gdf_proj = gdf_proj.merge(land, on=\"grid_id\", how=\"left\")\n",
        "gdf_proj[\"land_area_m2\"] = gdf_proj[\"land_area_m2\"].fillna(0.0)\n",
        "gdf_proj[\"land_frac\"] = (gdf_proj[\"land_area_m2\"] / gdf_proj[\"cell_area_m2\"]).clip(0,1)\n",
        "gdf_proj = gdf_proj[gdf_proj[\"land_frac\"] > 0].reset_index(drop=True)\n",
        "\n",
        "# 5) Reproject to requested output CRS and save\n",
        "grid_gdf = gdf_proj.to_crs(out_epsg)\n",
        "\n",
        "parquet_path = os.path.join(out_dir, f\"grid_{res_m}m_CA.parquet\")\n",
        "grid_gdf.to_parquet(parquet_path, index=False)\n",
        "\n",
        "geojson_path = os.path.join(out_dir, f\"grid_{res_m}m_CA_head10.geojson\")\n",
        "grid_gdf.head(10).to_file(geojson_path, driver=\"GeoJSON\")\n",
        "\n",
        "# Diagnostics\n",
        "cell_area_km2 = (res_m/1000.0)**2\n",
        "eff_land_km2 = float((grid_gdf.get(\"land_frac\",1.0) * cell_area_km2).sum())\n",
        "print(f\"Saved: {parquet_path}\")\n",
        "print(f\"Cells: {len(grid_gdf):,}\")\n",
        "print(f\"Effective land area ≈ {round(eff_land_km2):,} km²\")\n",
        "print(f\"Implied cell size ≈ {round((eff_land_km2/len(grid_gdf))**0.5,2)} km\")\n",
        "\n",
        "grid_gdf.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "026fd848",
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Persist config + save grid (3310 ops copy, 4326 preview) + write metadata ---\n",
        "\n",
        "# Inputs assumed from prior cell:\n",
        "# - grid_gdf            : current grid GeoDataFrame (any CRS)\n",
        "# - CONFIG              : dict with out_dir, grid_resolution_m, crs_epsg, ca_boundary_path\n",
        "# - CA_ALBERS_EPSG=3310 : defined earlier\n",
        "\n",
        "out_dir = CONFIG[\"out_dir\"]; os.makedirs(out_dir, exist_ok=True)\n",
        "res_m = int(CONFIG.get(\"grid_resolution_m\", 3000))\n",
        "out_epsg = int(CONFIG.get(\"crs_epsg\", 4326))\n",
        "boundary_path = CONFIG.get(\"ca_boundary_path\")\n",
        "\n",
        "# 1) Persist boundary path back to CONFIG \n",
        "if not boundary_path or not os.path.exists(boundary_path):\n",
        "    raise FileNotFoundError(\"CONFIG['ca_boundary_path'] missing or invalid. Rebuild boundary.\")\n",
        "CONFIG[\"ca_boundary_path\"] = boundary_path\n",
        "\n",
        "config_runtime_path = os.path.join(out_dir, \"config_runtime.json\")\n",
        "with open(config_runtime_path, \"w\") as f:\n",
        "    json.dump(CONFIG, f, indent=2)\n",
        "print(\"Saved:\", config_runtime_path)\n",
        "\n",
        "# 2) Ensure we have an EPSG:3310 version for spatial ops\n",
        "if grid_gdf.crs is None:\n",
        "    raise ValueError(\"grid_gdf has no CRS. Rebuild grid.\")\n",
        "grid_3310 = grid_gdf.to_crs(3310) if grid_gdf.crs.to_epsg() != 3310 else grid_gdf\n",
        "\n",
        "# 3) Save operational GeoParquet in 3310 + lightweight WGS84 preview\n",
        "parquet_3310 = os.path.join(out_dir, f\"grid_{res_m}m_CA_epsg3310.parquet\")\n",
        "grid_3310.to_parquet(parquet_3310, index=False)\n",
        "print(\"Saved:\", parquet_3310, \"| cells:\", len(grid_3310))\n",
        "\n",
        "# Optional small preview in 4326 for quick map checks\n",
        "preview_4326 = grid_3310.to_crs(4326).head(500)  # cap to avoid huge files\n",
        "geojson_preview = os.path.join(out_dir, f\"grid_{res_m}m_CA_head500_epsg4326.geojson\")\n",
        "preview_4326.to_file(geojson_preview, driver=\"GeoJSON\")\n",
        "print(\"Saved:\", geojson_preview)\n",
        "\n",
        "# 4) Compute and save metadata\n",
        "cell_area_km2 = (res_m/1000.0)**2\n",
        "effective_land_km2 = float((grid_3310.get(\"land_frac\", 1.0) * cell_area_km2).sum())\n",
        "implied_cell_km = float((effective_land_km2 / len(grid_3310))**0.5)\n",
        "minx, miny, maxx, maxy = grid_3310.total_bounds\n",
        "bbox_km = ((maxx-minx)/1000.0, (maxy-miny)/1000.0)\n",
        "\n",
        "meta = {\n",
        "    \"timestamp_utc\": dt.utcnow().isoformat(timespec=\"seconds\") + \"Z\",\n",
        "    \"grid_resolution_m\": res_m,\n",
        "    \"crs_ops_epsg\": 3310,\n",
        "    \"crs_export_default_epsg\": out_epsg,\n",
        "    \"cells\": int(len(grid_3310)),\n",
        "    \"effective_land_area_km2\": round(effective_land_km2, 2),\n",
        "    \"implied_cell_km\": round(implied_cell_km, 4),\n",
        "    \"bbox_km_width_height\": [round(bbox_km[0], 2), round(bbox_km[1], 2)],\n",
        "    \"has_land_frac\": bool(\"land_frac\" in grid_3310.columns),\n",
        "    \"boundary_path\": boundary_path,\n",
        "    \"parquet_3310_path\": parquet_3310,\n",
        "    \"geojson_preview_4326_path\": geojson_preview,\n",
        "}\n",
        "\n",
        "meta_path = os.path.join(out_dir, f\"grid_{res_m}m_CA_meta.json\")\n",
        "with open(meta_path, \"w\") as f:\n",
        "    json.dump(meta, f, indent=2)\n",
        "print(\"Saved:\", meta_path)\n",
        "meta\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29ff5b92",
      "metadata": {},
      "outputs": [],
      "source": [
        "# CDO data fetch and processing functions\n",
        "# repeat some variables for clarity\n",
        "OUT_DIR = CONFIG[\"out_dir\"]\n",
        "RAW_DIR = os.path.join(OUT_DIR, CONFIG[\"CDO_RAW_DIRNAME\"])\n",
        "CLEAN_DIR = os.path.join(OUT_DIR, CONFIG[\"CDO_CLEAN_DIRNAME\"])\n",
        "os.makedirs(RAW_DIR, exist_ok=True); os.makedirs(CLEAN_DIR, exist_ok=True)\n",
        "\n",
        "def month_windows(start_date, end_date):\n",
        "    s = dt.fromisoformat(start_date).date().replace(day=1)\n",
        "    e = dt.fromisoformat(end_date).date()\n",
        "    cur = s\n",
        "    while cur <= e:\n",
        "        nxt = (cur + relativedelta(months=1)) - relativedelta(days=1)\n",
        "        yield cur.isoformat(), min(nxt, e).isoformat()\n",
        "        cur = (cur + relativedelta(months=1)).replace(day=1)\n",
        "\n",
        "def parse_attributes(attr):\n",
        "    parts = (attr or \"\").split(\",\"); parts += [\"\"] * (4 - len(parts))\n",
        "    mflag, qflag, sflag, obs_hhmm = parts[:4]\n",
        "    return mflag or None, qflag or None, sflag or None, obs_hhmm or None\n",
        "\n",
        "def fetch_cdo_page(session, url, headers, params, max_retries=None, base_delay=None, timeout=None):\n",
        "    if max_retries is None:\n",
        "        max_retries = int(CONFIG.get(\"CDO_MAX_RETRIES\", 6))\n",
        "    if base_delay is None:\n",
        "        base_delay = float(CONFIG.get(\"CDO_BACKOFF_BASE\", 0.8))\n",
        "    if timeout is None:\n",
        "        timeout = int(CONFIG.get(\"CDO_TIMEOUT\", 180))\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            r = session.get(url, headers=headers, params=params, timeout=timeout)\n",
        "            if r.status_code in (429, 500, 502, 503, 504):\n",
        "                raise requests.HTTPError(f\"{r.status_code} retry\")\n",
        "            r.raise_for_status()\n",
        "            return r.json()\n",
        "        except Exception:\n",
        "            if attempt == max_retries - 1:\n",
        "                raise\n",
        "            time.sleep(base_delay * (2 ** attempt))\n",
        "\n",
        "\n",
        "def cdo_stream_monthly(datasetid, locationid, startdate, enddate, datatypes, token,\n",
        "                       units=\"standard\", page_limit=1000, force=False):\n",
        "    url = CONFIG[\"CDO_BASE_URL\"]\n",
        "    headers = {\"token\": token}\n",
        "    session = requests.Session()\n",
        "    written = []\n",
        "\n",
        "    for dtid in datatypes:\n",
        "        for ms, me in month_windows(startdate, enddate):\n",
        "            out_csv = os.path.join(RAW_DIR, f\"ghcnd_{dtid}_{ms[:7]}.csv\")\n",
        "            if skip_if_exists(out_csv) and not force:\n",
        "                # resume: skip existing month-datatype file\n",
        "                written.append(out_csv); continue\n",
        "\n",
        "            frames = []\n",
        "            offset = 1\n",
        "            while True:\n",
        "                params = {\n",
        "                    \"datasetid\": datasetid, \"locationid\": locationid,\n",
        "                    \"startdate\": ms, \"enddate\": me,\n",
        "                    \"datatypeid\": dtid, \"units\": units,\n",
        "                    \"limit\": page_limit, \"offset\": offset\n",
        "                }\n",
        "                js = fetch_cdo_page(session, url, headers, params)\n",
        "                rows = js.get(\"results\", [])\n",
        "                if not rows:\n",
        "                    break\n",
        "                frames.append(pd.json_normalize(rows))\n",
        "                if len(rows) < page_limit:\n",
        "                    break\n",
        "                offset += page_limit\n",
        "                time.sleep(0.15)  # gentle pacing\n",
        "\n",
        "            if frames:\n",
        "                df = pd.concat(frames, ignore_index=True)\n",
        "                # normalize\n",
        "                df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\").dt.date\n",
        "                parsed = df[\"attributes\"].apply(parse_attributes)\n",
        "                df[[\"mflag\",\"qflag\",\"sflag\",\"obs_hhmm\"]] = pd.DataFrame(parsed.tolist(), index=df.index)\n",
        "                # scale tenths\n",
        "                scale = {\"PRCP\": 0.1, \"TMAX\": 0.1, \"TMIN\": 0.1}\n",
        "                df[\"datatype\"] = df[\"datatype\"].astype(str)\n",
        "                df[\"value\"] = pd.to_numeric(df[\"value\"], errors=\"coerce\")\n",
        "                df[\"value_scaled\"] = df.apply(lambda r: r[\"value\"] * scale.get(r[\"datatype\"], 1.0), axis=1)\n",
        "                # write monthly raw\n",
        "                df[[\"date\",\"datatype\",\"station\",\"attributes\",\"mflag\",\"qflag\",\"sflag\",\"obs_hhmm\",\"value\",\"value_scaled\"]].to_csv(out_csv, index=False)\n",
        "                written.append(out_csv)\n",
        "            else:\n",
        "                # create an empty file with header to mark completion\n",
        "                with open(out_csv, \"w\", newline=\"\") as f:\n",
        "                    w = csv.writer(f); w.writerow([\"date\",\"datatype\",\"station\",\"attributes\",\"mflag\",\"qflag\",\"sflag\",\"obs_hhmm\",\"value\",\"value_scaled\"])\n",
        "                written.append(out_csv)\n",
        "    return written\n",
        "\n",
        "def build_clean_wide():\n",
        "    # read all monthly raw files and assemble cleaned wide once\n",
        "    files = sorted([os.path.join(RAW_DIR, f) for f in os.listdir(RAW_DIR) if f.endswith(\".csv\")])\n",
        "    if not files:\n",
        "        return None\n",
        "    df = pd.concat((pd.read_csv(f, dtype={\"datatype\":str,\"station\":str}) for f in files), ignore_index=True)\n",
        "    # convert types back\n",
        "    df[\"date\"] = pd.to_datetime(df[\"date\"]).dt.date\n",
        "    # keep good qflag\n",
        "    df = df[(df[\"qflag\"].isna()) | (df[\"qflag\"]==\"\")]\n",
        "    wide = (\n",
        "        df.pivot_table(index=[\"station\",\"date\"], columns=\"datatype\", values=\"value_scaled\", aggfunc=\"mean\")\n",
        "          .reset_index()\n",
        "          .rename(columns={\"date\":\"obs_date\",\"PRCP\":\"precipitation_mm\",\"TMAX\":\"temperature_max_c\",\"TMIN\":\"temperature_min_c\"})\n",
        "          .sort_values([\"obs_date\",\"station\"])\n",
        "    )\n",
        "    # attach obs time from PRCP\n",
        "    prcp_times = df[df[\"datatype\"]==\"PRCP\"][[\"station\",\"date\",\"obs_hhmm\"]].drop_duplicates().rename(columns={\"date\":\"obs_date\"})\n",
        "    wide = wide.merge(prcp_times, on=[\"station\",\"obs_date\"], how=\"left\")\n",
        "    raw_all = os.path.join(OUT_DIR, \"ghcnd_daily_raw_all.csv\")\n",
        "    wide_all = os.path.join(OUT_DIR, \"ghcnd_daily_wide.csv\")\n",
        "    df.to_csv(raw_all, index=False)\n",
        "    wide.to_csv(wide_all, index=False)\n",
        "    return raw_all, wide_all, len(df), len(wide), wide[\"station\"].nunique(), wide[\"obs_date\"].nunique()\n",
        "\n",
        "# ---- Run statewide with resume capability ----\n",
        "token = os.environ.get(\"CDO_TOKEN\") or CONFIG.get(\"CDO_TOKEN\", \"\")\n",
        "if token and token != \"YOUR_NCEI_CDO_TOKEN\":\n",
        "    written = cdo_stream_monthly(\n",
        "        datasetid=\"GHCND\",\n",
        "        locationid=\"FIPS:06\",                      # California statewide\n",
        "        startdate=CONFIG[\"start_date\"],\n",
        "        enddate=CONFIG[\"end_date\"],\n",
        "        datatypes=[\"TMAX\",\"TMIN\",\"PRCP\"],\n",
        "        token=token,\n",
        "        units=\"standard\",\n",
        "        page_limit=1000,\n",
        "        force=False                                 # set True to re-download\n",
        "    )\n",
        "    print(f\"Monthly files written: {len(written)} → {RAW_DIR}\")\n",
        "\n",
        "    res = build_clean_wide()\n",
        "    if res:\n",
        "        raw_all, wide_all, n_raw, n_wide, n_stn, n_dates = res\n",
        "        print(f\"Saved raw:  {raw_all}\")\n",
        "        print(f\"Saved wide: {wide_all}\")\n",
        "        print(f\"Counts → raw: {n_raw} | wide: {n_wide} | stations: {n_stn} | dates: {n_dates}\")\n",
        "else:\n",
        "    print(\"Skipping CDO (missing CDO token).\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54dc4706",
      "metadata": {},
      "outputs": [],
      "source": [
        "# === GHCND DAILY: raw (long) -> cleaned (wide with lat/lon in bbox) ===\n",
        "# Input  (from your earlier step):  results/ghcnd_daily_raw_all.csv  (long form)\n",
        "# Output (used by superset):        results/ghcnd_daily_cleaned.parquet  (wide per station-day with lat/lon)\n",
        "\n",
        "# Need to do this because we aren't getting proper \"joins\" in our superset setup.\n",
        "\n",
        "\n",
        "BASE = CONFIG[\"out_dir\"]\n",
        "RAW = resolve_out_path(CONFIG[\"GHCND_RAW_CSV_NAME\"])\n",
        "OUT_PARQ = resolve_out_path(CONFIG[\"GHCND_CLEAN_PARQUET_NAME\"])\n",
        "OUT_CSV = resolve_out_path(CONFIG[\"GHCND_CLEAN_CSV_NAME\"])\n",
        "\n",
        "assert os.path.exists(RAW), f\"Missing raw GHCND file: {RAW}\"\n",
        "\n",
        "# 1) Ensure we have a station catalog with lat/lon\n",
        "#    Prefer a local copy if you already saved one; otherwise download NOAA's reference once.\n",
        "CAT_DIR = os.path.join(BASE, CONFIG[\"MANUAL_DIRNAME\"]); os.makedirs(CAT_DIR, exist_ok=True)\n",
        "CAT_TXT = os.path.join(CAT_DIR, CONFIG[\"GHCND_STATIONS_TXT_NAME\"])\n",
        "\n",
        "if not os.path.exists(CAT_TXT):\n",
        "    url = CONFIG[\"GHCND_STATIONS_URL\"]\n",
        "    r = requests.get(url, timeout=int(CONFIG.get(\"CENSUS_STATES_TIMEOUT\", 120))); r.raise_for_status()\n",
        "    with open(CAT_TXT, \"wb\") as f: f.write(r.content)\n",
        "\n",
        "# Parse ghcnd-stations.txt (fixed-width)\n",
        "# Columns per docs: ID(1-11), LAT(13-20), LON(22-30), ELEV(32-37), STATE(39-40), NAME(42-71) ...\n",
        "def parse_stations(path):\n",
        "    recs = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "        for line in f:\n",
        "            if len(line) < 40: \n",
        "                continue\n",
        "            sid = line[0:11].strip()\n",
        "            try:\n",
        "                lat = float(line[12:20].strip())\n",
        "                lon = float(line[21:30].strip())\n",
        "            except ValueError:\n",
        "                continue\n",
        "            state = line[38:40].strip()\n",
        "            name  = line[41:71].strip()\n",
        "            recs.append((sid, lat, lon, state, name))\n",
        "    return pd.DataFrame(recs, columns=[\"station_core\",\"lat\",\"lon\",\"state\",\"name\"])\n",
        "\n",
        "stations = parse_stations(CAT_TXT)\n",
        "\n",
        "# 2) Load your raw long-form CDO file\n",
        "# Expected columns seen in your sample:\n",
        "# ['attributes','datatype','date','mflag','obs_hhmm','qflag','sflag','station','value','value_scaled']\n",
        "raw = pd.read_csv(RAW, low_memory=False)\n",
        "\n",
        "# Normalize station key: raw uses \"GHCND:USW00023232\" → core \"USW00023232\"\n",
        "raw[\"station_core\"] = raw[\"station\"].astype(str).str.replace(\"^GHCND:\", \"\", regex=True)\n",
        "\n",
        "# Pick a numeric value column: prefer value_scaled if present; else scale GHCND native units.\n",
        "# GHCND native: PRCP = tenths of mm, TMAX/TMIN = tenths of °C.\n",
        "have_scaled = \"value_scaled\" in raw.columns\n",
        "def scaled_val(row):\n",
        "    if have_scaled and pd.notna(row[\"value_scaled\"]):\n",
        "        return float(row[\"value_scaled\"])\n",
        "    v = pd.to_numeric(row[\"value\"], errors=\"coerce\")\n",
        "    if pd.isna(v): \n",
        "        return np.nan\n",
        "    if row[\"datatype\"] == \"PRCP\":\n",
        "        return v * 0.1             # → mm\n",
        "    if row[\"datatype\"] in (\"TMAX\",\"TMIN\"):\n",
        "        return v * 0.1             # → °C\n",
        "    return v\n",
        "\n",
        "raw[\"val_clean\"] = raw.apply(scaled_val, axis=1)\n",
        "\n",
        "# Filter to the analysis window if your raw contains more than needed\n",
        "if \"start_date\" in CONFIG and \"end_date\" in CONFIG:\n",
        "    sd = pd.to_datetime(CONFIG[\"start_date\"], utc=True, errors=\"coerce\")\n",
        "    ed = pd.to_datetime(CONFIG[\"end_date\"],   utc=True, errors=\"coerce\")\n",
        "    raw[\"date\"] = pd.to_datetime(raw[\"date\"], utc=True, errors=\"coerce\")\n",
        "    raw = raw[(raw[\"date\"]>=sd) & (raw[\"date\"]<=ed)]\n",
        "else:\n",
        "    raw[\"date\"] = pd.to_datetime(raw[\"date\"], utc=True, errors=\"coerce\")\n",
        "\n",
        "# 3) Keep only the datatypes we need and one value per (station,date,datatype)\n",
        "keep_types = {\"PRCP\":\"precipitation_mm\", \"TMAX\":\"temperature_max_c\", \"TMIN\":\"temperature_min_c\"}\n",
        "raw = raw[raw[\"datatype\"].isin(keep_types.keys())].copy()\n",
        "\n",
        "# If multiple rows per (station,date,datatype), average them\n",
        "agg = (raw.groupby([\"station_core\",\"date\",\"datatype\"], as_index=False)[\"val_clean\"]\n",
        "          .mean())\n",
        "\n",
        "# 4) Pivot to wide columns\n",
        "wide = (agg.pivot(index=[\"station_core\",\"date\"], columns=\"datatype\", values=\"val_clean\")\n",
        "           .reset_index())\n",
        "# Rename columns to our canonical names\n",
        "wide = wide.rename(columns={k:v for k,v in keep_types.items() if k in wide.columns})\n",
        "\n",
        "# 5) Attach lat/lon from station catalog and clip to CA bbox\n",
        "wide = wide.merge(stations[[\"station_core\",\"lat\",\"lon\"]], on=\"station_core\", how=\"left\")\n",
        "\n",
        "# Clip to CONFIG[\"bbox\"] (California in your setup)\n",
        "bbox = CONFIG[\"bbox\"]\n",
        "minx, miny, maxx, maxy = bbox[\"nwlng\"], bbox[\"selat\"], bbox[\"selng\"], bbox[\"nwlat\"]\n",
        "in_box = (wide[\"lon\"].between(minx, maxx)) & (wide[\"lat\"].between(miny, maxy))\n",
        "wide = wide[in_box].copy()\n",
        "\n",
        "# 6) Final tidy columns + sorts\n",
        "cols_order = [\"station_core\",\"date\",\"lat\",\"lon\",\n",
        "              \"precipitation_mm\",\"temperature_max_c\",\"temperature_min_c\"]\n",
        "for c in cols_order:\n",
        "    if c not in wide.columns: wide[c] = np.nan\n",
        "wide = wide[cols_order].sort_values([\"station_core\",\"date\"])\n",
        "\n",
        "# 7) Save for the superset\n",
        "wide.to_parquet(OUT_PARQ, index=False)\n",
        "wide.to_csv(OUT_CSV, index=False)\n",
        "print(f\"Saved cleaned CDO daily → {OUT_PARQ} (rows={len(wide)}, stations={wide['station_core'].nunique()})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "599931c0",
      "metadata": {},
      "outputs": [],
      "source": [
        "### We've changed our approach -- DO NOT RUN THIS ANY MORE ###\n",
        "\n",
        "# === PRISM | Cell A: Raw ingest via official Web Service (resumable + chunkable) ===\n",
        "#\n",
        "# Web service syntax (per PRISM doc):\n",
        "#   https://services.nacse.org/prism/data/get/<region>/<res>/<element>/<date><?format=[nc|asc|bil]>\n",
        "# One grid per request, returns a .zip.  [oai_citation:4‡Prism Group](https://prism.oregonstate.edu/documents/PRISM_downloads_web_service.pdf)\n",
        "#\n",
        "# PRISM download limits:\n",
        "# - If a file is downloaded twice in a 24-hour period, no more downloads of that file allowed in that period\n",
        "# - Excessive activity may result in IP blocking  [oai_citation:5‡Prism Group](https://prism.oregonstate.edu/documents/PRISM_downloads_web_service.pdf)\n",
        "#\n",
        "# This cell is designed for \"download once, then reuse\", and for running in small chunks.\n",
        "\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "import requests\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "# -----------------------------\n",
        "# Required config (uses your existing baseline keys)\n",
        "# -----------------------------\n",
        "BASELINE_START = pd.to_datetime(CONFIG[\"BASELINE_START_DATE\"])\n",
        "BASELINE_END   = pd.to_datetime(CONFIG[\"BASELINE_END_DATE\"])\n",
        "\n",
        "# PRISM web service parameters\n",
        "PRISM_SERVICE_BASE = str(CONFIG.get(\"PRISM_SERVICE_BASE_URL\", \"https://services.nacse.org/prism/data/get\")) \n",
        "PRISM_REGION = str(CONFIG.get(\"PRISM_REGION\", \"us\"))     # 'us' CONUS  [oai_citation:6‡Prism Group](https://prism.oregonstate.edu/documents/PRISM_downloads_web_service.pdf)\n",
        "PRISM_RES    = str(CONFIG.get(\"PRISM_RESOLUTION\", \"4km\"))# '4km' supported  [oai_citation:7‡Prism Group](https://prism.oregonstate.edu/documents/PRISM_downloads_web_service.pdf)\n",
        "PRISM_ELEMENTS = CONFIG.get(\"PRISM_ELEMENTS\", [\"ppt\", \"tmean\"])  # elements list in doc  [oai_citation:8‡Prism Group](https://prism.oregonstate.edu/documents/PRISM_downloads_web_service.pdf)\n",
        "\n",
        "# Output folder\n",
        "OUT_DIR = Path(CONFIG[\"out_dir\"])\n",
        "RAW_DIR = OUT_DIR / \"prism_raw_baseline_ws\"\n",
        "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# -----------------------------\n",
        "# Controls for \"small chunks over several days\"\n",
        "# -----------------------------\n",
        "# You can run year-by-year to reduce server load and make progress predictable.\n",
        "# Set these each session (or add to config later if you like).\n",
        "RUN_YEAR_START = int(CONFIG.get(\"PRISM_RUN_YEAR_START\", BASELINE_START.year))\n",
        "RUN_YEAR_END   = int(CONFIG.get(\"PRISM_RUN_YEAR_END\", RUN_YEAR_START))  # default: single year\n",
        "MAX_DOWNLOADS_THIS_RUN = int(CONFIG.get(\"PRISM_MAX_DOWNLOADS_PER_RUN\", 400))  # hard stop per run\n",
        "\n",
        "# Throttling/backoff\n",
        "BASE_SLEEP_S = float(CONFIG.get(\"PRISM_BASE_SLEEP_S\", 2.0))   # PRISM sample script uses sleep 2  [oai_citation:9‡Prism Group](https://prism.oregonstate.edu/documents/PRISM_downloads_web_service.pdf)\n",
        "JITTER_S     = float(CONFIG.get(\"PRISM_JITTER_S\", 0.75))\n",
        "TIMEOUT_S    = int(CONFIG.get(\"PRISM_TIMEOUT_S\", 120))\n",
        "\n",
        "MAX_RETRIES  = int(CONFIG.get(\"PRISM_MAX_RETRIES\", 6))\n",
        "BACKOFF_BASE = float(CONFIG.get(\"PRISM_BACKOFF_BASE\", 1.7))\n",
        "\n",
        "# Optional: request format (default returns COG package; doc mentions optional formats nc/asc/bil)  [oai_citation:10‡Prism Group](https://prism.oregonstate.edu/documents/PRISM_downloads_web_service.pdf)\n",
        "# Leave as None for default; or set to \"bil\" / \"nc\"\n",
        "PRISM_FORMAT = CONFIG.get(\"PRISM_FORMAT\", None)\n",
        "\n",
        "# Optional: releaseDate check (defaults OFF).\n",
        "# When OFF: \"download once\" behavior = if file exists, skip without checking.\n",
        "USE_RELEASEDATE_CHECK = bool(CONFIG.get(\"PRISM_USE_RELEASEDATE_CHECK\", False))\n",
        "\n",
        "# -----------------------------\n",
        "# URL helpers (per doc)\n",
        "# -----------------------------\n",
        "def prism_grid_url(element: str, yyyymmdd: str) -> str:\n",
        "    # https://services.nacse.org/prism/data/get/<region>/<res>/<element>/<date><?format=...>  [oai_citation:11‡Prism Group](https://prism.oregonstate.edu/documents/PRISM_downloads_web_service.pdf)\n",
        "    url = f\"{PRISM_SERVICE_BASE}/{PRISM_REGION}/{PRISM_RES}/{element}/{yyyymmdd}\"\n",
        "    if PRISM_FORMAT:\n",
        "        url += f\"?format={PRISM_FORMAT}\"\n",
        "    return url\n",
        "\n",
        "def prism_release_url(element: str, yyyymmdd: str) -> str:\n",
        "    # https://services.nacse.org/prism/data/get/releaseDate/<region>/<resolution>/<element>/<date>?json=true  [oai_citation:12‡Prism Group](https://prism.oregonstate.edu/documents/PRISM_downloads_web_service.pdf)\n",
        "    return f\"{PRISM_SERVICE_BASE}/releaseDate/{PRISM_REGION}/{PRISM_RES}/{element}/{yyyymmdd}?json=true\"\n",
        "\n",
        "def safe_session() -> requests.Session:\n",
        "    s = requests.Session()\n",
        "    s.headers.update({\n",
        "        \"User-Agent\": CONFIG.get(\"USER_AGENT_HEADERS\", {}).get(\"User-Agent\", \"BlueLeafLabs/HydroPulse\"),\n",
        "        \"Accept\": \"*/*\",\n",
        "    })\n",
        "    return s\n",
        "\n",
        "def parse_filename_from_cd(cd: str) -> str | None:\n",
        "    # Content-Disposition: attachment; filename=prism_ppt_us_4km_19910101.zip\n",
        "    if not cd:\n",
        "        return None\n",
        "    cd = cd.strip()\n",
        "    parts = cd.split(\";\")\n",
        "    for p in parts:\n",
        "        p = p.strip()\n",
        "        if p.lower().startswith(\"filename=\"):\n",
        "            fn = p.split(\"=\", 1)[1].strip().strip('\"')\n",
        "            return fn\n",
        "    return None\n",
        "\n",
        "# -----------------------------\n",
        "# Local pathing\n",
        "# -----------------------------\n",
        "def out_path_for(element: str, yyyymmdd: str, filename_hint: str | None = None) -> Path:\n",
        "    ed = RAW_DIR / element\n",
        "    ed.mkdir(parents=True, exist_ok=True)\n",
        "    if filename_hint:\n",
        "        return ed / filename_hint\n",
        "    # Fallback (stable and unique even if server changes naming slightly)\n",
        "    suffix = PRISM_FORMAT if PRISM_FORMAT else \"cog\"\n",
        "    return ed / f\"prism_{element}_{PRISM_REGION}_{PRISM_RES}_{yyyymmdd}_{suffix}.zip\"\n",
        "\n",
        "# -----------------------------\n",
        "# Optional releaseDate logic\n",
        "# -----------------------------\n",
        "def fetch_release_date(session: requests.Session, element: str, yyyymmdd: str) -> str | None:\n",
        "    # doc: releaseDate service provides release date; older than Apr 2014 may be unpopulated  [oai_citation:13‡Prism Group](https://prism.oregonstate.edu/documents/PRISM_downloads_web_service.pdf)\n",
        "    try:\n",
        "        r = session.get(prism_release_url(element, yyyymmdd), timeout=TIMEOUT_S)\n",
        "        if r.status_code != 200:\n",
        "            return None\n",
        "        js = r.json()\n",
        "        # The PDF describes fields; response structure may be list/dict depending on single vs range.\n",
        "        # We'll defensively extract any plausible release date string.\n",
        "        if isinstance(js, list) and js:\n",
        "            return js[0].get(\"releaseDate\") or js[0].get(\"ReleaseDate\") or js[0].get(\"release_date\")\n",
        "        if isinstance(js, dict):\n",
        "            return js.get(\"releaseDate\") or js.get(\"ReleaseDate\") or js.get(\"release_date\")\n",
        "        return None\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "# -----------------------------\n",
        "# Download with retries + backoff\n",
        "# -----------------------------\n",
        "def download_one(session: requests.Session, element: str, yyyymmdd: str) -> tuple[str, Path | None]:\n",
        "    url = prism_grid_url(element, yyyymmdd)\n",
        "\n",
        "    # First request HEAD-like via GET stream (server returns a zip)\n",
        "    for attempt in range(MAX_RETRIES):\n",
        "        try:\n",
        "            r = session.get(url, stream=True, timeout=TIMEOUT_S)\n",
        "            if r.status_code == 404:\n",
        "                return (\"unavailable\", None)\n",
        "            if r.status_code in (429, 500, 502, 503, 504):\n",
        "                raise RuntimeError(f\"transient {r.status_code}\")\n",
        "            r.raise_for_status()\n",
        "\n",
        "            fn = parse_filename_from_cd(r.headers.get(\"Content-Disposition\", \"\"))\n",
        "            out_path = out_path_for(element, yyyymmdd, fn)\n",
        "\n",
        "            if out_path.exists():\n",
        "                # Do not re-download. Close response promptly.\n",
        "                r.close()\n",
        "                return (\"exists\", out_path)\n",
        "\n",
        "            tmp = out_path.with_suffix(out_path.suffix + \".part\")\n",
        "            with open(tmp, \"wb\") as f:\n",
        "                for chunk in r.iter_content(chunk_size=1024 * 1024):\n",
        "                    if chunk:\n",
        "                        f.write(chunk)\n",
        "            os.replace(tmp, out_path)\n",
        "            return (\"downloaded\", out_path)\n",
        "\n",
        "        except Exception as e:\n",
        "            # backoff + jitter\n",
        "            sleep_s = (BACKOFF_BASE ** attempt) + random.random() * JITTER_S\n",
        "            print(f\"[WARN] {element} {yyyymmdd} attempt {attempt+1}/{MAX_RETRIES}: {e} -> sleep {sleep_s:.2f}s\")\n",
        "            time.sleep(sleep_s)\n",
        "\n",
        "    return (\"failed\", None)\n",
        "\n",
        "# -----------------------------\n",
        "# Build run date range (year-sliced)\n",
        "# -----------------------------\n",
        "run_start = max(BASELINE_START, pd.Timestamp(year=RUN_YEAR_START, month=1, day=1))\n",
        "run_end   = min(BASELINE_END,   pd.Timestamp(year=RUN_YEAR_END,   month=12, day=31))\n",
        "dates = pd.date_range(run_start, run_end, freq=\"D\")\n",
        "\n",
        "print(\"PRISM Cell A (web service) starting\")\n",
        "print(f\"Baseline window: {BASELINE_START.date()} → {BASELINE_END.date()}\")\n",
        "print(f\"Run slice      : {run_start.date()} → {run_end.date()} ({len(dates)} days)\")\n",
        "print(f\"Elements       : {PRISM_ELEMENTS}\")\n",
        "print(f\"Resolution     : {PRISM_RES} | Region: {PRISM_REGION}\")\n",
        "print(f\"Max downloads  : {MAX_DOWNLOADS_THIS_RUN}\")\n",
        "print(f\"Sleep (on dl)  : {BASE_SLEEP_S}s + jitter up to {JITTER_S}s\\n\")\n",
        "\n",
        "# -----------------------------\n",
        "# Main loop\n",
        "# -----------------------------\n",
        "session = safe_session()\n",
        "\n",
        "stats = {\"downloaded\": 0, \"exists\": 0, \"unavailable\": 0, \"failed\": 0, \"skipped_release\": 0}\n",
        "downloads_this_run = 0\n",
        "\n",
        "for element in PRISM_ELEMENTS:\n",
        "    print(f\"--- Element: {element} ---\")\n",
        "    for i, d in enumerate(dates, start=1):\n",
        "        yyyymmdd = d.strftime(\"%Y%m%d\")\n",
        "\n",
        "        # Enforce per-run cap (lets you run small chunks over multiple days)\n",
        "        if downloads_this_run >= MAX_DOWNLOADS_THIS_RUN:\n",
        "            print(f\"[STOP] Reached PRISM_MAX_DOWNLOADS_PER_RUN={MAX_DOWNLOADS_THIS_RUN}. Safe to rerun later.\")\n",
        "            break\n",
        "\n",
        "        # If file exists, skip immediately (resume behavior)\n",
        "        # We don’t know server filename until request, so check fallback name pattern too.\n",
        "        # We’ll do a cheap existence check by globbing element dir for this date.\n",
        "        el_dir = RAW_DIR / element\n",
        "        if el_dir.exists():\n",
        "            hits = list(el_dir.glob(f\"*{yyyymmdd}*.zip\"))\n",
        "            if hits:\n",
        "                stats[\"exists\"] += 1\n",
        "                continue\n",
        "\n",
        "        # Optional release-date check (OFF by default)\n",
        "        if USE_RELEASEDATE_CHECK:\n",
        "            _ = fetch_release_date(session, element, yyyymmdd)  # you can wire this into a manifest later\n",
        "\n",
        "        status, path = download_one(session, element, yyyymmdd)\n",
        "        stats[status] += 1\n",
        "\n",
        "        if status == \"downloaded\":\n",
        "            downloads_this_run += 1\n",
        "            # polite sleep only when we actually transfer bytes (PRISM sample script sleeps 2)  [oai_citation:14‡Prism Group](https://prism.oregonstate.edu/documents/PRISM_downloads_web_service.pdf)\n",
        "            time.sleep(BASE_SLEEP_S + random.random() * JITTER_S)\n",
        "\n",
        "        # Heartbeat every ~50 days\n",
        "        if i % 50 == 0:\n",
        "            print(f\"{element} day {i}/{len(dates)} | dl={stats['downloaded']} exist={stats['exists']} unavail={stats['unavailable']} failed={stats['failed']}\")\n",
        "\n",
        "    print(f\"Completed element: {element}\\n\")\n",
        "\n",
        "print(\"PRISM Cell A complete (this run slice)\")\n",
        "print(stats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4a4a349",
      "metadata": {},
      "outputs": [],
      "source": [
        "# === PRISM | Cell B: Build HydroPulse baseline from PRISM daily long-term normals (avg_30y) ===\n",
        "#\n",
        "# Inputs (local, no downloads):\n",
        "#   {out_dir}/manual/prism/prism_ppt_us_25m_YYYYMMDD_avg_30y.zip\n",
        "#   {out_dir}/manual/prism/prism_tmean_us_25m_YYYYMMDD_avg_30y.zip\n",
        "#\n",
        "# Output:\n",
        "#   {out_dir}/prism_baseline_normals/prism_normals_doy_grid_25m_to_3km.parquet\n",
        "#\n",
        "# Output schema:\n",
        "#   grid_id, doy, prism_ppt_norm_mm, prism_tmean_norm_c\n",
        "\n",
        "import re\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "\n",
        "import rasterio\n",
        "from rasterio.io import MemoryFile\n",
        "from pyproj import Transformer\n",
        "\n",
        "OUT_DIR = Path(CONFIG[\"out_dir\"])\n",
        "\n",
        "# Where you placed PRISM normals\n",
        "PRISM_DIR = Path(CONFIG.get(\"PRISM_MANUAL_DIR\", OUT_DIR / \"manual\" / \"prism\"))\n",
        "if not PRISM_DIR.exists():\n",
        "    raise FileNotFoundError(f\"PRISM_MANUAL_DIR not found: {PRISM_DIR}\")\n",
        "\n",
        "# HydroPulse grid (EPSG:3310)\n",
        "OPS_EPSG = int(CONFIG.get(\"OPS_EPSG\", 3310))\n",
        "GRID_PATH = Path(resolve_out_path(CONFIG[\"GRID_FILENAME\"]))\n",
        "\n",
        "# Output\n",
        "BASE_DIR = OUT_DIR / \"prism_baseline_normals\"\n",
        "BASE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "SHARDS_DIR = BASE_DIR / \"shards_doy\"\n",
        "SHARDS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "FINAL_PATH = BASE_DIR / \"prism_normals_doy_grid_25m_to_3km.parquet\"\n",
        "\n",
        "# --- Load grid centroids ---\n",
        "grid = gpd.read_parquet(GRID_PATH)\n",
        "if grid.crs is None or (grid.crs.to_epsg() or 0) != OPS_EPSG:\n",
        "    grid = grid.set_crs(f\"EPSG:{OPS_EPSG}\", allow_override=True) if grid.crs is None else grid.to_crs(OPS_EPSG)\n",
        "\n",
        "if \"grid_id\" not in grid.columns:\n",
        "    raise KeyError(\"Expected grid parquet to contain 'grid_id'.\")\n",
        "\n",
        "grid_id = grid[\"grid_id\"].astype(str).values\n",
        "centroids = grid.geometry.centroid\n",
        "xs = centroids.x.values\n",
        "ys = centroids.y.values\n",
        "n_cells = len(grid_id)\n",
        "\n",
        "print(f\"Grid: {n_cells} cells | EPSG:{OPS_EPSG}\")\n",
        "print(f\"PRISM normals dir: {PRISM_DIR}\")\n",
        "\n",
        "# --- PRISM filename parser (your observed convention) ---\n",
        "# Example: prism_ppt_us_25m_20200115_avg_30y.zip\n",
        "pat = re.compile(r\"^prism_(ppt|tmean)_us_25m_(\\d{8})_avg_30y\\.zip$\", re.IGNORECASE)\n",
        "\n",
        "ppt = {}\n",
        "tmean = {}\n",
        "\n",
        "for p in sorted(PRISM_DIR.glob(\"*.zip\")):\n",
        "    m = pat.match(p.name)\n",
        "    if not m:\n",
        "        continue\n",
        "    var = m.group(1).lower()\n",
        "    yyyymmdd = m.group(2)\n",
        "\n",
        "    # PRISM daily normals commonly use year=2020 as a convenient leap-year index;\n",
        "    # we convert YYYYMMDD -> DOY using that year token directly.\n",
        "    dt = pd.to_datetime(yyyymmdd, format=\"%Y%m%d\", utc=True)\n",
        "    doy = int(dt.dayofyear)\n",
        "\n",
        "    if var == \"ppt\":\n",
        "        ppt[doy] = p\n",
        "    elif var == \"tmean\":\n",
        "        tmean[doy] = p\n",
        "\n",
        "doys = sorted(set(ppt.keys()) & set(tmean.keys()))\n",
        "if not doys:\n",
        "    raise RuntimeError(\n",
        "        \"No matching PRISM ppt/tmean normals found.\\n\"\n",
        "        f\"Expected filenames like: prism_ppt_us_25m_20200115_avg_30y.zip in {PRISM_DIR}\"\n",
        "    )\n",
        "\n",
        "print(f\"Matched DOYs: {len(doys)} (e.g., {doys[:5]})\")\n",
        "\n",
        "# --- Read GeoTIFF inside PRISM zip via MemoryFile ---\n",
        "def open_tif_from_zip(zip_path: Path):\n",
        "    with zipfile.ZipFile(zip_path, \"r\") as z:\n",
        "        tif_names = [n for n in z.namelist() if n.lower().endswith(\".tif\")]\n",
        "        if not tif_names:\n",
        "            raise FileNotFoundError(f\"No .tif found inside {zip_path.name}\")\n",
        "        tif_name = tif_names[0]\n",
        "        tif_bytes = z.read(tif_name)\n",
        "\n",
        "    mem = MemoryFile(tif_bytes)\n",
        "    ds = mem.open()\n",
        "    return mem, ds\n",
        "\n",
        "def sample_zip_to_grid(zip_path: Path, xs3310: np.ndarray, ys3310: np.ndarray) -> np.ndarray:\n",
        "    mem, ds = open_tif_from_zip(zip_path)\n",
        "    try:\n",
        "        tf = Transformer.from_crs(f\"EPSG:{OPS_EPSG}\", ds.crs, always_xy=True)\n",
        "        sx, sy = tf.transform(xs3310, ys3310)\n",
        "\n",
        "        vals = np.array([v[0] for v in ds.sample(zip(sx, sy))], dtype=np.float64)\n",
        "        nodata = ds.nodata\n",
        "        if nodata is not None:\n",
        "            vals[vals == nodata] = np.nan\n",
        "        vals[~np.isfinite(vals)] = np.nan\n",
        "        return vals\n",
        "    finally:\n",
        "        ds.close()\n",
        "        mem.close()\n",
        "\n",
        "# --- Build shards per DOY (resumable) ---\n",
        "written = 0\n",
        "skipped = 0\n",
        "\n",
        "for doy in doys:\n",
        "    shard_path = SHARDS_DIR / f\"prism_normals_doy_{doy:03d}.parquet\"\n",
        "    if shard_path.exists():\n",
        "        skipped += 1\n",
        "        continue\n",
        "\n",
        "    ppt_vals = sample_zip_to_grid(ppt[doy], xs, ys).astype(np.float32)      # mm\n",
        "    tm_vals  = sample_zip_to_grid(tmean[doy], xs, ys).astype(np.float32)    # °C\n",
        "\n",
        "    out = pd.DataFrame({\n",
        "        \"grid_id\": grid_id,\n",
        "        \"doy\": np.full(n_cells, doy, dtype=np.int16),\n",
        "        \"prism_ppt_norm_mm\": ppt_vals,\n",
        "        \"prism_tmean_norm_c\": tm_vals,\n",
        "    })\n",
        "    out.to_parquet(shard_path, index=False)\n",
        "    written += 1\n",
        "\n",
        "    if written % 25 == 0:\n",
        "        print(f\"Shards written: {written} | latest DOY={doy:03d}\")\n",
        "\n",
        "print(f\"Shard pass complete | written={written} | skipped={skipped}\")\n",
        "\n",
        "# --- Combine to one baseline parquet (fast enough at 366 shards) ---\n",
        "shards = sorted(SHARDS_DIR.glob(\"prism_normals_doy_*.parquet\"))\n",
        "df_all = pd.concat((pd.read_parquet(p) for p in shards), ignore_index=True)\n",
        "df_all.to_parquet(FINAL_PATH, index=False)\n",
        "\n",
        "print(f\"Saved baseline: {FINAL_PATH}\")\n",
        "print(f\"Rows: {len(df_all)} | doys: {df_all['doy'].nunique()} | cells: {df_all['grid_id'].nunique()}\")\n",
        "print(df_all.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b9a2bba",
      "metadata": {},
      "outputs": [],
      "source": [
        "# === SMAP (SPL4SMGP) via Harmony: CA-only subset + resumeable monthly chunks ===\n",
        "# Requires: pip install earthaccess harmony-py\n",
        "# Notes:\n",
        "# - Harmony client will use EDL_USERNAME/EDL_PASSWORD env vars or ~/.netrc. We'll use env vars from .env.\n",
        "# - We chunk by month to keep each Harmony job reasonably sized and resumeable.\n",
        "\n",
        "from pathlib import Path\n",
        "import os, re\n",
        "import datetime as dt\n",
        "\n",
        "import earthaccess\n",
        "from harmony import BBox, Client, Collection, Request, CapabilitiesRequest\n",
        "\n",
        "# -----------------------\n",
        "# 0) Credentials (from .env) + \"reload\" without restarting\n",
        "# -----------------------\n",
        "# Add these lines to your .env (PROJECT_DIR already exists in your setup):\n",
        "#   EDL_USERNAME=\"your_earthdata_username\"\n",
        "#   EDL_PASSWORD=\"your_earthdata_password\"\n",
        "#\n",
        "# Then rerun ONLY your config-loader cell (the one that calls apply_env_overrides()).\n",
        "# If you want to reload .env right here as well:\n",
        "try:\n",
        "    apply_env_overrides()  # uses your existing helper from earlier cells\n",
        "except NameError:\n",
        "    pass\n",
        "\n",
        "# Harmony Client looks specifically for EDL_USERNAME/EDL_PASSWORD in env vars.  [oai_citation:2‡nasa-openscapes.github.io](https://nasa-openscapes.github.io/earthdata-cloud-cookbook/tutorials/Harmony.html)\n",
        "if not os.environ.get(\"EDL_USERNAME\"):\n",
        "    # accept EARTHDATA_* aliases if you prefer those names\n",
        "    if os.environ.get(\"EARTHDATA_USERNAME\"):\n",
        "        os.environ[\"EDL_USERNAME\"] = os.environ[\"EARTHDATA_USERNAME\"]\n",
        "    if os.environ.get(\"EARTHDATA_PASSWORD\"):\n",
        "        os.environ[\"EDL_PASSWORD\"] = os.environ[\"EARTHDATA_PASSWORD\"]\n",
        "\n",
        "if not os.environ.get(\"EDL_USERNAME\") or not os.environ.get(\"EDL_PASSWORD\"):\n",
        "    raise RuntimeError(\n",
        "        \"Missing Earthdata credentials. Put EDL_USERNAME and EDL_PASSWORD in .env and rerun config-loader cell.\"\n",
        "    )\n",
        "\n",
        "# Optional: login via earthaccess too (useful for troubleshooting auth early).\n",
        "earthaccess.login(strategy=\"environment\")\n",
        "\n",
        "# -----------------------\n",
        "# 1) Config-driven parameters\n",
        "# -----------------------\n",
        "# CA bbox conversion to UI order (W, S, E, N):\n",
        "#   W = nwlng, S = selat, E = selng, N = nwlat\n",
        "bbox = CONFIG[\"bbox\"]\n",
        "W = float(bbox[\"nwlng\"])\n",
        "S = float(bbox[\"selat\"])\n",
        "E = float(bbox[\"selng\"])\n",
        "N = float(bbox[\"nwlat\"])\n",
        "\n",
        "# Use your analysis window for “event period”.\n",
        "# If you later want a baseline, run a separate pipeline with BASELINE_* dates.\n",
        "START = dt.date.fromisoformat(CONFIG[\"start_date\"])\n",
        "END   = dt.date.fromisoformat(CONFIG[\"end_date\"])\n",
        "\n",
        "# Dataset choice:\n",
        "# SPL4SMGP is the “SMAP L4 Global 3-hourly … Geophysical Model Product” (as you selected).\n",
        "SHORT_NAME = CONFIG.get(\"SMAP_L4_SHORT_NAME\", \"SPL4SMGP\")\n",
        "\n",
        "# Variables: keep this list small. You can edit once you confirm exact names in capabilities.\n",
        "# (We auto-intersect with what Harmony reports as valid variable names for this collection.)\n",
        "DESIRED_VARS = CONFIG.get(\"SMAP_L4_VARIABLES\", [\n",
        "    # common high-value fields; actual availability is confirmed below\n",
        "    \"sm_surface\",\n",
        "    \"sm_rootzone\",\n",
        "])\n",
        "\n",
        "# Output directories (config-driven)\n",
        "OUT_DIR = Path(CONFIG[\"out_dir\"])\n",
        "SMAP_DIR = OUT_DIR / \"manual\" / \"smap\" / SHORT_NAME\n",
        "SMAP_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Resume policy: skip a month if we already see at least one file for that month.\n",
        "# (Harmony outputs may not keep original filenames, so we mark completion with a sentinel.)\n",
        "SENTINEL_DIR = SMAP_DIR / \"_done\"\n",
        "SENTINEL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"SMAP/Harmony setup\")\n",
        "print(\"  short_name :\", SHORT_NAME)\n",
        "print(\"  bbox (W,S,E,N):\", (W, S, E, N))\n",
        "print(\"  time window:\", START, \"→\", END)\n",
        "print(\"  out_dir   :\", str(SMAP_DIR))\n",
        "\n",
        "# -----------------------\n",
        "# 2) Discover Harmony capabilities and sanitize variable list\n",
        "# -----------------------\n",
        "harmony_client = Client()  # uses EDL_USERNAME/EDL_PASSWORD env vars  [oai_citation:3‡nasa-openscapes.github.io](https://nasa-openscapes.github.io/earthdata-cloud-cookbook/tutorials/Harmony.html)\n",
        "\n",
        "cap_req = CapabilitiesRequest(short_name=SHORT_NAME)\n",
        "cap = harmony_client.submit(cap_req)\n",
        "\n",
        "# capabilities response is a dict-like object (JSON). We defensively handle both dict/str.\n",
        "import json\n",
        "if isinstance(cap, str):\n",
        "    cap = json.loads(cap)\n",
        "\n",
        "concept_id = cap.get(\"conceptId\") or cap.get(\"concept_id\") or cap.get(\"conceptID\")\n",
        "if not concept_id:\n",
        "    raise RuntimeError(f\"Could not determine conceptId for {SHORT_NAME} from Harmony capabilities.\")\n",
        "\n",
        "available_vars = cap.get(\"variables\") or []\n",
        "available_var_names = set()\n",
        "\n",
        "# Harmony capabilities commonly return variables as list[dict] with \"name\" fields\n",
        "for v in available_vars:\n",
        "    if isinstance(v, dict) and \"name\" in v:\n",
        "        available_var_names.add(v[\"name\"])\n",
        "\n",
        "# If Harmony doesn't expose variables here, we’ll still try DESIRED_VARS as-is.\n",
        "if available_var_names:\n",
        "    desired = []\n",
        "    for name in DESIRED_VARS:\n",
        "        if name in available_var_names:\n",
        "            desired.append(name)\n",
        "        else:\n",
        "            # also allow partial matches (e.g., \"sm_surface\" matches \"sm_surface_analysis\")\n",
        "            hits = [vn for vn in available_var_names if vn == name or vn.endswith(name) or name in vn]\n",
        "            if hits:\n",
        "                desired.append(hits[0])\n",
        "    DESIRED_VARS = sorted(set(desired))\n",
        "    print(f\"  Harmony variableSubset available: {len(available_var_names)} variables advertised\")\n",
        "    print(\"  Using variables:\", DESIRED_VARS)\n",
        "else:\n",
        "    print(\"  Harmony did not advertise variable list in capabilities; using variables as provided:\", DESIRED_VARS)\n",
        "\n",
        "# -----------------------\n",
        "# 3) Helper: iterate months [START..END]\n",
        "# -----------------------\n",
        "def month_start(d: dt.date) -> dt.date:\n",
        "    return dt.date(d.year, d.month, 1)\n",
        "\n",
        "def next_month(d: dt.date) -> dt.date:\n",
        "    y, m = d.year, d.month\n",
        "    return dt.date(y + (m == 12), 1 if m == 12 else m + 1, 1)\n",
        "\n",
        "def month_range(start: dt.date, end: dt.date):\n",
        "    cur = month_start(start)\n",
        "    while cur <= end:\n",
        "        nxt = next_month(cur)\n",
        "        yield cur, min(end + dt.timedelta(days=1), nxt)  # stop is exclusive-ish for Harmony\n",
        "        cur = nxt\n",
        "\n",
        "# -----------------------\n",
        "# 4) Submit + download month-by-month with resume\n",
        "# -----------------------\n",
        "total_jobs = 0\n",
        "total_files = 0\n",
        "\n",
        "for m0, m1 in month_range(START, END):\n",
        "    tag = f\"{m0:%Y%m}\"\n",
        "    sentinel = SENTINEL_DIR / f\"{tag}.done\"\n",
        "    if sentinel.exists():\n",
        "        print(f\"[SKIP] {tag}: already marked done\")\n",
        "        continue\n",
        "\n",
        "    # Harmony Request: spatial bbox + temporal + variables\n",
        "    req = Request(\n",
        "        collection=Collection(id=concept_id),\n",
        "        spatial=BBox(W, S, E, N),\n",
        "        temporal={\"start\": dt.datetime(m0.year, m0.month, m0.day),\n",
        "                  \"stop\":  dt.datetime(m1.year, m1.month, m1.day)},\n",
        "        variables=DESIRED_VARS if DESIRED_VARS else None,\n",
        "        # Keep defaults unless you’ve confirmed a preferred output format for SPL4SMGP\n",
        "        # format=\"application/x-netcdf4\",\n",
        "    )\n",
        "\n",
        "    if not req.is_valid():\n",
        "        raise RuntimeError(f\"Invalid Harmony request for month {tag}: check bbox/time/vars\")\n",
        "\n",
        "    print(f\"[JOB] Submitting {tag} ...\")\n",
        "    job_id = harmony_client.submit(req)\n",
        "    total_jobs += 1\n",
        "\n",
        "    # Wait + download all outputs for this job into a month folder\n",
        "    harmony_client.wait_for_processing(job_id, show_progress=True)\n",
        "\n",
        "    month_dir = SMAP_DIR / tag\n",
        "    month_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # download_all returns futures; .result() yields file paths  [oai_citation:4‡HarmonyPy](https://harmony-py.readthedocs.io/en/stable/user/tutorial.html)\n",
        "    futures = harmony_client.download_all(job_id, directory=str(month_dir), overwrite=False)\n",
        "    files = [f.result() for f in futures]\n",
        "    files = [p for p in files if p]  # defensive\n",
        "\n",
        "    print(f\"  downloaded files: {len(files)} → {month_dir}\")\n",
        "    total_files += len(files)\n",
        "\n",
        "    # Mark month done (resume)\n",
        "    sentinel.write_text(f\"job_id={job_id}\\nfiles={len(files)}\\n\")\n",
        "    print(f\"[DONE] {tag}\")\n",
        "\n",
        "print(\"SMAP/Harmony complete\")\n",
        "print(\"  jobs submitted :\", total_jobs)\n",
        "print(\"  files downloaded:\", total_files)\n",
        "print(\"  root dir       :\", str(SMAP_DIR))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28ac19ff",
      "metadata": {},
      "outputs": [],
      "source": [
        "# === HydroPulse | Final Daily Grid Builder (v0: GHCND PRCP only) ===\n",
        "# Produces: CONFIG[\"FINAL_DAILY_FILENAME\"] as a canonical daily grid table:\n",
        "#   grid_id × date → prcp_mm + QC\n",
        "#\n",
        "# Inputs:\n",
        "#   - Grid parquet (EPSG:3310): CONFIG[\"GRID_FILENAME\"]\n",
        "#   - GHCND cleaned station-day parquet: resolve_out_path(CONFIG[\"GHCND_CLEAN_PARQUET_NAME\"])\n",
        "#\n",
        "# Notes for later HeatShield refactor:\n",
        "#   - This cell establishes the common \"final builder\" contract: {grid_id, date, variables..., QC...}\n",
        "#   - Keep the interface stable; only swap/extend source adapters per repo.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "from pathlib import Path\n",
        "from sklearn.neighbors import BallTree\n",
        "\n",
        "# -----------------------------\n",
        "# Paths\n",
        "# -----------------------------\n",
        "OUT_DIR = Path(CONFIG[\"out_dir\"])\n",
        "GRID_PATH = Path(resolve_out_path(CONFIG[\"GRID_FILENAME\"]))\n",
        "GHCND_PATH = Path(resolve_out_path(CONFIG[\"GHCND_CLEAN_PARQUET_NAME\"]))\n",
        "FINAL_PATH = Path(resolve_out_path(CONFIG[\"FINAL_DAILY_FILENAME\"]))\n",
        "\n",
        "# Resume via daily shards\n",
        "SHARDS_DIR = OUT_DIR / \"derived\" / \"final_daily_shards_prcp\"\n",
        "SHARDS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"GRID_PATH:\", GRID_PATH)\n",
        "print(\"GHCND_PATH:\", GHCND_PATH)\n",
        "print(\"SHARDS_DIR:\", SHARDS_DIR)\n",
        "print(\"FINAL_PATH:\", FINAL_PATH)\n",
        "\n",
        "# -----------------------------\n",
        "# Column contract (confirmed)\n",
        "# -----------------------------\n",
        "STATION_COL = \"station_core\"\n",
        "DATE_COL    = \"date\"\n",
        "LAT_COL     = \"lat\"\n",
        "LON_COL     = \"lon\"\n",
        "PRCP_COL_IN = \"precipitation_mm\"\n",
        "\n",
        "# Output column naming for the final table\n",
        "PRCP_COL_OUT = \"prcp_mm\"\n",
        "\n",
        "# -----------------------------\n",
        "# Tunables (can later move to config)\n",
        "# -----------------------------\n",
        "K = int(CONFIG.get(\"PRCP_IDW_K\", 8))                        # k nearest stations\n",
        "HARD_CAP_KM = float(CONFIG.get(\"PRCP_HARD_CAP_KM\", 100.0))  # ignore stations beyond this radius\n",
        "POWER = float(CONFIG.get(\"PRCP_IDW_POWER\", 2.0))            # IDW power\n",
        "\n",
        "OPS_EPSG = int(CONFIG.get(\"OPS_EPSG\", 3310))\n",
        "WGS84_EPSG = int(CONFIG.get(\"WGS84_EPSG\", 4326))\n",
        "\n",
        "# -----------------------------\n",
        "# Helpers (keep these stable across repos)\n",
        "# -----------------------------\n",
        "def ensure_epsg(gdf: gpd.GeoDataFrame, epsg: int) -> gpd.GeoDataFrame:\n",
        "    if gdf.crs is None:\n",
        "        gdf = gdf.set_crs(f\"EPSG:{WGS84_EPSG}\")\n",
        "    if (gdf.crs.to_epsg() or 0) != epsg:\n",
        "        gdf = gdf.to_crs(epsg)\n",
        "    return gdf\n",
        "\n",
        "def ensure_grid_id(grid: gpd.GeoDataFrame) -> tuple[gpd.GeoDataFrame, str]:\n",
        "    for c in [\"grid_id\", \"cell_id\", \"id\"]:\n",
        "        if c in grid.columns:\n",
        "            return grid, c\n",
        "    grid = grid.copy()\n",
        "    grid[\"grid_id\"] = np.arange(len(grid), dtype=np.int32)\n",
        "    return grid, \"grid_id\"\n",
        "\n",
        "def build_balltree_from_points(geom: gpd.GeoSeries) -> BallTree:\n",
        "    xy = np.column_stack([geom.x.values, geom.y.values])\n",
        "    return BallTree(xy, metric=\"euclidean\")\n",
        "\n",
        "def idw(dist_m: np.ndarray, vals: np.ndarray, power: float) -> float:\n",
        "    if np.any(dist_m == 0):\n",
        "        return float(vals[np.argmin(dist_m)])\n",
        "    w = 1.0 / np.power(dist_m, power)\n",
        "    return float(np.sum(w * vals) / np.sum(w))\n",
        "\n",
        "def interpolate_prcp_for_day(grid_centroids: gpd.GeoSeries, stations_day: gpd.GeoDataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Returns a DF aligned to grid_centroids order with columns:\n",
        "      prcp_mm, n_used, maxdist_km, method\n",
        "    \"\"\"\n",
        "    n_cells = len(grid_centroids)\n",
        "    out = pd.DataFrame({\n",
        "        PRCP_COL_OUT: np.full(n_cells, np.nan, dtype=float),\n",
        "        \"n_used\": np.zeros(n_cells, dtype=np.int16),\n",
        "        \"maxdist_km\": np.full(n_cells, np.nan, dtype=float),\n",
        "        \"method\": np.full(n_cells, None, dtype=object),\n",
        "    })\n",
        "\n",
        "    if stations_day is None or len(stations_day) == 0:\n",
        "        return out\n",
        "\n",
        "    # Build tree on station points\n",
        "    tree = build_balltree_from_points(stations_day.geometry)\n",
        "    vals = stations_day[PRCP_COL_IN].to_numpy(dtype=float)\n",
        "\n",
        "    qxy = np.column_stack([grid_centroids.x.values, grid_centroids.y.values])\n",
        "    k_eff = min(K, len(stations_day))\n",
        "    dist_m, idx = tree.query(qxy, k=k_eff)\n",
        "\n",
        "    hard_cap_m = HARD_CAP_KM * 1000.0\n",
        "\n",
        "    for i in range(n_cells):\n",
        "        d = dist_m[i]\n",
        "        j = idx[i]\n",
        "\n",
        "        # Apply hard cap\n",
        "        mask = d <= hard_cap_m\n",
        "        if not np.any(mask):\n",
        "            continue\n",
        "\n",
        "        d_use = d[mask]\n",
        "        v_use = vals[j[mask]]\n",
        "\n",
        "        # Drop NaNs (defensive)\n",
        "        good = np.isfinite(v_use)\n",
        "        d_use = d_use[good]\n",
        "        v_use = v_use[good]\n",
        "        if len(v_use) == 0:\n",
        "            continue\n",
        "\n",
        "        if len(v_use) == 1:\n",
        "            out.at[i, PRCP_COL_OUT] = float(v_use[0])\n",
        "            out.at[i, \"method\"] = \"nearest\"\n",
        "        else:\n",
        "            out.at[i, PRCP_COL_OUT] = idw(d_use, v_use, power=POWER)\n",
        "            out.at[i, \"method\"] = f\"idw_k{len(v_use)}\"\n",
        "\n",
        "        out.at[i, \"n_used\"] = int(len(v_use))\n",
        "        out.at[i, \"maxdist_km\"] = float(np.max(d_use) / 1000.0)\n",
        "\n",
        "    return out\n",
        "\n",
        "# -----------------------------\n",
        "# Load grid (EPSG:3310) and compute centroids\n",
        "# -----------------------------\n",
        "grid = gpd.read_parquet(GRID_PATH)\n",
        "grid = ensure_epsg(grid, OPS_EPSG)\n",
        "grid, GID_COL = ensure_grid_id(grid)\n",
        "centroids = grid.geometry.centroid\n",
        "\n",
        "print(f\"Grid: {len(grid)} cells | CRS EPSG: {grid.crs.to_epsg()} | id col: {GID_COL}\")\n",
        "\n",
        "# -----------------------------\n",
        "# Load cleaned GHCND station-day table\n",
        "# -----------------------------\n",
        "cdo = pd.read_parquet(GHCND_PATH)\n",
        "\n",
        "required = [STATION_COL, DATE_COL, LAT_COL, LON_COL, PRCP_COL_IN]\n",
        "missing = [c for c in required if c not in cdo.columns]\n",
        "if missing:\n",
        "    raise KeyError(f\"Missing required columns in {GHCND_PATH.name}: {missing}. Have: {list(cdo.columns)}\")\n",
        "\n",
        "# Normalize date to UTC day\n",
        "cdo[DATE_COL] = pd.to_datetime(cdo[DATE_COL], utc=True, errors=\"coerce\").dt.normalize()\n",
        "\n",
        "# Filter date window\n",
        "start = pd.to_datetime(CONFIG[\"start_date\"], utc=True).normalize()\n",
        "end = pd.to_datetime(CONFIG[\"end_date\"], utc=True).normalize()\n",
        "cdo = cdo[(cdo[DATE_COL] >= start) & (cdo[DATE_COL] <= end)].copy()\n",
        "\n",
        "# Drop invalid coords / missing precip\n",
        "cdo = cdo[np.isfinite(cdo[LAT_COL]) & np.isfinite(cdo[LON_COL])].copy()\n",
        "cdo = cdo[np.isfinite(cdo[PRCP_COL_IN])].copy()\n",
        "\n",
        "print(\"Station-day rows:\", len(cdo), \"| stations:\", cdo[STATION_COL].nunique(), \"| dates:\", cdo[DATE_COL].nunique())\n",
        "\n",
        "# GeoDataFrame in OPS_EPSG\n",
        "pts = gpd.GeoDataFrame(\n",
        "    cdo,\n",
        "    geometry=gpd.points_from_xy(cdo[LON_COL], cdo[LAT_COL]),\n",
        "    crs=f\"EPSG:{WGS84_EPSG}\"\n",
        ")\n",
        "pts = ensure_epsg(pts, OPS_EPSG)\n",
        "\n",
        "# Pre-group by date for speed (avoid repeated boolean filters)\n",
        "pts_by_date = {d: df for d, df in pts.groupby(DATE_COL)}\n",
        "all_dates = pd.date_range(start=start, end=end, freq=\"D\")\n",
        "\n",
        "# -----------------------------\n",
        "# Daily loop with resume\n",
        "# -----------------------------\n",
        "written = 0\n",
        "skipped = 0\n",
        "\n",
        "for d in all_dates:\n",
        "    tag = d.strftime(\"%Y%m%d\")\n",
        "    shard_path = SHARDS_DIR / f\"final_prcp_{tag}.parquet\"\n",
        "    if shard_path.exists():\n",
        "        skipped += 1\n",
        "        continue\n",
        "\n",
        "    day_pts = pts_by_date.get(d)\n",
        "    interp = interpolate_prcp_for_day(centroids, day_pts)\n",
        "\n",
        "    out = pd.DataFrame({\n",
        "        GID_COL: grid[GID_COL].values,\n",
        "        \"date\": np.full(len(grid), d),\n",
        "    })\n",
        "    out = pd.concat([out, interp], axis=1)\n",
        "\n",
        "    out.to_parquet(shard_path, index=False)\n",
        "    written += 1\n",
        "\n",
        "print(f\"Shards written: {written} | skipped (resume): {skipped} | total days: {len(all_dates)}\")\n",
        "\n",
        "# -----------------------------\n",
        "# Compose FINAL parquet\n",
        "# -----------------------------\n",
        "shards = sorted(SHARDS_DIR.glob(\"final_prcp_*.parquet\"))\n",
        "if not shards:\n",
        "    raise FileNotFoundError(f\"No shards found in {SHARDS_DIR}\")\n",
        "\n",
        "df_final = pd.concat((pd.read_parquet(p) for p in shards), ignore_index=True)\n",
        "\n",
        "df_final[\"date\"] = pd.to_datetime(df_final[\"date\"], utc=True).dt.normalize()\n",
        "df_final = df_final.sort_values([GID_COL, \"date\"]).reset_index(drop=True)\n",
        "\n",
        "FINAL_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
        "df_final.to_parquet(FINAL_PATH, index=False)\n",
        "\n",
        "print(\"Saved FINAL:\", FINAL_PATH)\n",
        "print(\"Final rows:\", len(df_final), \"| cells:\", df_final[GID_COL].nunique(), \"| dates:\", df_final[\"date\"].nunique())\n",
        "print(df_final.head())"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "blueleaflabs",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
